{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02794a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langgraph langchain_chroma langchain-google-genai langchain-community PYPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2576397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Connection timed out while downloading.\n",
      "WARNING: Attempting to resume incomplete download (8.4 MB/109.3 MB, attempt 1)\n",
      "WARNING: Attempting to resume incomplete download (58.0 MB/109.3 MB, attempt 2)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'f:\\\\Agents\\\\SQL_Agent_Langchain_v1\\\\.venv\\\\Lib\\\\site-packages\\\\scipy\\\\sparse\\\\_sparsetools.cp312-win_amd64.pyd'\n",
      "Check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.9.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-12.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2025.10.23-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\agents\\sql_agent_langchain_v1\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached regex-2025.10.23-cp312-cp312-win_amd64.whl (276 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading torch-2.9.0-cp312-cp312-win_amd64.whl (109.3 MB)\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.5/109.3 MB 57.7 kB/s eta 0:31:27\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 0.8/109.3 MB 47.1 kB/s eta 0:38:23\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.0/109.3 MB 44.4 kB/s eta 0:40:39\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "   ---------------------------------------- 1.3/109.3 MB 49.5 kB/s eta 0:36:24\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.6/109.3 MB 48.2 kB/s eta 0:37:16\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 1.8/109.3 MB 41.8 kB/s eta 0:42:50\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.1/109.3 MB 44.2 kB/s eta 0:40:25\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.4/109.3 MB 44.0 kB/s eta 0:40:29\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "    --------------------------------------- 2.6/109.3 MB 42.4 kB/s eta 0:41:53\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 2.9/109.3 MB 44.0 kB/s eta 0:40:19\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.1/109.3 MB 53.2 kB/s eta 0:33:17\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.4/109.3 MB 53.2 kB/s eta 0:33:10\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.7/109.3 MB 56.0 kB/s eta 0:31:27\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 3.9/109.3 MB 59.7 kB/s eta 0:29:26\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.2/109.3 MB 58.7 kB/s eta 0:29:50\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.5/109.3 MB 60.3 kB/s eta 0:28:58\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 4.7/109.3 MB 60.1 kB/s eta 0:29:00\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.0/109.3 MB 61.9 kB/s eta 0:28:05\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   - -------------------------------------- 5.2/109.3 MB 62.0 kB/s eta 0:27:58\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.5/109.3 MB 61.4 kB/s eta 0:28:11\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 5.8/109.3 MB 62.2 kB/s eta 0:27:46\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.0/109.3 MB 63.9 kB/s eta 0:26:56\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.3/109.3 MB 65.0 kB/s eta 0:26:26\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.6/109.3 MB 62.8 kB/s eta 0:27:15\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 6.8/109.3 MB 61.8 kB/s eta 0:27:38\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.1/109.3 MB 59.0 kB/s eta 0:28:54\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.3/109.3 MB 55.7 kB/s eta 0:30:29\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.6/109.3 MB 50.2 kB/s eta 0:33:46\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 7.9/109.3 MB 47.4 kB/s eta 0:35:38\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   -- ------------------------------------- 8.1/109.3 MB 48.1 kB/s eta 0:35:05\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.4/109.3 MB 49.5 kB/s eta 0:33:59\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.7/109.3 MB 50.1 kB/s eta 0:33:29\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 8.9/109.3 MB 49.4 kB/s eta 0:33:51\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.2/109.3 MB 50.1 kB/s eta 0:33:21\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.4/109.3 MB 46.0 kB/s eta 0:36:11\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 9.7/109.3 MB 44.1 kB/s eta 0:37:36\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.0/109.3 MB 37.6 kB/s eta 0:43:59\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 38.1 kB/s eta 0:43:17\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.5/109.3 MB 38.3 kB/s eta 0:43:01\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   --- ------------------------------------ 10.7/109.3 MB 37.7 kB/s eta 0:43:31\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.0/109.3 MB 42.0 kB/s eta 0:38:58\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.3/109.3 MB 38.9 kB/s eta 0:42:00\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 35.6 kB/s eta 0:45:49\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 29.3 kB/s eta 0:55:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 27.3 kB/s eta 0:59:22\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.3/109.3 MB 26.2 kB/s eta 1:01:39\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.6/109.3 MB 26.7 kB/s eta 1:00:20\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 27.8 kB/s eta 0:57:45\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.1/109.3 MB 28.8 kB/s eta 0:55:41\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 30.2 kB/s eta 0:52:55\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ---- ----------------------------------- 13.6/109.3 MB 31.1 kB/s eta 0:51:17\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 13.9/109.3 MB 32.0 kB/s eta 0:49:42\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 31.7 kB/s eta 0:50:06\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.4/109.3 MB 30.7 kB/s eta 0:51:26\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 31.0 kB/s eta 0:50:55\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 31.1 kB/s eta 0:50:37\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 28.4 kB/s eta 0:55:09\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 19.7 kB/s eta 1:19:25\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 20.5 kB/s eta 1:15:59\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.0/109.3 MB 29.3 kB/s eta 0:53:01\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 31.5 kB/s eta 0:49:10\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.5/109.3 MB 34.7 kB/s eta 0:44:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 16.8/109.3 MB 35.4 kB/s eta 0:43:31\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.0/109.3 MB 35.2 kB/s eta 0:43:42\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.3/109.3 MB 35.3 kB/s eta 0:43:26\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.6/109.3 MB 36.0 kB/s eta 0:42:25\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 17.8/109.3 MB 36.2 kB/s eta 0:42:07\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 37.7 kB/s eta 0:40:21\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.4/109.3 MB 36.0 kB/s eta 0:42:03\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.6/109.3 MB 36.0 kB/s eta 0:42:02\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------ --------------------------------- 18.9/109.3 MB 37.0 kB/s eta 0:40:47\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.1/109.3 MB 36.4 kB/s eta 0:41:16\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.4/109.3 MB 38.0 kB/s eta 0:39:28\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.7/109.3 MB 36.9 kB/s eta 0:40:27\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 19.9/109.3 MB 34.6 kB/s eta 0:43:02\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.2/109.3 MB 32.8 kB/s eta 0:45:14\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.4/109.3 MB 32.0 kB/s eta 0:46:15\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 20.7/109.3 MB 32.9 kB/s eta 0:44:51\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.0/109.3 MB 34.9 kB/s eta 0:42:12\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.2/109.3 MB 35.1 kB/s eta 0:41:49\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.5/109.3 MB 37.6 kB/s eta 0:38:58\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   ------- -------------------------------- 21.8/109.3 MB 38.0 kB/s eta 0:38:24\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.0/109.3 MB 37.3 kB/s eta 0:38:58\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.3/109.3 MB 37.4 kB/s eta 0:38:48\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.5/109.3 MB 36.4 kB/s eta 0:39:41\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 22.8/109.3 MB 35.4 kB/s eta 0:40:46\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.1/109.3 MB 37.4 kB/s eta 0:38:25\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.3/109.3 MB 36.8 kB/s eta 0:38:59\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.6/109.3 MB 37.2 kB/s eta 0:38:23\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 23.9/109.3 MB 38.9 kB/s eta 0:36:36\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.1/109.3 MB 39.5 kB/s eta 0:35:59\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   -------- ------------------------------- 24.4/109.3 MB 41.3 kB/s eta 0:34:15\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.6/109.3 MB 42.1 kB/s eta 0:33:31\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 24.9/109.3 MB 42.9 kB/s eta 0:32:47\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.2/109.3 MB 41.6 kB/s eta 0:33:42\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.4/109.3 MB 42.0 kB/s eta 0:33:16\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 25.7/109.3 MB 42.1 kB/s eta 0:33:05\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.0/109.3 MB 41.0 kB/s eta 0:33:53\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.2/109.3 MB 39.8 kB/s eta 0:34:47\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.5/109.3 MB 38.1 kB/s eta 0:36:12\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 26.7/109.3 MB 36.8 kB/s eta 0:37:22\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.0/109.3 MB 36.0 kB/s eta 0:38:05\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   --------- ------------------------------ 27.3/109.3 MB 36.7 kB/s eta 0:37:13\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.5/109.3 MB 36.8 kB/s eta 0:37:01\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 36.3 kB/s eta 0:37:27\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.0/109.3 MB 37.5 kB/s eta 0:36:07\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.3/109.3 MB 37.8 kB/s eta 0:35:43\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 39.6 kB/s eta 0:33:57\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 28.8/109.3 MB 42.1 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 42.8 kB/s eta 0:31:15\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.4/109.3 MB 42.4 kB/s eta 0:31:27\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 41.7 kB/s eta 0:31:52\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 40.3 kB/s eta 0:32:50\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.1/109.3 MB 37.9 kB/s eta 0:34:51\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 38.2 kB/s eta 0:34:27\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.7/109.3 MB 36.0 kB/s eta 0:36:22\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 35.4 kB/s eta 0:36:54\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 36.2 kB/s eta 0:35:59\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 28.5 kB/s eta 0:45:33\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 29.1 kB/s eta 0:44:28\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.0/109.3 MB 28.9 kB/s eta 0:44:34\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 38.1 kB/s eta 0:33:41\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 40.4 kB/s eta 0:31:40\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ----------- ---------------------------- 32.8/109.3 MB 41.2 kB/s eta 0:30:57\n",
      "   ------------ --------------------------- 33.0/109.3 MB 42.8 kB/s  0:29:43\n",
      "Resuming download torch-2.9.0-cp312-cp312-win_amd64.whl (33.0 MB/109.3 MB)\n",
      "   ------------ --------------------------- 33.0/109.3 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 33.0/109.3 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 33.0/109.3 MB ? eta -:--:--\n",
      "   ----------- --------------------------- 33.3/109.3 MB 399.6 kB/s eta 0:03:11\n",
      "   ----------- --------------------------- 33.3/109.3 MB 399.6 kB/s eta 0:03:11\n",
      "   ----------- --------------------------- 33.6/109.3 MB 568.6 kB/s eta 0:02:14\n",
      "   ------------ -------------------------- 33.8/109.3 MB 699.0 kB/s eta 0:01:48\n",
      "   ------------ -------------------------- 33.8/109.3 MB 699.0 kB/s eta 0:01:48\n",
      "   ------------ -------------------------- 33.8/109.3 MB 699.0 kB/s eta 0:01:48\n",
      "   ------------ -------------------------- 33.8/109.3 MB 699.0 kB/s eta 0:01:48\n",
      "   ------------ -------------------------- 34.6/109.3 MB 745.8 kB/s eta 0:01:41\n",
      "   ------------ -------------------------- 34.6/109.3 MB 745.8 kB/s eta 0:01:41\n",
      "   ------------ -------------------------- 34.6/109.3 MB 745.8 kB/s eta 0:01:41\n",
      "   ------------ -------------------------- 35.1/109.3 MB 745.8 kB/s eta 0:01:40\n",
      "   ------------ -------------------------- 35.4/109.3 MB 782.5 kB/s eta 0:01:35\n",
      "   ------------ -------------------------- 35.4/109.3 MB 782.5 kB/s eta 0:01:35\n",
      "   ------------ -------------------------- 35.6/109.3 MB 766.1 kB/s eta 0:01:37\n",
      "   ------------ -------------------------- 35.6/109.3 MB 766.1 kB/s eta 0:01:37\n",
      "   ------------ -------------------------- 35.6/109.3 MB 766.1 kB/s eta 0:01:37\n",
      "   ------------ -------------------------- 36.2/109.3 MB 789.6 kB/s eta 0:01:33\n",
      "   ------------- ------------------------- 36.4/109.3 MB 832.4 kB/s eta 0:01:28\n",
      "   ------------- ------------------------- 36.7/109.3 MB 844.8 kB/s eta 0:01:26\n",
      "   ------------- ------------------------- 36.7/109.3 MB 844.8 kB/s eta 0:01:26\n",
      "   ------------- ------------------------- 37.0/109.3 MB 811.8 kB/s eta 0:01:30\n",
      "   ------------- ------------------------- 37.0/109.3 MB 811.8 kB/s eta 0:01:30\n",
      "   ------------- ------------------------- 37.0/109.3 MB 811.8 kB/s eta 0:01:30\n",
      "   ------------- ------------------------- 37.5/109.3 MB 803.4 kB/s eta 0:01:30\n",
      "   ------------- ------------------------- 37.7/109.3 MB 834.3 kB/s eta 0:01:26\n",
      "   ------------- ------------------------- 37.7/109.3 MB 834.3 kB/s eta 0:01:26\n",
      "   ------------- ------------------------- 37.7/109.3 MB 834.3 kB/s eta 0:01:26\n",
      "   ------------- ------------------------- 38.0/109.3 MB 787.1 kB/s eta 0:01:31\n",
      "   ------------- ------------------------- 38.3/109.3 MB 806.6 kB/s eta 0:01:29\n",
      "   ------------- ------------------------- 38.5/109.3 MB 806.2 kB/s eta 0:01:28\n",
      "   ------------- ------------------------- 38.8/109.3 MB 827.5 kB/s eta 0:01:26\n",
      "   ------------- ------------------------- 38.8/109.3 MB 827.5 kB/s eta 0:01:26\n",
      "   ------------- ------------------------- 39.1/109.3 MB 819.3 kB/s eta 0:01:26\n",
      "   -------------- ------------------------ 39.3/109.3 MB 833.6 kB/s eta 0:01:24\n",
      "   -------------- ------------------------ 39.6/109.3 MB 825.7 kB/s eta 0:01:25\n",
      "   -------------- ------------------------ 39.6/109.3 MB 825.7 kB/s eta 0:01:25\n",
      "   -------------- ------------------------ 39.8/109.3 MB 816.8 kB/s eta 0:01:26\n",
      "   -------------- ------------------------ 39.8/109.3 MB 816.8 kB/s eta 0:01:26\n",
      "   -------------- ------------------------ 39.8/109.3 MB 816.8 kB/s eta 0:01:26\n",
      "   -------------- ------------------------ 40.1/109.3 MB 800.3 kB/s eta 0:01:27\n",
      "   -------------- ------------------------ 40.1/109.3 MB 800.3 kB/s eta 0:01:27\n",
      "   -------------- ------------------------ 40.6/109.3 MB 812.3 kB/s eta 0:01:25\n",
      "   -------------- ------------------------ 40.9/109.3 MB 819.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------ 40.9/109.3 MB 819.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------ 40.9/109.3 MB 819.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------ 41.2/109.3 MB 796.5 kB/s eta 0:01:26\n",
      "   -------------- ------------------------ 41.4/109.3 MB 800.1 kB/s eta 0:01:25\n",
      "   -------------- ------------------------ 41.4/109.3 MB 800.1 kB/s eta 0:01:25\n",
      "   -------------- ------------------------ 41.4/109.3 MB 800.1 kB/s eta 0:01:25\n",
      "   -------------- ------------------------ 41.4/109.3 MB 800.1 kB/s eta 0:01:25\n",
      "   -------------- ------------------------ 41.7/109.3 MB 770.0 kB/s eta 0:01:28\n",
      "   -------------- ------------------------ 41.9/109.3 MB 773.0 kB/s eta 0:01:28\n",
      "   -------------- ------------------------ 41.9/109.3 MB 773.0 kB/s eta 0:01:28\n",
      "   --------------- ----------------------- 42.2/109.3 MB 775.7 kB/s eta 0:01:27\n",
      "   --------------- ----------------------- 42.5/109.3 MB 783.4 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 42.5/109.3 MB 783.4 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 42.5/109.3 MB 783.4 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 42.5/109.3 MB 783.4 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 43.0/109.3 MB 773.7 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 43.3/109.3 MB 777.1 kB/s eta 0:01:25\n",
      "   --------------- ----------------------- 43.3/109.3 MB 777.1 kB/s eta 0:01:25\n",
      "   --------------- ----------------------- 43.3/109.3 MB 777.1 kB/s eta 0:01:25\n",
      "   --------------- ----------------------- 43.5/109.3 MB 750.6 kB/s eta 0:01:28\n",
      "   --------------- ----------------------- 43.5/109.3 MB 750.6 kB/s eta 0:01:28\n",
      "   --------------- ----------------------- 44.0/109.3 MB 765.1 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 44.0/109.3 MB 765.1 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 44.0/109.3 MB 765.1 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 44.0/109.3 MB 765.1 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 44.0/109.3 MB 765.1 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 44.6/109.3 MB 746.4 kB/s eta 0:01:27\n",
      "   --------------- ----------------------- 44.8/109.3 MB 751.2 kB/s eta 0:01:26\n",
      "   --------------- ----------------------- 44.8/109.3 MB 751.2 kB/s eta 0:01:26\n",
      "   ---------------- ---------------------- 45.1/109.3 MB 753.0 kB/s eta 0:01:26\n",
      "   ---------------- ---------------------- 45.3/109.3 MB 755.3 kB/s eta 0:01:25\n",
      "   ---------------- ---------------------- 45.3/109.3 MB 755.3 kB/s eta 0:01:25\n",
      "   ---------------- ---------------------- 45.6/109.3 MB 755.5 kB/s eta 0:01:25\n",
      "   ---------------- ---------------------- 45.9/109.3 MB 756.3 kB/s eta 0:01:24\n",
      "   ---------------- ---------------------- 46.1/109.3 MB 754.4 kB/s eta 0:01:24\n",
      "   ---------------- ---------------------- 46.1/109.3 MB 754.4 kB/s eta 0:01:24\n",
      "   ---------------- ---------------------- 46.1/109.3 MB 754.4 kB/s eta 0:01:24\n",
      "   ---------------- ---------------------- 46.7/109.3 MB 754.7 kB/s eta 0:01:23\n",
      "   ---------------- ---------------------- 46.9/109.3 MB 760.0 kB/s eta 0:01:23\n",
      "   ---------------- ---------------------- 46.9/109.3 MB 760.0 kB/s eta 0:01:23\n",
      "   ---------------- ---------------------- 47.2/109.3 MB 760.7 kB/s eta 0:01:22\n",
      "   ---------------- ---------------------- 47.4/109.3 MB 765.1 kB/s eta 0:01:21\n",
      "   ----------------- --------------------- 47.7/109.3 MB 768.8 kB/s eta 0:01:21\n",
      "   ----------------- --------------------- 48.0/109.3 MB 772.4 kB/s eta 0:01:20\n",
      "   ----------------- --------------------- 48.2/109.3 MB 776.6 kB/s eta 0:01:19\n",
      "   ----------------- --------------------- 48.2/109.3 MB 776.6 kB/s eta 0:01:19\n",
      "   ----------------- --------------------- 48.2/109.3 MB 776.6 kB/s eta 0:01:19\n",
      "   ----------------- --------------------- 48.2/109.3 MB 776.6 kB/s eta 0:01:19\n",
      "   ----------------- --------------------- 48.5/109.3 MB 762.6 kB/s eta 0:01:20\n",
      "   ----------------- --------------------- 48.8/109.3 MB 764.3 kB/s eta 0:01:20\n",
      "   ----------------- --------------------- 49.0/109.3 MB 769.5 kB/s eta 0:01:19\n",
      "   ----------------- --------------------- 49.0/109.3 MB 769.5 kB/s eta 0:01:19\n",
      "   ----------------- --------------------- 49.3/109.3 MB 768.8 kB/s eta 0:01:19\n",
      "   ----------------- --------------------- 49.5/109.3 MB 769.8 kB/s eta 0:01:18\n",
      "   ----------------- --------------------- 49.8/109.3 MB 771.4 kB/s eta 0:01:18\n",
      "   ----------------- --------------------- 49.8/109.3 MB 771.4 kB/s eta 0:01:18\n",
      "   ----------------- --------------------- 50.1/109.3 MB 773.4 kB/s eta 0:01:17\n",
      "   ----------------- --------------------- 50.1/109.3 MB 773.4 kB/s eta 0:01:17\n",
      "   ----------------- --------------------- 50.1/109.3 MB 773.4 kB/s eta 0:01:17\n",
      "   ----------------- --------------------- 50.3/109.3 MB 763.7 kB/s eta 0:01:18\n",
      "   ------------------ -------------------- 50.6/109.3 MB 764.2 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 50.6/109.3 MB 764.2 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 50.9/109.3 MB 763.6 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 50.9/109.3 MB 763.6 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 50.9/109.3 MB 763.6 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 50.9/109.3 MB 763.6 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 50.9/109.3 MB 763.6 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 50.9/109.3 MB 763.6 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 51.1/109.3 MB 731.7 kB/s eta 0:01:20\n",
      "   ------------------ -------------------- 51.6/109.3 MB 746.8 kB/s eta 0:01:18\n",
      "   ------------------ -------------------- 51.6/109.3 MB 746.8 kB/s eta 0:01:18\n",
      "   ------------------ -------------------- 51.6/109.3 MB 746.8 kB/s eta 0:01:18\n",
      "   ------------------ -------------------- 51.9/109.3 MB 739.3 kB/s eta 0:01:18\n",
      "   ------------------ -------------------- 52.2/109.3 MB 741.4 kB/s eta 0:01:18\n",
      "   ------------------ -------------------- 52.4/109.3 MB 744.8 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 52.4/109.3 MB 744.8 kB/s eta 0:01:17\n",
      "   ------------------ -------------------- 52.7/109.3 MB 746.8 kB/s eta 0:01:16\n",
      "   ------------------ -------------------- 53.0/109.3 MB 745.2 kB/s eta 0:01:16\n",
      "   ------------------ -------------------- 53.0/109.3 MB 745.2 kB/s eta 0:01:16\n",
      "   ------------------- ------------------- 53.5/109.3 MB 754.7 kB/s eta 0:01:14\n",
      "   ------------------- ------------------- 53.5/109.3 MB 754.7 kB/s eta 0:01:14\n",
      "   ------------------- ------------------- 53.7/109.3 MB 754.4 kB/s eta 0:01:14\n",
      "   ------------------- ------------------- 54.0/109.3 MB 756.2 kB/s eta 0:01:14\n",
      "   ------------------- ------------------- 54.3/109.3 MB 759.6 kB/s eta 0:01:13\n",
      "   ------------------- ------------------- 54.5/109.3 MB 762.6 kB/s eta 0:01:12\n",
      "   ------------------- ------------------- 54.8/109.3 MB 764.7 kB/s eta 0:01:12\n",
      "   ------------------- ------------------- 55.0/109.3 MB 768.0 kB/s eta 0:01:11\n",
      "   ------------------- ------------------- 55.0/109.3 MB 768.0 kB/s eta 0:01:11\n",
      "   ------------------- ------------------- 55.0/109.3 MB 768.0 kB/s eta 0:01:11\n",
      "   ------------------- ------------------- 55.3/109.3 MB 765.5 kB/s eta 0:01:11\n",
      "   ------------------- ------------------- 55.6/109.3 MB 767.9 kB/s eta 0:01:10\n",
      "   ------------------- ------------------- 55.8/109.3 MB 771.9 kB/s eta 0:01:10\n",
      "   -------------------- ------------------ 56.1/109.3 MB 775.4 kB/s eta 0:01:09\n",
      "   -------------------- ------------------ 56.4/109.3 MB 777.7 kB/s eta 0:01:09\n",
      "   -------------------- ------------------ 56.4/109.3 MB 777.7 kB/s eta 0:01:09\n",
      "   -------------------- ------------------ 56.9/109.3 MB 788.5 kB/s eta 0:01:07\n",
      "   -------------------- ------------------ 57.1/109.3 MB 792.6 kB/s eta 0:01:06\n",
      "   -------------------- ------------------ 57.4/109.3 MB 795.2 kB/s eta 0:01:06\n",
      "   -------------------- ------------------ 57.7/109.3 MB 795.2 kB/s eta 0:01:05\n",
      "   -------------------- ------------------ 57.9/109.3 MB 814.2 kB/s eta 0:01:04\n",
      "   -------------------- ------------------ 58.2/109.3 MB 817.1 kB/s eta 0:01:03\n",
      "   -------------------- ------------------ 58.5/109.3 MB 820.3 kB/s eta 0:01:02\n",
      "   -------------------- ------------------ 58.5/109.3 MB 820.3 kB/s eta 0:01:02\n",
      "   -------------------- ------------------ 58.7/109.3 MB 812.5 kB/s eta 0:01:03\n",
      "   -------------------- ------------------ 58.7/109.3 MB 812.5 kB/s eta 0:01:03\n",
      "   --------------------- ----------------- 59.0/109.3 MB 810.7 kB/s eta 0:01:03\n",
      "   --------------------- ----------------- 59.0/109.3 MB 810.7 kB/s eta 0:01:03\n",
      "   --------------------- ----------------- 59.2/109.3 MB 793.9 kB/s eta 0:01:04\n",
      "   --------------------- ----------------- 59.2/109.3 MB 793.9 kB/s eta 0:01:04\n",
      "   --------------------- ----------------- 59.2/109.3 MB 793.9 kB/s eta 0:01:04\n",
      "   --------------------- ----------------- 60.0/109.3 MB 809.8 kB/s eta 0:01:01\n",
      "   --------------------- ----------------- 60.3/109.3 MB 812.6 kB/s eta 0:01:01\n",
      "   --------------------- ----------------- 60.6/109.3 MB 801.0 kB/s eta 0:01:01\n",
      "   --------------------- ----------------- 61.1/109.3 MB 815.0 kB/s eta 0:01:00\n",
      "   --------------------- ----------------- 61.1/109.3 MB 815.0 kB/s eta 0:01:00\n",
      "   --------------------- ----------------- 61.3/109.3 MB 815.6 kB/s eta 0:00:59\n",
      "   --------------------- ----------------- 61.6/109.3 MB 826.4 kB/s eta 0:00:58\n",
      "   --------------------- ----------------- 61.6/109.3 MB 826.4 kB/s eta 0:00:58\n",
      "   ---------------------- ---------------- 61.9/109.3 MB 825.3 kB/s eta 0:00:58\n",
      "   ---------------------- ---------------- 62.1/109.3 MB 821.0 kB/s eta 0:00:58\n",
      "   ---------------------- ---------------- 62.4/109.3 MB 824.2 kB/s eta 0:00:57\n",
      "   ---------------------- ---------------- 62.7/109.3 MB 825.7 kB/s eta 0:00:57\n",
      "   ---------------------- ---------------- 62.7/109.3 MB 825.7 kB/s eta 0:00:57\n",
      "   ---------------------- ---------------- 62.7/109.3 MB 825.7 kB/s eta 0:00:57\n",
      "   ---------------------- ---------------- 62.9/109.3 MB 817.3 kB/s eta 0:00:57\n",
      "   ---------------------- ---------------- 62.9/109.3 MB 817.3 kB/s eta 0:00:57\n",
      "   ---------------------- ---------------- 63.4/109.3 MB 816.9 kB/s eta 0:00:57\n",
      "   ---------------------- ---------------- 63.4/109.3 MB 816.9 kB/s eta 0:00:57\n",
      "   ---------------------- ---------------- 63.7/109.3 MB 813.1 kB/s eta 0:00:57\n",
      "   ---------------------- ---------------- 64.0/109.3 MB 815.6 kB/s eta 0:00:56\n",
      "   ---------------------- ---------------- 64.0/109.3 MB 815.6 kB/s eta 0:00:56\n",
      "   ---------------------- ---------------- 64.0/109.3 MB 815.6 kB/s eta 0:00:56\n",
      "   ---------------------- ---------------- 64.2/109.3 MB 815.8 kB/s eta 0:00:56\n",
      "   ---------------------- ---------------- 64.2/109.3 MB 815.8 kB/s eta 0:00:56\n",
      "   ---------------------- ---------------- 64.2/109.3 MB 815.8 kB/s eta 0:00:56\n",
      "   ----------------------- --------------- 64.7/109.3 MB 815.6 kB/s eta 0:00:55\n",
      "   ----------------------- --------------- 64.7/109.3 MB 815.6 kB/s eta 0:00:55\n",
      "   ----------------------- --------------- 65.0/109.3 MB 808.1 kB/s eta 0:00:55\n",
      "   ----------------------- --------------- 65.3/109.3 MB 814.5 kB/s eta 0:00:55\n",
      "   ----------------------- --------------- 65.5/109.3 MB 820.8 kB/s eta 0:00:54\n",
      "   ----------------------- --------------- 65.8/109.3 MB 819.5 kB/s eta 0:00:54\n",
      "   ----------------------- --------------- 66.1/109.3 MB 824.4 kB/s eta 0:00:53\n",
      "   ----------------------- --------------- 66.1/109.3 MB 824.4 kB/s eta 0:00:53\n",
      "   ----------------------- --------------- 66.1/109.3 MB 824.4 kB/s eta 0:00:53\n",
      "   ----------------------- --------------- 66.1/109.3 MB 824.4 kB/s eta 0:00:53\n",
      "   ----------------------- --------------- 66.6/109.3 MB 830.1 kB/s eta 0:00:52\n",
      "   ----------------------- --------------- 66.6/109.3 MB 830.1 kB/s eta 0:00:52\n",
      "   ----------------------- --------------- 66.8/109.3 MB 826.6 kB/s eta 0:00:52\n",
      "   ----------------------- --------------- 66.8/109.3 MB 826.6 kB/s eta 0:00:52\n",
      "   ----------------------- --------------- 67.1/109.3 MB 832.6 kB/s eta 0:00:51\n",
      "   ----------------------- --------------- 67.1/109.3 MB 832.6 kB/s eta 0:00:51\n",
      "   ------------------------ -------------- 67.4/109.3 MB 831.4 kB/s eta 0:00:51\n",
      "   ------------------------ -------------- 67.4/109.3 MB 831.4 kB/s eta 0:00:51\n",
      "   ------------------------ -------------- 67.4/109.3 MB 831.4 kB/s eta 0:00:51\n",
      "   ------------------------ -------------- 68.2/109.3 MB 841.1 kB/s eta 0:00:49\n",
      "   ------------------------ -------------- 68.2/109.3 MB 841.1 kB/s eta 0:00:49\n",
      "   ------------------------ -------------- 68.4/109.3 MB 843.3 kB/s eta 0:00:49\n",
      "   ------------------------ -------------- 68.7/109.3 MB 845.9 kB/s eta 0:00:48\n",
      "   ------------------------ -------------- 69.2/109.3 MB 854.3 kB/s eta 0:00:47\n",
      "   ------------------------ -------------- 69.5/109.3 MB 858.1 kB/s eta 0:00:47\n",
      "   ------------------------ -------------- 69.7/109.3 MB 876.8 kB/s eta 0:00:46\n",
      "   ------------------------ -------------- 70.0/109.3 MB 879.7 kB/s eta 0:00:45\n",
      "   ------------------------- ------------- 70.3/109.3 MB 883.9 kB/s eta 0:00:45\n",
      "   ------------------------- ------------- 70.5/109.3 MB 884.9 kB/s eta 0:00:44\n",
      "   ------------------------- ------------- 70.8/109.3 MB 886.2 kB/s eta 0:00:44\n",
      "   ------------------------- ------------- 70.8/109.3 MB 886.2 kB/s eta 0:00:44\n",
      "   ------------------------- ------------- 71.0/109.3 MB 874.7 kB/s eta 0:00:44\n",
      "   ------------------------- ------------- 71.3/109.3 MB 872.8 kB/s eta 0:00:44\n",
      "   ------------------------- ------------- 71.6/109.3 MB 878.9 kB/s eta 0:00:43\n",
      "   ------------------------- ------------- 71.8/109.3 MB 883.0 kB/s eta 0:00:43\n",
      "   ------------------------- ------------- 72.1/109.3 MB 886.7 kB/s eta 0:00:42\n",
      "   ------------------------- ------------- 72.3/109.3 MB 887.2 kB/s eta 0:00:42\n",
      "   ------------------------- ------------- 72.6/109.3 MB 892.3 kB/s eta 0:00:42\n",
      "   -------------------------- ------------ 72.9/109.3 MB 895.5 kB/s eta 0:00:41\n",
      "   -------------------------- ------------ 72.9/109.3 MB 895.5 kB/s eta 0:00:41\n",
      "   -------------------------- ------------ 72.9/109.3 MB 895.5 kB/s eta 0:00:41\n",
      "   -------------------------- ------------ 72.9/109.3 MB 895.5 kB/s eta 0:00:41\n",
      "   -------------------------- ------------ 73.4/109.3 MB 886.2 kB/s eta 0:00:41\n",
      "   -------------------------- ------------ 73.4/109.3 MB 886.2 kB/s eta 0:00:41\n",
      "   -------------------------- ------------ 73.7/109.3 MB 884.4 kB/s eta 0:00:41\n",
      "   -------------------------- ------------ 73.9/109.3 MB 884.9 kB/s eta 0:00:40\n",
      "   -------------------------- ------------ 73.9/109.3 MB 884.9 kB/s eta 0:00:40\n",
      "   -------------------------- ------------ 73.9/109.3 MB 884.9 kB/s eta 0:00:40\n",
      "   -------------------------- ------------ 75.0/109.3 MB 894.5 kB/s eta 0:00:39\n",
      "   -------------------------- ------------ 75.5/109.3 MB 917.7 kB/s eta 0:00:37\n",
      "   --------------------------- ----------- 75.8/109.3 MB 920.3 kB/s eta 0:00:37\n",
      "   --------------------------- ----------- 75.8/109.3 MB 920.3 kB/s eta 0:00:37\n",
      "   --------------------------- ----------- 76.0/109.3 MB 919.4 kB/s eta 0:00:37\n",
      "   --------------------------- ----------- 76.0/109.3 MB 919.4 kB/s eta 0:00:37\n",
      "   --------------------------- ----------- 76.0/109.3 MB 919.4 kB/s eta 0:00:37\n",
      "   --------------------------- ----------- 76.3/109.3 MB 903.8 kB/s eta 0:00:37\n",
      "   --------------------------- ----------- 76.5/109.3 MB 912.6 kB/s eta 0:00:36\n",
      "   --------------------------- ----------- 76.5/109.3 MB 912.6 kB/s eta 0:00:36\n",
      "   --------------------------- ----------- 77.1/109.3 MB 917.5 kB/s eta 0:00:36\n",
      "   --------------------------- ----------- 77.3/109.3 MB 918.0 kB/s eta 0:00:35\n",
      "   --------------------------- ----------- 77.3/109.3 MB 918.0 kB/s eta 0:00:35\n",
      "   --------------------------- ----------- 77.6/109.3 MB 925.6 kB/s eta 0:00:35\n",
      "   --------------------------- ----------- 77.6/109.3 MB 925.6 kB/s eta 0:00:35\n",
      "   --------------------------- ----------- 77.9/109.3 MB 920.4 kB/s eta 0:00:35\n",
      "   --------------------------- ----------- 78.1/109.3 MB 919.4 kB/s eta 0:00:34\n",
      "   --------------------------- ----------- 78.4/109.3 MB 923.8 kB/s eta 0:00:34\n",
      "   ---------------------------- ---------- 78.6/109.3 MB 927.2 kB/s eta 0:00:34\n",
      "   ---------------------------- ---------- 78.9/109.3 MB 963.4 kB/s eta 0:00:32\n",
      "   ---------------------------- ---------- 79.2/109.3 MB 966.7 kB/s eta 0:00:32\n",
      "   ---------------------------- ---------- 79.4/109.3 MB 969.5 kB/s eta 0:00:31\n",
      "   ---------------------------- ---------- 79.7/109.3 MB 972.7 kB/s eta 0:00:31\n",
      "   ---------------------------- ---------- 80.0/109.3 MB 976.0 kB/s eta 0:00:31\n",
      "   ---------------------------- ---------- 80.2/109.3 MB 974.0 kB/s eta 0:00:30\n",
      "   ---------------------------- ---------- 80.2/109.3 MB 974.0 kB/s eta 0:00:30\n",
      "   ---------------------------- ---------- 80.2/109.3 MB 974.0 kB/s eta 0:00:30\n",
      "   ---------------------------- ---------- 80.2/109.3 MB 974.0 kB/s eta 0:00:30\n",
      "   ---------------------------- ---------- 80.2/109.3 MB 974.0 kB/s eta 0:00:30\n",
      "   ---------------------------- ---------- 81.0/109.3 MB 966.2 kB/s eta 0:00:30\n",
      "   ----------------------------- --------- 81.3/109.3 MB 969.9 kB/s eta 0:00:29\n",
      "   ----------------------------- --------- 81.5/109.3 MB 967.7 kB/s eta 0:00:29\n",
      "   ----------------------------- --------- 81.5/109.3 MB 967.7 kB/s eta 0:00:29\n",
      "   ----------------------------- --------- 81.5/109.3 MB 967.7 kB/s eta 0:00:29\n",
      "   ----------------------------- --------- 81.5/109.3 MB 967.7 kB/s eta 0:00:29\n",
      "   ----------------------------- --------- 82.3/109.3 MB 972.0 kB/s eta 0:00:28\n",
      "   ----------------------------- --------- 82.6/109.3 MB 969.9 kB/s eta 0:00:28\n",
      "   ----------------------------- --------- 82.8/109.3 MB 975.0 kB/s eta 0:00:28\n",
      "   ----------------------------- --------- 83.1/109.3 MB 978.1 kB/s eta 0:00:27\n",
      "   ----------------------------- --------- 83.6/109.3 MB 983.3 kB/s eta 0:00:27\n",
      "   ----------------------------- --------- 83.9/109.3 MB 988.4 kB/s eta 0:00:26\n",
      "   ------------------------------ -------- 84.1/109.3 MB 990.5 kB/s eta 0:00:26\n",
      "   ------------------------------ -------- 84.7/109.3 MB 993.6 kB/s eta 0:00:25\n",
      "   ------------------------------ -------- 84.9/109.3 MB 998.7 kB/s eta 0:00:25\n",
      "   ------------------------------- -------- 85.5/109.3 MB 1.0 MB/s eta 0:00:24\n",
      "   ------------------------------- -------- 85.7/109.3 MB 1.0 MB/s eta 0:00:24\n",
      "   ------------------------------- -------- 86.0/109.3 MB 1.0 MB/s eta 0:00:23\n",
      "   ------------------------------- -------- 86.2/109.3 MB 1.0 MB/s eta 0:00:23\n",
      "   ------------------------------- -------- 86.2/109.3 MB 1.0 MB/s eta 0:00:23\n",
      "   ------------------------------- -------- 86.2/109.3 MB 1.0 MB/s eta 0:00:23\n",
      "   ------------------------------- -------- 86.2/109.3 MB 1.0 MB/s eta 0:00:23\n",
      "   ------------------------------- -------- 87.3/109.3 MB 1.0 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 87.6/109.3 MB 1.0 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 87.8/109.3 MB 1.0 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 88.1/109.3 MB 1.0 MB/s eta 0:00:21\n",
      "   -------------------------------- ------- 88.1/109.3 MB 1.0 MB/s eta 0:00:21\n",
      "   -------------------------------- ------- 88.1/109.3 MB 1.0 MB/s eta 0:00:21\n",
      "   -------------------------------- ------- 88.9/109.3 MB 1.0 MB/s eta 0:00:21\n",
      "   -------------------------------- ------- 89.1/109.3 MB 1.0 MB/s eta 0:00:20\n",
      "   -------------------------------- ------- 89.4/109.3 MB 1.0 MB/s eta 0:00:20\n",
      "   -------------------------------- ------- 89.7/109.3 MB 1.0 MB/s eta 0:00:20\n",
      "   -------------------------------- ------- 89.9/109.3 MB 1.0 MB/s eta 0:00:19\n",
      "   -------------------------------- ------- 89.9/109.3 MB 1.0 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 90.4/109.3 MB 1.0 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 90.4/109.3 MB 1.0 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 90.7/109.3 MB 1.1 MB/s eta 0:00:18\n",
      "   --------------------------------- ------ 91.0/109.3 MB 1.1 MB/s eta 0:00:18\n",
      "   --------------------------------- ------ 91.5/109.3 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------------------------- ------ 92.0/109.3 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------------------------- ------ 92.3/109.3 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------------------------- ------ 92.5/109.3 MB 1.1 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 92.8/109.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 93.1/109.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 93.1/109.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 93.1/109.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 93.1/109.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 93.1/109.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 93.8/109.3 MB 1.1 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 93.8/109.3 MB 1.1 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 93.8/109.3 MB 1.1 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 94.6/109.3 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 94.9/109.3 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 95.4/109.3 MB 1.1 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 95.7/109.3 MB 1.1 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 95.9/109.3 MB 1.1 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 95.9/109.3 MB 1.1 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 96.5/109.3 MB 1.1 MB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 96.5/109.3 MB 1.1 MB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 97.0/109.3 MB 1.1 MB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 97.0/109.3 MB 1.1 MB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 97.5/109.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 97.5/109.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 97.5/109.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 97.8/109.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 98.0/109.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 98.3/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 98.6/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 98.6/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 98.8/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 99.1/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 99.1/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 99.1/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 99.1/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 99.4/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 99.4/109.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 99.6/109.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 100.4/109.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 100.7/109.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 100.7/109.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 100.9/109.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 101.4/109.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 101.7/109.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 102.0/109.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 102.2/109.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 102.2/109.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 102.2/109.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 102.2/109.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 103.0/109.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 103.3/109.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 103.5/109.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 103.8/109.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 104.1/109.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 104.3/109.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 104.3/109.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 104.3/109.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 104.3/109.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 104.9/109.3 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 104.9/109.3 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 105.6/109.3 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 105.6/109.3 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 106.2/109.3 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 106.2/109.3 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 106.2/109.3 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 106.2/109.3 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 106.2/109.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  107.0/109.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  107.2/109.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  107.5/109.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  107.5/109.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  107.7/109.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  107.7/109.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  107.7/109.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  108.3/109.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  108.3/109.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  108.3/109.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  108.8/109.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.0/109.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 109.3/109.3 MB 1.1 MB/s  0:01:20\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached pillow-12.0.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/38.6 MB 645.7 kB/s eta 0:00:59\n",
      "    --------------------------------------- 0.5/38.6 MB 645.7 kB/s eta 0:00:59\n",
      "    --------------------------------------- 0.8/38.6 MB 780.2 kB/s eta 0:00:49\n",
      "   - -------------------------------------- 1.0/38.6 MB 882.6 kB/s eta 0:00:43\n",
      "   - -------------------------------------- 1.0/38.6 MB 882.6 kB/s eta 0:00:43\n",
      "   - -------------------------------------- 1.0/38.6 MB 882.6 kB/s eta 0:00:43\n",
      "   - -------------------------------------- 1.6/38.6 MB 783.9 kB/s eta 0:00:48\n",
      "   - -------------------------------------- 1.8/38.6 MB 805.4 kB/s eta 0:00:46\n",
      "   - -------------------------------------- 1.8/38.6 MB 805.4 kB/s eta 0:00:46\n",
      "   -- ------------------------------------- 2.1/38.6 MB 809.8 kB/s eta 0:00:46\n",
      "   -- ------------------------------------- 2.1/38.6 MB 809.8 kB/s eta 0:00:46\n",
      "   -- ------------------------------------- 2.4/38.6 MB 758.2 kB/s eta 0:00:48\n",
      "   -- ------------------------------------- 2.6/38.6 MB 762.5 kB/s eta 0:00:48\n",
      "   -- ------------------------------------- 2.9/38.6 MB 787.7 kB/s eta 0:00:46\n",
      "   --- ------------------------------------ 3.1/38.6 MB 799.0 kB/s eta 0:00:45\n",
      "   --- ------------------------------------ 3.4/38.6 MB 821.8 kB/s eta 0:00:43\n",
      "   --- ------------------------------------ 3.7/38.6 MB 835.7 kB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 3.9/38.6 MB 848.0 kB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 3.9/38.6 MB 848.0 kB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 4.2/38.6 MB 864.8 kB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 4.5/38.6 MB 865.9 kB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 5.0/38.6 MB 912.3 kB/s eta 0:00:37\n",
      "   ----- ---------------------------------- 5.2/38.6 MB 926.6 kB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 5.5/38.6 MB 942.6 kB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 5.8/38.6 MB 949.6 kB/s eta 0:00:35\n",
      "   ------ --------------------------------- 6.0/38.6 MB 968.8 kB/s eta 0:00:34\n",
      "   ------ --------------------------------- 6.3/38.6 MB 976.9 kB/s eta 0:00:34\n",
      "   ------ --------------------------------- 6.6/38.6 MB 994.2 kB/s eta 0:00:33\n",
      "   ------- -------------------------------- 6.8/38.6 MB 986.8 kB/s eta 0:00:33\n",
      "   ------- -------------------------------- 7.1/38.6 MB 1.0 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 7.6/38.6 MB 1.0 MB/s eta 0:00:30\n",
      "   -------- ------------------------------- 7.9/38.6 MB 1.0 MB/s eta 0:00:30\n",
      "   -------- ------------------------------- 8.1/38.6 MB 1.1 MB/s eta 0:00:29\n",
      "   -------- ------------------------------- 8.4/38.6 MB 1.1 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 8.9/38.6 MB 1.1 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 9.2/38.6 MB 1.1 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 9.4/38.6 MB 1.1 MB/s eta 0:00:27\n",
      "   ---------- ----------------------------- 9.7/38.6 MB 1.1 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 10.0/38.6 MB 1.1 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 10.0/38.6 MB 1.1 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 10.7/38.6 MB 1.1 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 11.0/38.6 MB 1.1 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 11.3/38.6 MB 1.2 MB/s eta 0:00:24\n",
      "   ------------ --------------------------- 12.1/38.6 MB 1.2 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 12.3/38.6 MB 1.2 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 12.6/38.6 MB 1.2 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 13.1/38.6 MB 1.2 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 13.6/38.6 MB 1.2 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 13.9/38.6 MB 1.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 14.4/38.6 MB 1.3 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 14.9/38.6 MB 1.3 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 15.5/38.6 MB 1.3 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 16.0/38.6 MB 1.3 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 16.0/38.6 MB 1.3 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 16.8/38.6 MB 1.4 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 17.3/38.6 MB 1.4 MB/s eta 0:00:16\n",
      "   ------------------ --------------------- 17.6/38.6 MB 1.4 MB/s eta 0:00:16\n",
      "   ------------------ --------------------- 17.8/38.6 MB 1.4 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 18.6/38.6 MB 1.4 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 18.9/38.6 MB 1.4 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 19.4/38.6 MB 1.4 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 19.9/38.6 MB 1.4 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 20.4/38.6 MB 1.4 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 21.0/38.6 MB 1.5 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 21.8/38.6 MB 1.5 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 22.5/38.6 MB 1.5 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 23.1/38.6 MB 1.5 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 23.9/38.6 MB 1.6 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 24.6/38.6 MB 1.6 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 25.4/38.6 MB 1.6 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 26.2/38.6 MB 1.7 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 26.5/38.6 MB 1.7 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 27.3/38.6 MB 1.7 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 28.0/38.6 MB 1.7 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 28.6/38.6 MB 1.7 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 28.6/38.6 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 30.4/38.6 MB 1.8 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 31.2/38.6 MB 1.8 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 32.2/38.6 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 32.8/38.6 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 33.0/38.6 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 33.8/38.6 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 34.9/38.6 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 34.9/38.6 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 34.9/38.6 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 35.7/38.6 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 35.9/38.6 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 37.0/38.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.0/38.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 1.9 MB/s  0:00:20\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: threadpoolctl, setuptools, scipy, safetensors, regex, Pillow, networkx, MarkupSafe, joblib, scikit-learn, jinja2, torch, transformers, sentence-transformers\n",
      "\n",
      "   ----------------------------------------  0/14 [threadpoolctl]\n",
      "   ----------------------------------------  0/14 [threadpoolctl]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   -- -------------------------------------  1/14 [setuptools]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   ----- ----------------------------------  2/14 [scipy]\n",
      "   -------- -------------------------------  3/14 [safetensors]\n",
      "   ----------- ----------------------------  4/14 [regex]\n",
      "   ----------- ----------------------------  4/14 [regex]\n",
      "   ----------- ----------------------------  4/14 [regex]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   -------------- -------------------------  5/14 [Pillow]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   ----------------- ----------------------  6/14 [networkx]\n",
      "   -------------------- -------------------  7/14 [MarkupSafe]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ---------------------- -----------------  8/14 [joblib]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ------------------------- --------------  9/14 [scikit-learn]\n",
      "   ---------------------------- ----------- 10/14 [jinja2]\n",
      "   ---------------------------- ----------- 10/14 [jinja2]\n",
      "   ---------------------------- ----------- 10/14 [jinja2]\n",
      "   ---------------------------- ----------- 10/14 [jinja2]\n",
      "   ---------------------------- ----------- 10/14 [jinja2]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ------------------------------- -------- 11/14 [torch]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ---------------------------------- ----- 12/14 [transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ------------------------------------- -- 13/14 [sentence-transformers]\n",
      "   ---------------------------------------- 14/14 [sentence-transformers]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.3 Pillow-12.0.0 jinja2-3.1.6 joblib-1.5.2 networkx-3.5 regex-2025.10.23 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.3 sentence-transformers-5.1.2 setuptools-80.9.0 threadpoolctl-3.6.0 torch-2.9.0 transformers-4.57.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Attempting to resume incomplete download (33.0 MB/109.3 MB, attempt 1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e7c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ef099",
   "metadata": {},
   "source": [
    "# Load Docuemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b663ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://arxiv.org/pdf/2501.04040\"\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b281c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 0, 'page_label': '1'}, page_content='A Survey on Large Language Models with some Insights\\non their Capabilities and Limitations\\nAndrea Matarazzo\\nExpedia Group\\nItaly\\na.matarazzo@gmail.com\\nRiccardo Torlone\\nRoma Tre University\\nItaly\\nriccardo.torlone@uniroma3.it\\nAbstract\\nThe rapid advancement of artificial intelligence, particularly with the development of\\nLarge Language Models (LLMs) built on the transformer architecture, has redefined the\\ncapabilities of natural language processing. These models now exhibit remarkable perfor-\\nmance across various language-related tasks, such as text generation, question answering,\\ntranslation, and summarization, often rivaling human-like comprehension. More intrigu-\\ningly, LLMs have demonstrated emergent abilities extending beyond their core functions,\\nshowing proficiency in tasks like commonsense reasoning, code generation, and arithmetic.\\nThis survey paper explores the foundational components, scaling mechanisms, and\\narchitectural strategies that drive these capabilities. Emphasizing models like GPT and\\nLLaMA, we analyze the impact of exponential data and computational growth on LLM\\nperformance, while also addressing the trade-offs associated with scaling. We also ex-\\namine LLM applications across sectors, such as healthcare, finance, education, and law,\\nhighlighting their adaptability and potential to solve domain-specific challenges.\\nCentral to this work are the questions of how LLMs generalize across diverse tasks,\\nexhibit planning, and reasoning abilities, and whether these emergent abilities can be\\nsystematically elicited or enhanced. In particular, we provide some insights into the CoT\\n(Chain of Thought) and PoT (Plan of Thought) abilities within LLMs, focusing on how\\npre-training data influences their emergence. Additionally, we investigate LLM-modulo\\nframeworks that integrate external systems, allowing LLMs to handle complex, dynamic\\ntasks. By analyzing these factors, this paper aims to foster the ongoing discussion on the\\ncapabilities and limits of LLMs, promoting their responsible development and application\\nin novel and increasingly complex environments.\\n1\\narXiv:2501.04040v2  [cs.CL]  9 Feb 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 4\\n1.1 Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 Goals of the paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.3 Content and organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2 Large Language Models 6\\n2.1 Definition and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2 Scaling Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n2.3 Prominent Model Families . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.3.1 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.3.2 T5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.3.3 GPT Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.3.4 Llama . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.3.5 Gemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n2.3.6 Claude . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n2.4 Specialized Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n2.4.1 LLMs in Healthcare . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n2.4.2 LLMs in Finance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n2.4.3 LLMs in Education . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n2.4.4 LLMs in Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n2.4.5 LLMs in Scientific Research . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n3 Foundations of Large Language Models 42\\n3.1 Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n3.1.1 Unsupervised pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n3.1.2 Supervised pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n3.1.3 Semi-supervised pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n3.2 Data sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n3.2.1 General Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n3.2.2 Specialized Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n3.2.3 Commonly-used data sources. . . . . . . . . . . . . . . . . . . . . . . . . 48\\n3.3 Data preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n3.3.1 Quality Filtering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n3.3.2 Deduplication. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n3.3.3 Privacy reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n3.3.4 Tokenization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n3.4 LLM Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n3.4.1 Instruction Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n3.4.2 Alignment Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n3.5 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n3.5.1 Encoder-decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n3.5.2 Casual decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n3.5.3 Prefix decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\n3.5.4 Transformer Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n3.5.5 Emerging architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\n3.6 Tuning and Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\n3.6.1 Parameter-efficient model adaptation . . . . . . . . . . . . . . . . . . . . 75\\n3.6.2 Memory-efficient model adaptation . . . . . . . . . . . . . . . . . . . . . 80\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 2, 'page_label': '3'}, page_content='4 Utilization Strategies and Techniques 81\\n4.1 In-Context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n4.1.1 ICL strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n4.1.2 ICL performance and origins . . . . . . . . . . . . . . . . . . . . . . . . . 86\\n4.1.3 ICL future research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\\n4.2 Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\\n4.2.1 CoT strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\\n4.2.2 CoT performance and origins . . . . . . . . . . . . . . . . . . . . . . . . 93\\n4.3 Program-of-Thoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\\n4.4 Planning for complex tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\\n4.4.1 Commonsense knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\\n4.4.2 Prompt and code based planning . . . . . . . . . . . . . . . . . . . . . . 99\\n4.4.3 Plan generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\\n4.4.4 Feedback and plan refinement . . . . . . . . . . . . . . . . . . . . . . . . 108\\n4.4.5 LLM-modulo Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\\n4.5 Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 117\\n5 Testing the CoT Capabilities of LLMs 124\\n5.1 What is eliciting the Chain-of-Thought? . . . . . . . . . . . . . . . . . . . . . . 124\\n5.2 Empirical evidences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\\n5.3 Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\\n5.4 Examples of generated text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\\n6 Conclusions 150\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 3, 'page_label': '4'}, page_content='1 Introduction\\n1.1 Motivations\\nIn recent years, the field of artificial intelligence has witnessed an extraordinary transforma-\\ntion, fueled mainly by the development of Large Language Models (LLMs) based on the Trans-\\nformer architecture. These models, exemplified by OpenAIs GPT series and Metas LLaMA,\\nhave revolutionized how we approach natural language processing tasks, achieving comprehen-\\nsion, learning, and generation levels that were once considered unattainable. Their impressive\\nperformance spans a variety of tasks, including text generation, question answering, language\\ntranslation, and summarization, showcasing their potential in tackling intricate language chal-\\nlenges. Surprisingly, these models have also exhibited some abilities that go beyond their\\nprimary task of text generation, such as commonsense reasoning, code generation, arithmetic\\noperations, and other complex tasks in various domains.\\nSeveral key factors have driven the evolution of LLMs, most notably the exponential growth\\nin available data and computational resources. Indeed, on the one hand, social media platforms,\\ndigital libraries, and other sources have provided vast amounts of textual and multimedia\\ninformation, enabling LLMs to be trained on extensive and diverse datasets. On the other\\nhand, the availability of powerful GPUs, TPUs, and distributed computing frameworks has\\nmade it feasible to train models with billions, and even trillions, of parameters. Together, these\\ntwo factors have led LLMs to capture nuanced linguistic patterns, cultural context, and domain-\\nspecific knowledge, enhancing their ability to generate coherent, contextually appropriate, and\\nhighly versatile outputs.\\nHowever, with their increasing complexity and capabilities, these models have introduced\\nnew challenges and raised critical questions about their applicability, limitations, and potential\\nfor future development. Questions surrounding their ethical use and long-term impact not only\\nto the AI landscape but also to our own lives have become central to discussions about their\\nfuture. Addressing these concerns is critical as researchers and practitioners continue to explore\\nthe transformative possibilities that LLMs can offer.\\n1.2 Goals of the paper\\nThe goal of this paper is twofold.\\nWe first aim to provide an in-depth survey on LLMs and their applications, beginning\\nwith a foundational overview of their development, pre-training strategies, and architectural\\nvariations. This includes an examination of the progression from early language models to the\\nsophisticated architectures of LLMs, such as BERT, GPT, and Llama. In particular, we explore\\nthe concept of scaling laws, which have been instrumental in understanding how the size and\\ncomplexity of LLMs contribute to their performance and capabilities, as well as the trade-offs\\nand challenges associated with building increasingly larger and more powerful models. We will\\nalso investigate their application across various domains, such as healthcare, finance, education,\\nlaw, and scientific research. Each of these domains presents unique challenges and opportu-\\nnities for LLMs, highlighting the versatility and adaptability of these models. For instance,\\nin healthcare, LLMs have shown promise in assisting with clinical decision-making, while in\\nfinance, they are being utilized for tasks such as sentiment analysis and market prediction.\\nThe second objective of the present paper is to deepen some of the mechanisms that en-\\nable LLMs to perform tasks previously deemed impossible for machine learning systems. In\\nparticular, we will try to address some fundamental questions. How do these models learn and\\ngeneralize across tasks and domains? What are these emergent abilities, and how can they be\\nelicited? Which factors contribute to their development (e.g., model size, data, architecture)?\\nWhat are the inherent limitations of these models and how can they be addressed?\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 4, 'page_label': '5'}, page_content='The central motivation of this work is therefore to investigate the current capabilities\\nand boundaries of LLMs, focusing on their ability to generalize, plan, and execute tasks au-\\ntonomously.\\n1.3 Content and organization\\nBelow, is a summary of the paper organized by its structure.\\n Section 2 introduces LLMs, tracing their development from early statistical language\\nmodels to modern transformer-based architectures. It underscores the significant role of\\nthe scaling law in LLM development, where increasing model size, data volume, and com-\\nputational resources leads to substantial performance enhancements across a wide range\\nof language tasks. The section also illustrates prominent LLM families like BERT, T5,\\nGPT series, and LLaMA, highlighting their distinctive architectures, strengths, and con-\\ntributions to the advancement of natural language processing. Additionally, it emphasizes\\nthe transformative impact of LLMs across various domains, including healthcare, finance,\\neducation, law, and scientific research.\\n Section 3 focuses on the fundamental building blocks of LLMs, covering data preprocess-\\ning techniques, pre-training methodologies, and model adaptation strategies. It explores\\nvarious pre-training approaches, including unsupervised, supervised, and semi-supervised\\nlearning, emphasizing their impact on model performance and adaptability. The section\\nalso examines different data sources used in LLM training, categorizing them into gen-\\neral data like Web pages, books, and conversation text, specialized data such as scientific\\nliterature and code, and widely used datasets like Wikipedia, BookCorpus, and Com-\\nmonCrawl. It details the critical data preprocessing steps, such as quality filtering, data\\ncleaning, deduplication, and tokenization, and their role in preparing data for effective\\nLLM training. Moreover, it discusses model adaptation techniques like instruction tuning\\nand alignment tuning, which fine-tune models for specific tasks and align their behaviour\\nwith desired human values. Crucially, the section provides a comprehensive analysis of\\nthe Transformer architecture, the dominant framework for modern LLMs, detailing its\\ncomponents (encoder, decoder, self-attention mechanisms), normalization methods, acti-\\nvation functions, positional embeddings, and optimization strategies.\\n Section 4 addresses the effective strategies and techniques for utilizing LLMs, emphasizing\\nin-context learning (ICL), chain-of-thought prompting (CoT), and planning capabilities.\\nIt explains ICL as a unique prompting technique that empowers LLMs to learn from\\nexamples presented within the prompt, allowing them to tackle new tasks without re-\\nquiring explicit gradient updates. It elaborates on various ICL strategies, such as demon-\\nstration design, prompt engineering, and the selection of appropriate scoring functions,\\nwhile also exploring the factors influencing ICL performance. It then introduces CoT\\nprompting as a powerful method for enhancing LLM reasoning abilities. This involves\\nintegrating intermediate reasoning steps within the prompt, guiding the model to adopt\\na structured thought process, particularly beneficial for tasks requiring logical deduction,\\nproblem-solving, and mathematical calculations. Finally, the section explores the plan-\\nning capabilities of LLMs, focusing on prompt-based planning. This technique involves\\ndecomposing complex tasks into manageable sub-tasks and generating a plan of action\\nfor execution. Different planning approaches, including text-based and programmatic\\nmethods, are discussed and the critical role of feedback and plan refinement mechanisms\\nin achieving successful plan execution is highlighted.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 5, 'page_label': '6'}, page_content=' Section 5 investigates the origins of CoT capabilities in LLMs, exploring the hypothesis\\nthat the presence of code in pre-training data may contribute to the emergence of these\\nreasoning abilities. For this, it presents empirical evidence obtained from experiments\\nconducted on publicly available Llama family models using LMStudio software on the\\nHuggingFace platform. The analysis focuses on the performance of these models on rea-\\nsoning tasks derived from the GSM8k and gsm-hard datasets, evaluating their capabilities\\nin utilizing CoT and Program of Thought (PoT) approaches.\\n Finally, section 6 summarizes the key points of the paper, reiterating the transformative\\npotential of LLMs across diverse fields. It also acknowledges the existing ethical, technical,\\nand practical challenges associated with LLM development and advocates for continued\\nresearch to ensure their responsible and beneficial application in the future.\\n2 Large Language Models\\n2.1 Definition and Overview\\nAt their core, LLMs are designed to comprehend, learn, and generate coherent and contextually\\nrelevant language on an unparalleled scale.\\nHistorically, the development of Language Models (LMs) has been rooted in the quest to\\nunderstand and replicate human language, and four main stages can be identified:\\n1. Statistical Language Models: These models were developed to capture the statistical\\nproperties of language, such as word frequencies and co-occurrences, to predict the like-\\nlihood of a given sequence of words based on the Markov assumption, which states that\\nthe probability of a word depends only on the previous n words. If the context length n\\nis fixed, the model is called an n-gram model.\\nHowever, these models are limited by the exponential number of transition probabilities\\nto be estimated and the Markov assumption 1, which may not always hold true in the\\ncomplexity of natural languages. Language understanding often involves capturing de-\\npendencies over longer distances than the Markov assumption allows. Models considering\\nbroader contexts, such as recurrent neural networks (RNNs) and transformers, have been\\ndeveloped to address these long-range dependencies in language processing tasks.\\n2. Neural Language Models: The advent of neural networks led to the development of\\nlanguage models that utilised neural architectures to capture languages complex pat-\\nterns and dependencies. These models, such as recurrent neural networks (RNNs) and\\nlong short-term memory (LSTM) networks, could capture long-range dependencies and\\ncontextual information, enabling them to generate coherent and contextually relevant\\ntext. Bengio et al. [6] introduced the concept of distributed representation of words and\\nbuilt the word prediction function of the distributed word vectors. Later, word2vec [21,\\n22] introduced the word2vec model, a shallow, two-layer neural network trained to recon-\\nstruct the linguistic contexts of words. These models were a significant leap forward in the\\ndevelopment of language models, representing a shift from word sequencing to learning\\nrepresentation.\\n1The Markov assumption proposes that the future state of a process relies solely on the current state by\\ndisregarding the journey to the current state. Mathematically, it is expressed in terms the conditional probability\\nP(St)  the likelihood of an event occurring given the past states  as P(St+1|St, St1 . . . , S1) = P(St+1|St),\\nwhere St is the state at time t. The Markov assumption simplifies the modelling process by reducing the number\\nof parameters to estimate.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 6, 'page_label': '7'}, page_content='3. Pre-trained language models (PLM): The development of pre-trained language mod-\\nels (PLMs) marked a significant milestone in the evolution of language models. These\\nmodels were trained on large data corpora in an unsupervised or self-supervised manner\\nbefore being fine-tuned on specific tasks. The idea is to pre-train a model on a diverse\\ndata set and then transfer its knowledge to a narrower task by fine-tuning it on a smaller,\\ntask-specific dataset. ELMo 2 [50] was one of the first PLMs which used a bidirectional\\nLSTM to generate word embeddings instead of learning fixed word representations. Devlin\\net al. [65] introduced BERT (Bidirectional Encoder Representations from Transformers),\\na transformer-based model pre-trained on a large corpus of text and then fine-tuned it on\\nspecific tasks. BERT was a significant advancement in natural language processing, as\\nit demonstrated the potential of pre-trained language models to achieve state-of-the-art\\nperformance on a wide range of tasks. These studies introduced the pre-training and\\nfine-tuning paradigm, which has become a standard practice in the development of lan-\\nguage models and inspired a significant number of models, such as GPT-2 [75]), GPT-3\\n(Brown et al. [88]), T5 (Raffel et al. [99], and many others.\\n4. Large Language Models (LLM): The emergence of large language models, charac-\\nterised by their immense scale and complexity, has redefined the capabilities of language\\nprocessing systems. Studies find that language models performance improves as the\\nnumber of parameters (e.g., model size) or data size increases, a phenomenon known as\\nthe scaling law in large language models. Many LLMs are built on the transformer ar-\\nchitecture, designed to capture long-range dependencies and contextual information in\\nlanguage. The transformer architecture has become the foundation for many state-of-\\nthe-art language models. Unlike earlier models that were unidirectional (e.g., traditional\\nRNNs), LLMs, especially those based on transformers, are bidirectional. They consider\\nthe context of preceding and following words, enhancing their language understanding.\\nLLMs find applications across various domains, including but not limited to:\\n Text Generation: Producing coherent and contextually relevant text.\\n Question Answering: Answering questions based on provided context.\\n Language Translation: Translating text from one language to another.\\n Summarization: Creating concise summaries of longer texts.\\n Sentiment Analysis: Determining the sentiment expressed in a text.\\nThese large-sized PLMs have been shown to outperform their smaller (e.g., 330M-parameters\\nvs 1.5B-parameters) and show surprising capabilities 3, also called emergent abilities by\\nWei et al. [232].\\nEmergence is when quantitative changes in a system result in qualitative changes\\nin behavior [1].\\nThese emergent abilities include but are not limited to, the ability to perform tasks for\\nwhich they were not explicitly trained, such as translation, summarisation, and question-\\nanswering, and to generalise to new tasks and domains, such as zero-shot learning 4,\\n2Embeddings from Language Models\\n3Note that a LLM is not necessarily more capable than a small PLM, and emergent abilities may not occur\\nin some LLMs.\\n4It refers to a machine learning scenario where a model makes predictions or performs tasks for classes or\\nexamples it has never seen during training\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 7, 'page_label': '8'}, page_content='Figure 1:Two examples of in-context learning, where a language model (LM) is given a list of training\\nexamples (black) and a test input (green) and asked to make a prediction (orange) by predicting the\\nnext tokens/words to fill in the blank. Source: Lab [288].\\nfew-shot learning 5, and even one-shot 6 learning7. Three typical examples of emergent\\nabilities are:\\n(a) In-context learning: this ability has been formally observed in GPT-3, which is\\nprovided with a natural language instruction or task demonstrations; it can generate\\nthe expected output for test instances by completing the word sequence of the input\\ntext (as shown in Figure 1). Importantly, this can be achieved without requiring\\nadditional training or gradient updates 8. The surprising fact is that the LM isnt\\ntrained to learn from examples. Because of this, theres seemingly a mismatch\\nbetween pretraining (what its trained to do, which is next token prediction) and\\nin-context learning (what were asking it to do).\\n(b) Instruction following: Through the process called instruction tuning  that we\\nwill see more in-depth in Section 3.4.1  LLMs exhibit strong performance on un-\\nseen tasks described through natural language instructions [209, 205, 231]. This\\napproach involves fine-tuning the model using diverse multitask datasets, each ac-\\ncompanied by detailed natural language descriptions. The result is an LLM that\\neffectively interprets and follows instructions for new and unseen tasks without rely-\\ning on explicit examples. Experiments detailed in Wei et al. [231] demonstrate that\\nLaMDA-PT, fine-tuned with instructions, begins to outperform its untuned coun-\\nterpart significantly when the model size reaches 68 billion parameters. However,\\nthis performance gain is not observed for 8 billion or smaller model sizes. Further-\\nmore, Chung et al. [156] highlights that a model size of at least 62 billion parameters\\nis necessary for PaLM to excel across various tasks in evaluation benchmarks like\\nMMLU, BBH, TyDiQA, and MGSM. Nevertheless, it is noted that certain specific\\ntasks, such as MMLU, might suffice with much smaller model size, emphasising the\\nnuanced relationship between model size and task performance.\\n5It involves training a model with a minimal number of examples per class, usually much fewer than what\\ntraditional machine learning models require\\n6It is a specific case of few-shot learning where the model is trained with only one example per class\\n7A shot is an example or demonstration of what type of prompt and response you expect from a large\\nlanguage model. This term originates from training computer vision models on photographs, where one shot\\nwas one example or instance that the model used to classify an image [12].\\n8Dai et al. [158] shows that in-context learning implicitly performs meta optimisation through the attention\\nmechanism\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 8, 'page_label': '9'}, page_content='(c) Step-by-step reasoning: For small LMs, it is usually difficult to solve com-\\nplex tasks that involve multiple reasoning steps (e.g., mathematical word problems).\\nIn contrast, the chain-of-thought (CoT) prompting strategy [230] empowers Large\\nLanguage Models (LLMs) to surmount these challenges. By leveraging the CoT\\nprompting mechanism, which involves intermediate reasoning steps to derive the\\nfinal solution, LLMs exhibit proficiency in tasks requiring intricate cognitive pro-\\ncesses. This capability is speculated to be honed through training on code by Wei\\net al. [230]. Authors demonstrate that the employment of CoT prompting yields\\nperformance gains, particularly on arithmetic reasoning benchmarks, when applied\\nto variants of models like PaLM and LaMDA, especially with a model size surpass-\\ning 60B. The advantages of CoT prompting become more pronounced as the model\\nsize exceeds 100B. Furthermore, the effectiveness of CoT prompting exhibits vari-\\nability across different tasks, with performance improvement observed in the order\\nof GSM8k > MAWPS > SWAMP for PaLM [230]. Recent studies have shown that\\nsize is not a deciding factor in the models ability to perform step-by-step reasoning\\ntasks. We will investigate this further in Section 4.2.2.\\n5. Small Language Models: Small Language Models (SLMs) are a rapidly emerging\\nsubset of artificial intelligence designed to provide efficient natural language processing\\n(NLP) capabilities. As outlined in IBMs analysis, SLMs operate with a fraction of\\nthe parameters used by large language models (LLMs), ranging from a few million to\\nseveral billion parameters. This reduction in size allows them to function in resource-\\nconstrained environments such as edge devices, mobile platforms, and offline scenarios,\\nwhere computational resources and connectivity may be limited. SLMs, like their larger\\ncounterparts, leverage a transformer architecture. To reduce model size while retaining\\nfunctionality, model compression techniques are applied. These include:\\n(a) Pruning: Eliminating redundant parameters from neural networks to simplify com-\\nputations while preserving core performance.\\n(b) Quantization: Representing model weights and activations in lower precision (e.g.,\\n8-bit integers) to improve speed and reduce memory usage.\\n(c) Low-Rank Factorization: Decomposing weight matrices into simpler approximations\\nto lower computational demands.\\n(d) Knowledge Distillation: Transferring knowledge from larger teacher models to\\nsmaller student models, enabling compact versions to retain critical features.\\nA wide range of SLMs are gaining traction due to their adaptability and efficiency.\\nSome notable examples include DistilBERT, Google Gemma, Minstral and others. SLMs\\nare particularly suited to scenarios where computational efficiency and adaptability are\\nparamount, such as edge computing, mobile applications, and offline seetings. The de-\\nvelopment of Small Language Models marks a transformative step in AI, emphasizing\\nefficiency and accessibility without sacrificing core capabilities. As model compression\\ntechniques continue to evolve, SLMs are poised to play a crucial role in shaping the\\nfuture of AI deployment across diverse domains.\\nThe advent of LLMs has led to a paradigm shift in the field of natural language processing,\\nwith applications ranging from machine translation to text summarisation and from question-\\nanswering systems to language generation. The development of LLMs has been driven by the\\nexponential growth of data and computational resources, which has enabled the training of\\nmodels with billions of parameters. The scale of these models has enabled them to capture\\ncomplex patterns in language and generate coherent and contextually relevant text.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 9, 'page_label': '10'}, page_content='The potential of LLMs is vast, and their impact on natural language processing is profound.\\nThe advent of ChatGPT [86] and GPT-4 [370] has further expanded the capabilities of LLMs,\\nleading to the rethinking of the possibilities of artificial general intelligence (AGI).\\nRegarding NLP, LLMs can serve somewhat as general-purpose language task solvers. In\\nthe IR field, LLMs can be used to improve the performance of information retrieval systems\\nthrough AI chatbots (i.e., ChatGPT), or integrating search engines like the New Bing 9 or\\nusing RAG10 [375] pipelines. RAG addresses these challenges by combining LLMs with exter-\\nnal knowledge bases. This integration allows models to retrieve relevant information during\\ngeneration, enhancing accuracy and credibility.\\nIn the CV field, LLMs can be used to improve the performance of computer vision systems\\nthrough multimodal models 11 (i.e., CLIP12 [130] and DALL-E [132]).\\nThis work will mainly focus on model sizes larger than 10B parameters to explore their\\ncapabilities, limitations, and potential applications. We will delve into the emergent abilities of\\nLLMs, such as in-context learning, instruction following, and step-by-step reasoning, and how\\nthese abilities can be leveraged to solve complex tasks in Section 4. The study will investigate\\nand compare the abilities of different LLMs, focusing on the impact of various parameters on\\ntheir performance.\\nLLMs are not without challenges, including ethical concerns, environmental impact, and\\nthe potential for bias and hallucination in generated text.\\n2.2 Scaling Law\\nThe Scaling Law in LLMs constitutes a fundamental principle underlining their development\\nand performance. At its essence, the scaling law posits that as language models increase in size,\\ntheir capabilities and performance on linguistic tasks exhibit disproportionately positive growth.\\nThis concept has become a guiding force in pushing the boundaries of language processing and\\nunderstanding.\\nAs LLMs scale up in terms of parameters, encompassing tens or hundreds of billions, or even\\ntrillions, they demonstrate an unprecedented ability to generalise from diverse datasets and\\ngenerate contextually coherent text. The essence of the scaling law lies in the direct correlation\\nbetween the size of a language model and the number of parameters it encompasses. Parameters\\nare the internal variables the model learns during training, representing the connections and\\nweights defining its understanding of language. As the number of parameters increases, so does\\nthe models capacity to encapsulate complex linguistic structures.\\nOne primary outcome of adhering to the scaling law is the substantial improvement in per-\\nformance across a spectrum of language-related tasks. From language generation to sentiment\\nanalysis, question-answering, and summarization, larger models consistently outperform their\\nsmaller counterparts. The increased capacity for learning intricate language features enables\\nLLMs to excel in understanding and producing more human-like text.\\nWhen writing, most of the LLMs are based on the transformer architecture, where multi-\\nheaded self-attention layers are stacked in a very deep neural network. Well dive deep into\\nthe transformer architecture in Section 3.5.4, but for now, we can say that self-attention is\\na mechanism that allows a model to weigh different parts of the input sequence differently,\\ncapturing dependencies between words. The multi-headed self-attention mechanism lets the\\nmodel capture different dependencies and relationships between words, enhancing language\\n9https://www.microsoft.com/it-it/bing?form=MA13FV\\n10Retrieval-Augmented Generation\\n11Models are designed to process and understand information from multiple modalities or sources (e.g., text,\\nimage, audio, video). Multimodal models aim to handle and integrate data from two or more modalities.\\n12Contrastive LanguageImage Pre-training\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 10, 'page_label': '11'}, page_content='understanding. The idea is that different attention heads can focus on different aspects or\\nrelationships within the data, allowing the model to capture more nuanced patterns. Multi-\\nple layers of these multi-headed self-attention mechanisms are stacked in a very deep neural\\nnetwork. Each layer in the stack processes the previous layers output, learning hierarchical\\nrepresentations of the input data and capturing increasingly complex relationships and abstrac-\\ntions.\\nTwo representative scaling laws for Transformer-based LLMs are the following [93, 172]:\\n1. KM scaling law: named in this way in Zhao et al. [364] and proposed by the OpenAI\\nteam in Kaplan et al. [93]. Given model size M, dataset size D, amount of training\\ncompute C, and a compute budget c, the KM scaling law states that the performance of\\na language model scales as per the following three formulas:\\nL(N) = (Nc\\nN )N , N  0.076, Nc  8.8  1013\\nL(D) = (Dc\\nD )D , D  0.095, Dc  5.4  1013\\nL(C) = (Cc\\nC )C , C  0.050, Cc  3.1  108\\n(1)\\nwhere L(N), L(D), and L(C) denote the cross-entropy loss of the model, the dataset,\\nand the amount of training computed, respectively. The three laws were formulated by\\nanalysing the models performance across a range of data sizes (from 22M to 23B tokens),\\nmodel sizes (from 768M to 1.5B non-embedding parameters), and training compute, with\\ncertain assumptions (e.g., ensuring that the other two factors do not constrain the analysis\\nof one factor). The findings demonstrated a robust interdependence among the three\\nfactors influencing model performance.\\n2. Chinchilla scaling law: An alternative form of the scaling law has been proposed by\\nthe Google DeepMind team in Hoffmann et al. [172] experimenting with an extensive\\nrange of model size (70M to 16B) and data sizes (5B to 500B tokens). The Chinchilla\\nscaling law posits that the performance of a language model scales as per the following\\nformula:\\nL(N, D) = E + A\\nN + B\\nD , (2)\\nwhere E = 1.69, A= 406.4, B= 410.7, = 0.34, = 0.28\\nAuthors showed that optimal allocation of compute budget to model size and data size\\ncan be derived as follows 13:\\nNopt(C) = G(C\\n6 )a, Dopt(C) = G1(C\\n6 )b, (3)\\nwhere a = \\n+ , b= \\n+ and G is a scaling coefficient. The KM scaling law favours a more\\nsignificant budget allocation in model size than the data size. In contrast, the Chinchilla\\nscaling law argues that the two sizes should be increased in equal scales [172] (i.e., having\\nsimilar values for a and b in (3)).\\n13under the constraint C  6ND\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 11, 'page_label': '12'}, page_content='Scaling boosts performance and addresses inherent limitations in smaller language models.\\nLarger models excel in managing long-range dependencies, comprehending ambiguous language\\nconstructs, and displaying a nuanced understanding of contextcapabilities that smaller mod-\\nels frequently find challenging. The eliciting of emergent abilities, such as Chain-of-Thought\\nprompting and in-context learning, have shown a phase change in the first Scaling Law, where\\nthe performance increases linearly as the model size increases exponentially (Figure 2). Emer-\\ngency is still a debated topic: Schaeffer, Miranda, and Koyejo [318] shows that different metrics\\ncan reveal continuous improvement in LLM performance, challenging the concept of emergent\\nabilities14, while others argue that the unpredictability of when and which metrics show abrupt\\nimprovement still supports the idea of emergence. While the study provides valuable insights,\\nresearchers agree that discontinuities and jump-like improvements in model performance still\\nexist as model size increases.\\nAt its core, the scaling law is a guiding principle in the development of LLMs, directing the\\nallocation of resources and the design of models to maximise performance and capabilities.\\nFigure 2: Left: scaling law. Model performance increases linearly as the model size increases ex-\\nponentially. Right: emergent abilities show a phase change at a certain scale where the performance\\nsuddenly increases. Source: Fu [267].\\nDespite propelling the field of LLMs to new heights, the scaling law comes with computa-\\ntional challenges. Training huge models requires significant computational resources, encom-\\npassing processing power and memory. The computational budget is an upper bound limit,\\ndemanding innovations in hardware and distributed training techniques to exploit the potential\\nof scaled-up language models fully.\\n2.3 Prominent Model Families\\nThe development of Large Language Models (LLMs) has been driven by the emergence of\\nprominent model families, each characterised by its unique architecture and capabilities. These\\nmodel families have played a pivotal role in shaping the landscape of language processing and\\nunderstanding and have been instrumental in pushing the boundaries of LLMs.\\nSome of the most prominent large language models (having a size larger than 10B) are\\ndepicted in Figure 3.\\n2.3.1 BERT\\nIntroduced by Google in 2018, BERT [65] marked a significant evolution in LLMs by focus-\\ning on bidirectional context in text processing. BERTs model architecture is a multi-layer\\n14Choosing a different metric its possible to show that increasing the model size, leads to an improvement in\\ncorrect sequences prediction in addition problems. Looking at these metrics, the add ability is not emergent\\nbut gradual and predictable.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 12, 'page_label': '13'}, page_content='Figure 3: A diagram showing the evolution of publicly available LLMs. Source: Zhao et al. [364].\\nbidirectional Transformer encoder based on the original transformer architecture introduced\\nby Vaswani et al. [334]. Unlike its predecessors, BERT analyses text in both directions (left-\\nto-right and right-to-left), providing a more nuanced understanding of language context. This\\nbi-directionality enables BERT to achieve state-of-the-art results in various NLP tasks, such as\\nquestion answering, named entity recognition, and sentiment analysis. BERTs architecture and\\ntraining methodology have influenced numerous subsequent models and research initiatives [65].\\nFigure 4: BERT Architecture: The bottom layer contains the embedding representations\\nE1, E2, . . . EN , which encode input tokens and serve as the input to the transformer layers (Trm).\\nEach transformer bidirectionally processes the input embeddings, and the final output is used for down-\\nstream tasks. Source: Devlin et al. [65].\\nEven BERT is built on the transformer architecture [334], which relies heavily on attention\\nmechanisms to understand the context of words in a sentence. The innovation in BERT is its\\nbidirectional nature and the use of a mechanism called the Masked Language Model (MLM).\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 13, 'page_label': '14'}, page_content='In MLM, some percentage of the input tokens are randomly masked, and the objective is to\\npredict these masked tokens based on their context, leveraging information from both sides of\\nthe sequence. BERT also incorporates a next-sentence prediction (NSP) task that helps the\\nmodel learn relationships between sentences, further enhancing its understanding of context.\\nBERTs bidirectional context understanding significantly improves its performance on vari-\\nous NLP tasks, including sentiment analysis, question answering, and named entity recognition.\\nBy pre-training on a large corpus of text and then fine-tuning on specific tasks, BERT can adapt\\nto various domains with relatively little task-specific data, demonstrating impressive transfer\\nlearning capabilities. Its architecture has set a new standard in the field, inspiring many sub-\\nsequent models that build on or modify its foundational structure.\\nDespite its strengths, BERT is not without limitations. The models size and complexity\\nrequire substantial computational resources for training, which can be a barrier for some or-\\nganisations or researchers. BERTs focus on context from surrounding text does not inherently\\nsolve all challenges in language understanding, particularly concerning ambiguity, nuance, or\\nthe subtleties of human language. The model can sometimes struggle with tasks requiring\\nextensive world knowledge or reasoning beyond the scope of its training data.\\nWhile BERT itself does not exhibit emergent abilities in the same way that scaling up GPT\\nmodels does, its architecture has enabled new approaches to handling context and language\\nunderstanding that were not feasible with prior models. Subsequent iterations and variations\\nof BERT, like RoBERTa15 and ALBERT16, have sought to optimise and expand upon BERTs\\nfoundational principles, exploring how changes in model size, training methodology, and archi-\\ntecture can influence performance and capabilities.\\n2.3.2 T5\\nDeveloped by Google in 2019, T5 17 re-framed all NLP tasks as a unified text-to-text problem,\\nwhere every task is cast as generating text from input text. This approach simplifies using a\\nsingle model across diverse tasks, encouraging a more generalised understanding of language.\\nFigure 5: A diagram of the T5 text-to-text framework. Every task  including translation, question\\nanswering, and classification  is cast as feeding the model text as input and training it to generate\\nsome target text. This approach allows the same model, loss function, hyperparameters, etc., to be\\nused across diverse tasks. Source: Raffel et al. [99].\\nT5 demonstrated its prowess across a range of benchmarks, setting new standards in the field\\n15Robustly Optimized BERT Pre-training Approach\\n16A Lite BERT\\n17Text-to-Text Transfer Transformer\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 14, 'page_label': '15'}, page_content='of NLP [99]. Its built on the transformer model, similar to its predecessors, BERT and GPT.\\nIt leverages the effective self-attention mechanism for processing data sequences. The model is\\ndesigned to handle various tasks without needing task-specific architectural modifications. It\\nuses a unified text-to-text framework, where tasks are converted into a format where the input\\nand output are always text strings. T5 is pre-trained on a multitask mixture of unsupervised\\nand supervised tasks, utilising a large-scale dataset known as C4 18.\\nT5s approach simplifies integrating new tasks into the models training regime, as they only\\nneed to be reformulated into the text-to-text format. While T5s unified approach offers consid-\\nerable advantages, it might not be optimal for all types of tasks. Some tasks could potentially\\nbenefit from more specialised model architectures or formats. The training process for T5 is\\nresource-intensive, requiring substantial computational power, which could be a limiting factor\\nfor smaller organisations or independent researchers. As with other large language models, T5s\\noutputs can sometimes include biases in the training data, necessitating careful monitoring and\\npotential post-hoc adjustments.\\n2.3.3 GPT Series\\nDeveloped by OpenAI, the GPT series has been at the forefront of LLM research. The original\\nGPT model, introduced in 2018, laid the groundwork with its transformer-based architecture,\\nsignificantly improving previous models understanding of context and generating text. It was\\ndeveloped based on a generative, decoder-only Transformer architecture, and it adopted a\\nhybrid approach of unsupervised pre-training and supervised fine-tuning.\\nGPT-2 [75], released in 2019, expanded on this with 1.5 billion parameters and was trained\\nwith a large webpage dataset, WebText, demonstrating unprecedented text generation capabil-\\nities.\\nThe subsequent GPT-3 model, unveiled in 2020, further pushed the boundaries with 175\\nbillion parameters, showcasing remarkable abilities in generating human-like text, perform-\\ning language translation, question-answering, and more without task-specific training. In the\\nresearch paper on GPT-3 [88], the authors explained the concept known as in-context learn-\\ning (ICL). This approach enables Large Language Models (LLMs) to function in few-shot or\\nzero-shot scenarios. ICL empowers LLMs to comprehend tasks when they are described using\\nnatural language. This method aligns LLMs pre-training and application phases under a uni-\\nfied framework. During pre-training, the model predicts subsequent text sequences based on\\nthe prior context. In contrast, during in-context learning, the model generates the appropri-\\nate solution to a task in the form of a text sequence using the provided task instructions and\\nexamples.\\nThe GPT series is based on the transformer architecture by Vaswani et al. [334]. This\\narchitecture leverages self-attention mechanisms to process input data, which allows the model\\nto weigh the importance of different words within the input context, enhancing its ability to\\nunderstand and generate language. GPT models are characterized by their stacked trans-\\nformer blocks, which consist of multi-headed self-attention layers followed by fully connected\\nfeed-forward neural networks. The series has seen an exponential increase in the number of\\nparameters: GPT with 110 million, GPT-2 with 1.5 billion, and GPT-3 with 175 billion pa-\\nrameters.\\nGPT models exhibit a remarkable ability to generate coherent and contextually relevant\\ntext, simulating human-like writing styles. They demonstrate strong performance in a wide\\narray of NLP tasks without task-specific data training, showcasing their versatility in few-shot,\\none-shot, or zero-shot learning scenarios. The architectures scalability has shown that larger\\nmodels tend to exhibit better performance and capture subtler patterns in data.\\n18Colossal Clean Crawled Corpus\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 15, 'page_label': '16'}, page_content='One significant criticism is their data-hungry nature, requiring vast amounts of text data\\nfor training, which raises concerns about environmental impact and computational costs. The\\nmodels can sometimes generate plausible but factually incorrect or nonsensical information, a\\nphenomenon often referred to as hallucination. The black-box nature of these models poses\\nchallenges in interpretability and transparency, making it difficult to understand how decisions\\nare made or how to correct biases.\\nGPT-3 demonstrated surprising emergent behaviours, such as improved reasoning, problem-\\nsolving, and creative writing, which were not explicitly programmed or observed in their pre-\\ndecessors. These abilities suggest that scaling up model size can lead to qualitative changes\\nin how models understand and interact with language, although the relationship is not yet\\nfully understood. OpenAI has explored two major approaches to further improving the GPT-3\\nmodel, i.e., training on code data and alignment with human preference, which are detailed as\\nfollows:\\n1. Training on code data: This approach involves fine-tuning the model on a diverse set\\nof programming tasks, such as code completion, code generation, and code summariza-\\ntion. The model is trained on a large corpus of code data, which includes code snippets,\\nprogramming languages, and software development documentation. The goal is to im-\\nprove the models understanding of programming languages and its ability to generate\\ncode, thereby enhancing its performance on programming-related tasks.\\n2. Alignment with human preference: This approach involves training the model to\\ngenerate outputs that align with human preferences and values and can be dated back\\nto a work that applied reinforcement learning (RL) Christiano et al. [38] (similar to the\\nreward training step in the aligning algorithm of InstructGPT.\\nGTP-4 GPT-4 [370], the successor to GPT-3, marks a further advancement in the GPT series\\ndeveloped by OpenAI. While specific details about GPT-4s architecture and capabilities are\\nproprietary, it is known to build upon the foundational concepts of its predecessors, emphasizing\\nscale, capability, and efficiency. GPT-4 is a multimodal model which can accept image and\\ntext inputs and produce text outputs. Such models are an important area of study as they\\nhave the potential to be used in a wide range of applications, such as dialogue systems, text\\nsummarization, and machine translation.\\nOn the MMLU benchmark [113], an English-language suite of multiple-choice questions\\ncovering 57 subjects, GPT-4 outperforms existing models by a considerable margin in English\\nand demonstrates strong performance in other languages. GPT-4 development was enabled by\\ndeep learning infrastructure and optimization methods that behave predictably across various\\nscales. The approach allowed the authors to predict the expected performance of GPT-4 (based\\non small runs trained similarly), which was tested against the final run, to increase confidence\\nin the training. The primary reason is that extensive model-specific tuning is not feasible for\\nvery large training runs.\\nThe prediction GPT-4s final loss was predicted by fitting a scaling law with an irreducible\\nloss term (as in Henighan et al. [90]):\\nL(C) = aCb + c (4)\\nfrom models trained using the same methodology but using at most 10,000 less compute than\\nGPT-4. The fitted scaling law predicted GPT-4s final loss with high accuracy. In addition\\nto predicting the final loss, a metric of capability was also predicted. One such metric is the\\npass rate on HumanEval dataset [108], which measures the ability to write Python functions\\nof various complexity. The approximate power law relationship is\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 16, 'page_label': '17'}, page_content='User What is funny about this image? Describe it panel by\\npanel.\\nGPT-4 The image shows a Lightning Cable adapter package\\nwith three panels.\\nPanel 1: A smartphone with a VGA connector (a large,\\nblue, 15-pin connector typically used for computer mon-\\nitors) plugged into its charging port.\\nPanel 2: The package for the Lightning Cable adapter\\nwith a picture of a VGA connector on it.\\nPanel 3: A close-up of the VGA connector with a\\nsmall Lightning connector (used for charging iPhones\\nand other Apple devices) at the end.\\nThe humour in this image comes from the absurdity of\\nplugging a large, outdated VGA connector into a small,\\nmodern smartphone charging port.\\nTable 1: Example prompt demonstrating GPT-4s visual input capability. The prompt consists of a\\nquestion about an image with multiple panels.\\nThe model can grasp the context of the image and provide a detailed description of each panel, high-\\nlighting the humour in the visual juxtaposition of old and new technology. There are literal websites\\nthat explain jokes, and why something is funny. So, its not possible to know whether LLMs explana-\\ntion of the joke is coming from the true understanding of language or from those retrievals.\\nEP [log pass rate(C)] =   Ck (5)\\nwhere k and  are positive constants, and P is a subset of problems in the dataset.\\nGPT-4 accepts prompts consisting of images and text, which lets the user specify any\\nvision or language task in parallel to the text-only setting. Specifically, the model generates\\ntext outputs, given inputs consisting of arbitrarily interlaced text and images. Despite its\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 17, 'page_label': '18'}, page_content='capabilities, GPT-4 has similar limitations to earlier GPT models: it is not fully reliable (e.g.\\ncan suffer from hallucinations), has a limited context window, and does not learn from\\nexperience. Care should be taken when using the outputs of GPT-4, particularly in contexts\\nwhere reliability is important.\\nOpenAI o1 OpenAI o1 [383] is a multimodal model developed by OpenAI, designed to pro-\\ncess and generate text and images. It is a new large language model trained with reinforcement\\nlearning to perform complex reasoning. The model is able to produce long internal chain of\\nthoughts before responding to a prompt. The o1 model family represents a transition from fast,\\nintuitive thinking to now also using slower, more deliberate reasoning.\\nThe large-scale reinforcement learning algorithm teaches the model how to think produc-\\ntively using its chain of thought in a highly data-efficient training process. Authors found that\\nthe performance of o1 consistently improves with more reinforcement learning (train-time com-\\npute) and with more time spent thinking (test-time compute). The constraints on scaling this\\napproach differ substantially from those of LLM pretraining, and this approach is still under\\nactive research.\\nFigure 6: o1 greatly improves over GPT-4o on challenging reasoning benchmarks. Solid bars show\\npass@1 accuracy and the shaded region shows the performance of majority vote (consensus) with 64\\nsamples. Source: OpenAI [383].\\no1 has demonstrated proficiency in various domains, including advanced mathematics, cod-\\ning, and scientific problem-solving, showcasing its versatility and potential for real-world ap-\\nplications as shown in Figure 6 19. Datasets like MATH and GSM8K are no longer effective\\nat differentiating models for recent frontier models 20. o1 version trained for coding also shows\\na significant improvement in performance on competitive programming questions from 2024\\nInternational Olympiad in Informatics (IOI) and in Codeforces competitive programming con-\\ntests.\\nHuman evaluations show that o1-preview is preferred to gpt-4o by a large margin in reasoning-\\nheavy categories like data analysis, coding, and math. However, o1-preview is not preferred on\\nsome natural language tasks, suggesting that it is not well-suited for all use cases.\\nIntegrating policies for model behaviour into chain-of-thought reasoning looks promising\\nfor improving model safety and alignment. This approach is a more robust way to teach\\nhuman values and principles to model, improving performance on known jailbreaks21 and safety\\nbenchmarks [369].\\n19OpenAI o1 ranks in the 89th percentile on competitive programming questions (Codeforces), places among\\nthe top 500 students in the US in a qualifier for the USA Math Olympiad (AIME), and exceeds human PhD-level\\naccuracy on a benchmark of physics, biology, and chemistry problems (GPQA).\\n20Claude 3.5 Sonnet and Gemini\\n21adversarial prompts that purposely try to circumvent model refusals for content its not supposed to produce\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 18, 'page_label': '19'}, page_content='Unfortunately authors decided to not show the raw chains of thoughts generated by the\\nmodel to the users, as they can be unaligned with human values and principles. The model\\nshows instead a summary of the chain of thoughts, where the summariser is trained to avoid\\ndisallowed content.\\nMETR, a nonprofit research organization focused on assessing catastrophic risks from ad-\\nvanced AI systems, evaluated the autonomous capabilities of AI models o1-preview-early, o1-\\nmini, and o1-preview between late August and early September 2024. Their methodology\\ninvolved testing these models in virtual environments on multi-step tasks. While the models\\ndemonstrated strong reasoning and planning abilities, their overall performance in autonomy\\ntasks did not surpass the best public model, Claude 3.5 Sonnet . The models struggled with\\ntool usage and feedback responsiveness when placed in basic agent scaffolds 22. However, they\\nexcelled at one-step code generation, creating coherent plans, and offering useful suggestions.\\nWhen integrated into optimized agent scaffolds (i.e., where they act as advisors to other agents)\\nthe performance aligned with the best public model.\\nIn terms of their planning capabilities, Wang et al. [387] finds that the models excel at follow-\\ning constraints but face difficulties in decision-making and spatial reasoning. The o1 model is\\nevaluated from three key perspectives: feasibility 23, optimality24, and generalizability25. While\\no1 outperforms GPT-4 in some areas, it struggles with generating optimal solutions and gen-\\neralizing across various scenarios, such as memory handling and decision-making processes.\\nThe new version of o1, o3, has been recently released, and it is expected to further improve\\nthe models reasoning capabilities and performance on a wide range of tasks. As reported by\\nNew Scientist in New Scientist [382], o3 also scored a record high of 75.7% on the Abstrac-\\ntion and Reasoning Corpus (ARC) developed by Google software engineer Fran cois Chollet, a\\nprestigious AI reasoning test, but did not yet complete the requirements for the Grand Prize\\nrequiring 85% accuracy. Without the computing cost requirements imposing by the test, the\\nmodel also achieves a new record high of 87.5%, while humans score, on average, 84%.\\n2.3.4 Llama\\nLlama 26 is a language model developed by Meta AI, designed to be a versatile and efficient\\nfoundation for a wide range of natural language processing (NLP) tasks. Llama is built on a\\ntransformer architecture [334], similar to other large language models, with a range from 7B to\\n65B parameters. Main differences between Llama and original Transformer architecture [334]\\nare the following:\\n1. Pre-normalization27 Llama uses pre-normalization28, which means that the normaliza-\\ntion layer is placed before the self-attention and feed-forward layers. Pre-normalization\\nhas improved training stability and convergence in large language models, making it a\\npopular choice for many state-of-the-art models.\\n22Agent scaffolding refers to a structured framework or setup that supports and organizes how an AI agent\\ninteracts with tasks, tools, and its environment to achieve specific goals effectively. It serves as a support\\nstructure - much like scaffolding used in construction - to guide the agent in reasoning, planning, and decision-\\nmaking.\\n23A plan must not only be executable but also ensure goal completion under real-world constraints.\\n24optimality concerns how efficiently the plan achieves its goal\\n25Whether a language model can successfully plan across a diverse range of scenarios, including those it may\\nnot have explicitly encountered during training.\\n26Large Language Model Meta AI\\n27Inspired by GTP-3 model\\n28See Section 3.5.4\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 19, 'page_label': '20'}, page_content='2. SwiGLU activation function 29 LLaMA uses the SwiGLU 30 activation function by\\nShazeer [100], which is a variant of the Gated Linear Unit (GLU) activation function.\\nSwiGLU has been shown to improve the performance of large language models by en-\\nhancing the flow of information through the network.\\n3. Rotary Embeddings31 Llama uses rotary embeddings by Su et al. [134], which are a\\ntype of positional encoding that helps the model capture long-range dependencies in the\\ninput data.\\nModel params dimension #heads #layers learning rate batch size #tokens context\\nLLaMA 6.7B 4096 32 32 3 .0  104 4M 1.0T 2k\\nLLaMA 13.0B 5120 40 40 3 .0  104 4M 1.0T 2k\\nLLaMA 32.5B 6656 52 60 1 .5  104 4M 1.4T 2k\\nLLaMA 65.2B 8192 64 80 1 .5  104 4M 1.4T 2k\\nCodeLlama 2 7B 4096 32 32 2 .0  104 4M 1.8T 16k\\nLLaMA 2 7B 4096 32 32 2 .0  104 4M 1.8T 4k\\nLLaMA 2 13B 5120 40 40 2 .0  104 4M 1.8T 4k\\nLLaMA 2 70B 8192 64 80 1 .5  104 4M 1.8T 4k\\nLLaMA 3 8B 4096 32 32 2 .5  104 4M 15T 8k\\nLLaMA 3 70B 8192 64 80 1 .0  104 4M 15T 8k\\nLLaMA 3.1 8B 4096 32 32 3 .0  104 4M 15T 128k\\nLLaMA 3.1 70B 8192 64 80 1 .5  104 4M 15T 128k\\nLLaMA 3.1 504B 16384 128 126 8 .0  105 4M 15T 128k\\nTable 2:Llama models sizes, architectures, and optimization hyper-parameters. Params: This column\\nrepresents the total number of parameters in billions. Dimension: The dimension of the models hidden\\nlayers. # heads: The number of attention heads in the model. # layers: The number of transformer\\nlayers in the model. Learning rate: The learning rate used during training. Batch size: The batch size\\nused during training. # tokens: The total number of tokens in the training dataset. Source: Touvron\\net al. [330].\\nBased on the Llama paper by Touvron et al. [330], even though Llama 13B is smaller than\\nmany competitors, it outperforms GPT-3 on most benchmarks, and the 65B model is competi-\\ntive with the best large language models available, such as Chinchilla and PaLM-540B, despite\\nbeing x10 smaller (as shown in Table 3).\\nModel Params BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA\\nGPT-3 175B 60.5 81.0 - 78.9 70.2 68.8 51.4 57.6\\nGopher 280B 79.3 81.8 50.6 79.2 70.1 - - -\\nChinchilla 70B 83.7 81.8 51.3 80.8 74.9 - - -\\nPaLM 62B 84.8 80.5 - 79.7 77.0 75.2 52.5 50.4\\nPaLM-cont 62B 83.9 81.4 - 80.6 77.0 - - -\\nPaLM 540B 88.0 82.3 - 83.4 81.1 76.6 53.0 53.4\\nLlama 7B 76.5 79.8 48.9 76.1 70.1 72.8 47.6 57.2\\nLlama 13B 78.1 80.1 50.4 79.2 73.0 74.8 52.7 56.4\\nLlama 33B 83.1 82.3 50.4 82.8 76.0 80.0 57.8 58.6\\nLlama 65B 85.3 82.8 52.3 84.2 77.0 78.9 56.0 60.2\\nTable 3: Zero-shot performance on Common Sense Reasoning tasks. Source: Touvron et al. [330].\\n29Inspired by PaLM model\\n30See Section 3.5.4\\n31Inspired by GPTNeo model\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 20, 'page_label': '21'}, page_content='The Llama models were trained exclusively on publicly available data, setting them apart\\nfrom other models that rely on proprietary datasets 32. The dataset is a mixture of several\\nsources (webpages, books, scientific data and code) as reported in Table 4.\\nDataset Classification Sampling prop. Epochs Disk size\\nCommonCrawl Webpages 67.0% 1.10 3.3 TB\\nC4 Web 15.0% 1.06 783 GB\\nGithub Code 4.5% 0.64 328 GB\\nWikipedia Webpages 4.5% 2.45 83 GB\\nBooks Books 4.5% 2.23 85 GB\\nArXiv Scientific Data 2.5% 1.06 92 GB\\nStackExchange Conversation Data 2.0% 1.03 78 GB\\nTable 4: Pre-training data. Data mixtures used for pre-training for each subset, the table reports the\\nsampling proportion, number of epochs performed on the subset when training on 1.4T tokens, and\\ndisk size. The pre-training runs on 1T tokens have the same sampling proportion. Source: Touvron\\net al. [330].\\nLlama models were designed with efficiency in mind, both in training and inference, allowing\\neven the 13B parameter model to run on a single GPU. A synthetic view of the Llama model\\nfamily parameters is reported in Table 2. The optimizer used during the training is the same\\nAdamW with the following hyper-parameters: 1 = 0.9, 2 = 0.95, eps= 105, a weight decay\\nof 0.1, gradient clipping of 1.0, a cosine learning rate schedule and a warmup of 2000 steps.\\nTouvron et al. [330] acknowledges the presence of biases and toxicity in the models due to\\nthe nature of web data and evaluates these aspects using benchmarks from the responsible AI\\ncommunity.\\nLlama 2. Llama 2 [329] is a continuation of the Llama series, developed by Meta AI, released\\nin scale from 7B to 70B parameters. The pre-training data of the Llama2 model is a new mix\\nof data from publicly available sources. The training corpus is 40% larger than the one used for\\nLlama 1, and it is composed of a mix of text and a percentage of code data that is roughly 8%\\nof the total. The exact composition of the data mix is not disclosed, but the code percentage is\\nreported in the caption of the Table 5 extracted from the original paper [329]. The pre-training\\nselection focuses on addressing biases and toxicity recognised in the previous version of the\\nmodel.\\nLlama 2 adopts most of the pretraining settings and model architecture from Llama 1, in-\\ncluding the standard transformer architecture, pre-normalization using RMSNorm, the SwiGLU\\nactivation function, and rotary positional embeddings. The optimizer used during the training\\nis the same AdamW with the following hyper-parameters: 1 = 0 .9, 2 = 0 .95, eps= 10 5,\\na weight decay of 0.1, gradient clipping of 1.0, a cosine learning rate schedule and a warmup\\nof 2000 steps. The primary architectural differences from Llama 1 include increased context\\nlength and grouped-query attention (GQA).\\nCode Llama. Code Llama [384] is a family of large language models for code generation\\nbased on Llama 2 providing infilling 33 capabilities, support for large input contexts and zero-\\nshot instruction following ability for programming tasks. It comes in three flavours: the vanilla\\n32Such as Books  2TB or Social media conversations\\n33With the terms infilling or code completion, we refer to the process of generating code snippets that complete\\na given code fragment.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 21, 'page_label': '22'}, page_content='Language Percent Language Percent\\nen 89.70% uk 0.07%\\nunknown 8.38% ko 0.06%\\nde 0.17% ca 0.04%\\nfr 0.16% sr 0.04%\\nsv 0.15% id 0.03%\\nzh 0.13% cs 0.03%\\nes 0.13% fi 0.03%\\nru 0.13% hu 0.03%\\nnl 0.12% no 0.03%\\nit 0.11% ro 0.03%\\nja 0.10% bg 0.02%\\npl 0.09% da 0.02%\\npt 0.09% sl 0.01%\\nvi 0.08% hr 0.01%\\nTable 5:Language distribution in pretraining data with percentage 0.005%. Most data is in English,\\nmeaning that LLaMA 2 will perform best for English-language use cases. The large unknown category\\nis partially made up of programming code data.\\nmodel, the Python specialized model, and the instruction-following model with 7B, 13B, 34B,\\nand 70B parameters each (see Figure 7).\\nFigure 7:The Code Llama 70B specialization pipeline. The different fine-tuning stages are annotated\\nwith the number of tokens seen during training. Infilling-capable models are marked with the  symbol.\\nSource: Rozi` ere et al. [384].\\nWhile most of the code generation models are trained on code only, Code Llama was fine-\\ntuned starting from Llama 2, which was trained on general-purpose text and code data. The\\ncomparison in Rozi` ere et al. [384] shows that initializing from Llama 2 leads to better perfor-\\nmance on code generation tasks than initializing from a code-only model for a given budget as\\nshown in Figure 8. Code Llama was fine-tuned on 500B extra tokens consisting mostly of code\\ndata (85%).\\nLlama 3. Llama 3 [389] is a continuation of the Llama series, developed by Meta AI, with\\ndifferent model sizes: 8B, 70B, and 405B parameters.\\nLlama 3 uses a standard, dense Transformer architecture. It does not deviate significantly\\nfrom Llama and Llama 2 in terms of model architecture; therefore performance gains are\\nprimarily driven by improvements in data quality and diversity as well as by increased training\\nscale. Compared to Llama 2, Llama 3 has a few small changes in the model architecture:\\n1. improve inference speed and key-value caches during decoding by using grouped query\\nattention (GQA) with 8 key-value heads\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 22, 'page_label': '23'}, page_content='Figure 8: Comparison of Code Llama models versus an identical model trained from scratch. Source:\\nRozi` ere et al. [384].\\n2. an attention mask that prevents self-attention between different documents within the\\nsame sequence. This change has limited impact during standard pre-training, but its\\nimportant in continued pre-training on very long sequences.\\n3. a vocabulary with 128K tokens. It improves compression rates on English data compared\\nto the Llama 2 tokenizer.\\n4. the RoPE base frequency hyper-parameter increased to 500,000 to support longer con-\\ntexts.\\nA summary of the key hyper-parameters of Llama 3 is shown in Table 6.\\n8B 70B 405B\\nLayers 32 80 126\\nModel Dimension 4096 8192 16384\\nFFN Dimension 14336 28672 53248\\nAttention Heads 32 64 128\\nKey/Value Heads 8 8 8\\nPeak Learning Rate 3  104 1.5  104 8  105\\nActivation Function SwiGLU SwiGLU SwiGLU\\nVocabulary Size 128,000 128,000 128,000\\nPositional Embed-\\ndings\\nRoPE ( = 500, 000) RoPE (  = 500, 000) RoPE (  = 500, 000)\\nTable 6: Overview of the key hyperparameters of Llama 3. Source: AI [389].\\nThe authors improved the quantity and quality of the data we used for pre-training and\\npost-training compared to prior versions of Llama. These improvements include developing\\nmore careful pre-processing and curation pipelines for pre-training data and more rigorous\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 23, 'page_label': '24'}, page_content='quality assurance and filtering approaches for post-training data. The pre-training corpus\\nconsists of about 15T tokens, which is about 50The dataset comprises approximately 50%\\ngeneral knowledge tokens, 25% mathematical and reasoning tokens, 17% code tokens, and 8%\\nmultilingual tokens [389]. The resulting models possess a wide array of capabilities. They\\ncan respond to questions in at least eight languages, generate high-quality code, solve complex\\nreasoning tasks, and utilize tools directly or in a zero-shot manner.\\n2.3.5 Gemma\\nThe recent development in the domain of Natural Language Processing has seen Googles intro-\\nduction of a new family of models named Gemma [372, 385]. Derived from the same research\\nlineage as the renowned Gemini models, Gemma is a testament to the rapid advancements in\\nlightweight, high-performance language models designed for a broad spectrum of computational\\nenvironments.\\nGemma is built upon a transformer-based architecture by Vaswani et al. [334], optimized\\nto deliver state-of-the-art performance with a fraction of the parameter count typically seen\\nin large language models (LLMs). Notable enhancements include the adoption of Multi-Query\\nAttention, RoPE embeddings, GeGLU activations, and RMSNorm, indicating an evolution of\\nthe original transformer architecture. The family comprises two main configurations: Gemma\\n2B and Gemma 7B, available in pre-trained and instruction-tuned variants. The design philos-\\nophy targets efficient deployment across diverse hardware platforms, including but not limited\\nto mobile devices, laptops, desktop computers, and servers.\\nFigure 9: Gemma models exhibit superior performance in language understanding and reasoning\\ntasks compared to larger models. Source: Team et al. [385].\\nIn comparative benchmarks, Gemma models have demonstrated capabilities that exceed\\nthose of larger parameter models, such as Llama 2 (13B), indicating a significant efficiency in\\nparameter utilization. Improvements are particularly evident in language understanding and\\nreasoning tasks where Gemma models have been pitted against their contemporaries.\\nOne prominent strength of Gemma models is their deployment efficiency, which democratizes\\naccess to state-of-the-art NLP tools. The models are designed to be run on common developer\\nhardware, eschewing the need for specialized AI accelerators.\\nDespite their efficiencies, the Gemma models are not without limitations. While the re-\\nduced parameter count is advantageous for accessibility and computational efficiency, it may\\nimpact performance in complex NLP tasks that can benefit from larger models. Additionally,\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 24, 'page_label': '25'}, page_content='Figure 10: Gemma models are designed to be lightweight and efficient, making them accessible to a\\nwide range of developers and applications. Source: Banks and Warkentin [372].\\nethical considerations, such as bias in language models, remain an area of concern and active\\ndevelopment.\\nGoogle has emphasized the responsible development of AI, which is evident in Gemmas\\ndesign. Techniques to mitigate sensitive data inclusion and reinforcement learning from human\\nfeedback are incorporated to ensure the models outputs adhere to safety standards. Moreover,\\nGoogles release includes a Responsible Generative AI Toolkit to aid developers in prioritizing\\nthe creation of ethical AI applications.\\n2.3.6 Claude\\nClaude models are a family of large language models developed by Anthropic, a research or-\\nganization focused on building advanced AI systems [371]. The most advanced model in the\\nClaude series, Claude 3.5 Sonnet, excels at natural language understanding and generation,\\nincluding summarization, creative writing, and more. It shows marked improvements in log-\\nical and mathematical reasoning, outperforming prior versions on benchmarks. The model is\\ncapable of writing, debugging, and explaining code snippets. It is optimized for dialogues and\\ninteractive workflows, allowing for dynamic and iterative engagement with users.\\nClaude 3 has demonstrated significant improvements in its ability to perform logical and\\nmathematical reasoning tasks. Logical reasoning, in particular, showcases the models ability to\\ndeduce patterns, validate arguments, and resolve abstract puzzles. For example, tasks involving\\nsyllogistic reasoning or the identification of valid logical structures benefit from the models\\nenhanced understanding of formal rules.\\nIn mathematical reasoning, the model has shown its ability to parse and solve complex\\nproblems across multiple steps. Benchmarks such as GSM8K, which contains grade-school-level\\narithmetic and word problems, highlight Claude 3s ability to provide structured and accurate\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 25, 'page_label': '26'}, page_content='solutions. The model can further engage in higher-level mathematics, including algebra and\\nbasic calculus, as evaluated by the MATH dataset, though challenges remain in more specialized\\ndomains.\\nBeyond formal reasoning, Claude 3 excels in commonsense understanding, a critical aspect\\nof human-like intelligence. Benchmarks such as CommonSenseQA and PIQA demonstrate\\nits ability to reason about everyday scenarios and physical phenomena, respectively. These\\ncapabilities are crucial for applications that require intuitive decision-making, such as virtual\\nassistants or educational tools.\\nClaude 3s ethical reasoning is a particularly interesting facet. Leveraging training paradigms\\nfocused on safety and alignment, the model is adept at identifying and addressing ethical dilem-\\nmas. Benchmarks like the Winogender Schema, which tests gender bias, and other ethical\\nreasoning tests confirm the models ability to minimize bias and generate responsible outputs.\\nDespite its strengths, Claude 3 is not without limitations. Contextual understanding can\\nfalter in multi-layered or ambiguously phrased tasks. Similarly, abstract reasoning outside the\\nbounds of its training data can present significant hurdles. Another limitation arises in the\\nhandling of uncertainty; the model can occasionally overcommit to answers even when the\\nunderlying confidence is low. These challenges underscore the need for further improvements,\\nparticularly in domains requiring highly abstract thinking or multi-turn contextual reasoning.\\nIntegrating enhanced memory mechanisms may help the model process longer or more complex\\ncontexts, thereby reducing errors and improving overall coherence.\\nClaude 3.5 Sonnet shows substantial enhancements in both logical and commonsense rea-\\nsoning. This improvement is particularly evident in graduate-level problem-solving tasks and\\nother advanced reasoning benchmarks, such as the ARC dataset. The model demonstrates a\\nbetter ability to:\\n Parse complex, multi-step problems and provide structured solutions.\\n Handle abstract reasoning with improved accuracy in scenarios involving nuanced logical\\npatterns or uncommon use cases.\\nComparing this version to its predecessors Claude 3 and Claude 3 Opus, the advancements in\\nClaude 3.5 Sonnet are clear:\\n On reasoning benchmarks, Claude 3.5 Sonnet achieves higher accuracy, particularly in\\ntests like GSM8K and MATH datasets.\\n Interaction speeds are significantly faster, improving usability in real-time applications.\\n Its coding capabilities surpass earlier versions in complexity and versatility, reflecting\\ndeeper training on software development datasets.\\n2.4 Specialized Large Language Models\\nSpecialized Large Language Models (LLMs) are model checkpoints refined for particular fields\\nor tasks, such as healthcare and finance. The existing domain-specific models are developed by\\npre-training on specialized datasets [191, 254, 220]), by adapting a very large general-purpose\\nmodel to domain-specific tasks [213, 185], or mixing both approaches [350]. These models serve\\nas domain-specific problem solvers and are evaluated based on general competencies, such\\nas fundamental complex reasoning, and more nuanced capabilities, like alignment with human\\nintent, as well as their performance in areas specific to their application. To accurately measure\\ntheir efficacy, specialized benchmarks are developed that cater to these distinct sectors. These\\ntailored benchmarks are then employed in conjunction with broader assessments to provide\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 26, 'page_label': '27'}, page_content='a holistic and focused evaluation of the models capabilities. The following sections highlight\\nsome of LLMs key applications and their impact on different sectors, from healthcare to finance\\nand education to research.\\n2.4.1 LLMs in Healthcare\\nThe intersection of artificial intelligence (AI) and healthcare has precipitated unparalleled ad-\\nvances in the provision of medical services, diagnosis, treatment, and patient care. Central\\nto these advancements are Large Language Models (LLMs), which have been instrumental in\\ncatalyzing transformative changes across the healthcare sector:\\n1. Medical image analysis: Large Language Models (LLMs) have been integrated with\\nmedical imaging technologies to enhance diagnostic accuracy and efficiency. By analyzing\\nradiological images and clinical reports, LLMs can assist radiologists in interpreting im-\\nages, identifying abnormalities, and providing diagnostic insights. These models leverage\\ntheir natural language processing capabilities to extract information from textual reports\\nand correlate it with visual data, thereby augmenting the diagnostic process [120, 140].\\n2. Clinical Decision Support: LLMs have been pivotal in augmenting clinical decision\\nsupport systems (CDSS). By analyzing patient data and medical literature, LLMs assist\\nclinicians in diagnosing conditions, suggesting treatment options, and predicting patient\\noutcomes. For instance, models like BERT and its derivatives have been fine-tuned on\\nmedical corpora, yielding tools that can parse clinical notes, interpret lab results, and\\nprovide evidence-based recommendations [61].\\n3. Medical Documentation and Coding: The onus of medical documentation and billing\\nhas traditionally been a significant administrative burden for healthcare providers. LLMs\\nhave demonstrated the ability to streamline these processes by automating the transla-\\ntion of clinical dialogue and notes into structured electronic health records (EHRs) and\\naccurately coding medical procedures, thus mitigating errors and saving time [53].\\n4. Drug Discovery and Development: In the domain of pharmaceuticals, LLMs have\\nexpedited the drug discovery and development pipelines. By mining through vast chemical\\nlibraries and medical databases, these models facilitate the identification of potential drug\\ncandidates and the repurposing of existing drugs for new therapeutic uses [84].\\n5. Personalized Medicine: Personalized medicine, which tailors treatment to individual\\npatient characteristics, has benefited from LLMs by generating patient-specific models\\nthat predict disease susceptibility and drug response. This personalization extends to cre-\\nating tailored health interventions based on patient history and genetic information [15].\\n6. Patient Engagement and Self-Management: LLMs are also revolutionizing patient\\nengagement by powering intelligent virtual health assistants capable of providing infor-\\nmation, reminders, and motivational support for chronic disease self-management. These\\nAI assistants interact with patients in natural language, thus fostering an environment\\nconducive to patient education and adherence to treatment regimens [70].\\nDespite these strengths, LLMs face significant challenges within healthcare applications.\\nConcerns regarding patient privacy, data security, and the need for explainability in AI-driven\\ndecisions are paramount [43]. Additionally, biases inherent in training data can perpetuate\\ndisparities in patient care, necessitating rigorous validation and fairness assessments before\\nclinical deployment [63].\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 27, 'page_label': '28'}, page_content='Large Language Models represent a transformative force in healthcare, enhancing efficiency,\\naccuracy, and personalization in various medical domains. Their integration into clinical prac-\\ntice must be pursued with diligent oversight to navigate ethical considerations and ensure\\nequitable and safe applications.\\nMed-PaLM One of the most advanced LLMs for healthcare is Med-PaLM, a derivative of the\\nPaLM (540B) model developed by Google and its instruction-tuned variant, Flan-PaLM. Using\\na combination of few-shot [88]), chain-of-thought (CoT) (Wei et al. [230]), and self-consistency\\n(Wang et al. [227] prompting strategies, Flan-PaLM achieved state-of-the-art accuracy on every\\nMultiMedQA 34 multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical\\ntopics and a newly introduced dataset, HealthSearchQA, which consists of commonly searched\\nhealth questions).\\nModel (number of parameters) MedQA (USMLE) Accuracy %\\nFlan-PaLM (540 B) 67.6\\nPubMedGPT (2.7 B) 50.3\\nDRAGON (360 M) 47.5\\nBioLinkBERT (340 M) 45.1\\nGalactica (120 B) 44.4\\nPubMedBERT (100 M) 38.1\\nGPT-Neo (2.7 B) 33.3\\nTable 7: Performance comparison of different models on the MedQA (USMLE) benchmark. Source:\\nSinghal et al. [213].\\nDespite these remarkable results, human evaluation reveals key gaps in Flan-PaLM responses\\nand remains inferior to clinicians [213]. To resolve this issue, researchers introduced instruction\\ntuning35 to align the Flan-PaLM model to the medical domain. Thus, Instruction tuning\\ncan be seen as a lightweight way (data-efficient, parameter-efficient, compute-efficient during\\ntraining and inference) of training a model to follow instructions in one or more domains.\\nInstruction tuning adapted LLMs to follow better the specific type of instructions used in the\\nfamily of medical datasets. The result was Med-PaLM, a model that significantly reduces\\nthe gap (or even compares favourably) to clinicians on several evaluation axes, according to\\nclinicians and lay users.\\n2.4.2 LLMs in Finance\\nThere has been growing interest in applying NLP to various financial tasks, including sentiment\\nanalysis, question answering, and stock market prediction. Despite the extensive research\\ninto general-domain LLMs and their immense potential in finance, Financial LLM (Fin-LLM)\\nresearch remains limited, and the field of financial LLMs is at an early stage [380]. An overview\\nof the evolution of selected PLM/LLM releases from the general domain to the financial domain\\nis shown in Figure 12.\\nSome of these models have demonstrated the potential of LLMs to understand complex\\nfinancial jargon, generate insights, predict market trends, and enhance customer interaction\\nwith unprecedented precision and relevance.\\n34Stands for a multi-domain medical question answering benchmark. It has been designed to evaluate the\\nperformance of LLMs in the healthcare sector. This benchmark likely encompasses a wide range of medical\\nquestions, covering various disciplines, conditions, and scenarios that medical professionals encounter.\\n35See Section 3.4.1\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 28, 'page_label': '29'}, page_content='Figure 11: Large Language Models (LLMs) have revolutionized healthcare by enhancing diagnostic\\naccuracy, clinical decision support, and patient engagement. Source: Singhal et al. [213].\\nHere are some key applications of LLMs in the financial sector:\\n1. Algorithmic Trading: LLMs analyze vast amounts of unstructured data, including\\nnews articles, financial reports, and social media, to gauge market sentiment and pre-\\ndict stock price movements. Their predictive insights enable more informed algorithmic\\ntrading strategies [44].\\n2. Risk Management: In risk management, LLMs contribute by parsing and interpret-\\ning complex regulatory documents, identifying potential compliance risks, and offering\\nactionable insights to mitigate financial and reputational risks [96].\\n3. Customer Service Automation: Financial institutions leverage LLMs to power chat-\\nbots and virtual assistants, providing real-time, personalized customer service. These\\nAI-driven systems can handle inquiries, execute transactions, and offer financial advice,\\nenhancing customer experience and operational efficiency [127].\\n4. Fraud Detection: LLMs enhance fraud detection systems by analyzing transactional\\ndata and customer communication to identify patterns indicative of fraudulent activities,\\nthereby bolstering the security of financial transactions [79].\\nSome of the models in Figure 12 have augmented the accuracy and efficiency of financial\\nanalyses and expedited the decision-making processes, enabling more timely and informed de-\\ncisions. Additionally, their role in risk management is noteworthy, where their data processing\\nand analytical prowess help identify potential risks and adherence issues more effectively than\\ntraditional methodologies [44].\\nDespite their potential, LLMs in finance face challenges, including data privacy concerns,\\nthe need for interpretability in model decisions, and the risk of perpetuating biases from train-\\ning data. Ensuring these models adhere to ethical standards and regulatory compliance is\\nparamount [92, 44].\\nLets delve deeper into the techniques used to adapt LLMs for the financial sector to en-\\nhance their performance on finance-specific tasks [380]. These techniques enhance the models\\nunderstanding of financial language, data, and context, improving their performance on finance-\\nspecific tasks. Heres a more detailed look at these techniques:\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 29, 'page_label': '30'}, page_content='Figure 12: Timeline showing the evolution of selected PLM/LLM releases from the general domain\\nto the financial domain. Source: Lee et al. [380].\\nModel Backbone Paras. PT\\nTech-\\nniques\\nPT\\nData Size\\nEvaluation\\nTask\\nDataset O.S.\\nModel\\nPT IFT\\nBloombergGPT [350] BLOOM 50B PT, PE (G) 345B\\ntokens\\n(F) 363B\\ntokens\\nSA, TC FPB,\\nFiQA-SA,\\nHeadline\\nN N N\\nFinMA [353] Llama 7B,\\n30B\\nIFT,\\nPE\\n(G) 1T to-\\nkens\\nSA, TC,\\nNER, QA\\nFPB,\\nFiQA-SA,\\nHeadline\\nFIN,\\nFinQA,\\nCon-\\nvFinQA\\nY Y Y\\nInvestLM [358] Llama 65B PEFT (G) 1.4T\\ntokens\\nSA, TC,\\nSMP\\nStockNet,\\nCIKM18,\\nBigData22\\nY N N\\nFinGPT [339] 6 open-\\nsource\\nLLMs\\n7B PEFT (G) 2T to-\\nkens\\nSA, TC,\\nNER, RE\\nFPB,\\nFiQA-SA,\\nHeadline\\nFIN,\\nFinRED\\nY Y Y\\nTable 8:The abbreviations correspond to Paras.= Model Parameter Size (Billions); Disc. = Discrim-\\ninative, Gen. = Generative; Post-PT = Post-Pre-training, PT = Pre-training, FT = Fine-Tuning,\\nPE = Prompt Engineering, IFT = Fine-Tuning, PEFT = Parameter Efficient Fine-Tuning; (G) =\\nGeneral domain, (F) = Financial domain; (in Evaluation) [SA] Sentiment Analysis, [TC] Text Clas-\\nsification, [SBD] Structure Boundary Detection, [NER] Named Entity Recognition, [QA] Question\\nAnswering, [SMP] Stock Movement Prediction, [Summ] Text Summarization, [RE] Relation Extrac-\\ntion; O.S. Model = Open Source Model. It is marked as Y if it is publicly accessible as of Dec 2023.\\nSource: Lee et al. [380].\\n Domain-Specific Pre-training: This technique involves further training a general LLM\\non a financial corpus. The idea is to refine the models language understanding and\\ngeneration capabilities within the financial domain. By exposing the model to a large\\nvolume of financial texts, such as reports, news, and analysis, the model learns the specific\\njargon, styles, and nuances of financial language.\\n Continual Pre-training: After initial pre-training on a general dataset, the model\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 30, 'page_label': '31'}, page_content='undergoes additional pre-training phases on financial data. This step-by-step refinement\\nhelps the model gradually adapt from a broad understanding of language to a more\\nspecialized comprehension of financial texts. Its a way to incrementally infuse financial\\nknowledge into the model without losing its general language capabilities.\\n Mixed-Domain Pre-training: In this approach, the LLM is trained on a mixed dataset\\ncomprising both general and financial texts. The goal is to maintain the models general\\nlanguage understanding while also equipping it with the ability to process and generate\\nfinancial content. This method aims to strike a balance, ensuring the model is not overly\\nspecialized and retains versatility.\\n Task-Specific Fine-tuning: Once a model has been pre-trained with financial data, it\\ncan be fine-tuned for specific financial tasks. For example, a model could be fine-tuned\\non a dataset of financial sentiment analysis, stock market prediction, or fraud detection.\\nThis fine-tuning process sharpens the models skills on tasks that are directly relevant to\\nthe financial industry.\\n Transfer Learning: Techniques from transfer learning can be applied where a model\\ntrained on one financial task is adapted for another. This approach leverages the knowl-\\nedge the model has gained from one context, applying it to a different but related task,\\nthereby enhancing learning efficiency and performance.\\n Custom Tokenization: Financial texts often contain unique symbols, terms, and nu-\\nmerical expressions. Employing custom tokenization strategies that recognize these pecu-\\nliarities can significantly enhance the models ability to process and understand financial\\ndocuments.\\nWithin the four FinLLMs in Figure 8, FinMA [353], InvestLM [358], and FinGPT [339] are\\nbased on Llama or other open-source based models, while BloombergGPT [350] is a BLOOM-\\nstyle closed-source model.\\nRegarding the evaluation tasks, the models are assessed on a range of financial NLP tasks,\\nas shown below:\\n Sentiment Analysis (SA):This task involves analyzing the sentiment embedded within\\nfinancial documents, such as market reports and news articles. The capability to accu-\\nrately discern sentiment is crucial for applications such as market prediction and the\\nformulation of trading strategies.\\n Named Entity Recognition (NER): Essential for extracting actionable insights from\\nfinancial documents, this task focuses on the identification and categorization of salient fi-\\nnancial entities, including but not limited to company names, stock tickers, and monetary\\nvalues.\\n Question Answering (QA): FinLLMs are tasked with providing cogent answers to\\nqueries based on an expansive financial corpus. This benchmark often requires the syn-\\nthesis of information from dense financial reports or news events.\\n Text Classification (TC): The classification of financial documents into predefined\\ncategories aids in the automated sorting and analysis of financial data, an essential task\\nin managing the voluminous data generated by financial markets.\\n Regulatory Compliance (RE): Given the stringent regulatory environment of the\\nfinancial sector, FinLLMs are often evaluated on their ability to parse and verify the\\ncompliance of financial texts with industry regulations.\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 31, 'page_label': '32'}, page_content='To accurately measure the effectiveness of FinLLMs in performing these tasks, several\\ndatasets have been curated, each tailored to challenge different aspects of a models financial\\nacumen:\\n Financial PhraseBank (FPB): A dataset comprising sentences from financial news,\\nannotated to reflect sentiment polarity, which is instrumental in the training and testing\\nof models for sentiment analysis [24].\\n FiQA - Financial Opinion Mining and Question Answering Challenge (FiQA-\\nSA, FiQA-QA): This dataset encompasses annotated financial news and social media\\ntexts for sentiment analysis alongside a collection of question-and-answer pairs for the\\nevaluation of QA capabilities [48].\\n FIN: A Financial Document Dataset for NER: Designed for entity recognition,\\nthis dataset consists of financial news articles with annotated entities, testing the models\\ncapacity to identify and classify financial terms Alvarado, Verspoor, and Baldwin [25].\\nAnother financial NER dataset is FiNER-139, consisting of 1.1M sentences from financial\\nnews articles, annotated with 139 eXtensive Business Reporting Language (XBRL) word-\\nlevel tags [189]. This dataset is designed for Entity Extraction and Numerical Reasoning\\ntasks, predicting the XBRL tags (e.g., cash and cash equivalents) based on numeric input\\ndata within sentences (e.g., 24.8 million).\\n ConvFinQA: A conversational finance QA dataset challenging models to understand and\\nrespond within the context of financial dialogues, demonstrating an advanced application\\nof FinLLMs in customer interaction Chen et al. [153]. Its an extension of FinQA and\\nis a multi-turn conversational hybrid QA dataset consisting of 3,892 conversations with\\n14,115 questions.\\n StockNet: This dataset combines historical price data with relevant tweets to com-\\nprehensively view SMP tasks. It has been widely used to assess the impact of market\\nsentiment on stock prices [59].\\n CIKM18: A dataset designed for SMP tasks, CIKM18 comprises stock price data and\\nnews headlines, challenging models to predict stock movements based on textual infor-\\nmation [58].\\n BigData22: A dataset for SMP tasks, BigData22 combines financial news articles with\\nstock price data, evaluating models on their ability to predict stock movements based on\\ntextual information Soun et al. [216].\\n Headline: A dataset of financial news headlines, used for text classification [133]. This\\ndataset comprises 11,412 news headlines, where each headline is labelled with a binary\\nclassification (e.g., price up or price down).\\n ECT-Sum: A dataset for text summarization tasks, ECT-Sum consists of consists of\\n2,425 document-summary pairs, containing Earnings Call Transcripts (ECTs) and bullet-\\npoint summarizations from Reuters [201].\\nThe listed datasets are not exhaustive but represent a comprehensive selection of tasks and\\nbenchmarks used to evaluate FinLLMs across a range of financial NLP tasks. As highlighted\\nin Lee et al. [380], in the sentiment analysis task, FLANG-ELECTRA achieved the best results\\n(92% on F1) while FinMA-30B and GPT-4 achieved similar results (87% on F1) with a 5-shot\\nprompting.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 32, 'page_label': '33'}, page_content='These datasets are instrumental in assessing the models performance, guiding their devel-\\nopment, and fostering innovation in the financial sector to address more advanced financial\\ntasks:\\n Relation Extraction (RE): FinRED [210] is a key dataset curated from financial news\\nand earnings call transcripts, containing 29 finance-specific relation tags (i.e., owned by).\\nIts instrumental in identifying and classifying relationships between entities within finan-\\ncial texts.\\n Event Detection (ED): The Event-Driven Trading (EDT) dataset, comprising news\\narticles with event labels and stock price information, facilitates the detection of corporate\\nevents affecting stock prices [143].\\n Causality Detection (CD):FinCausal20 from the Financial Narrative Processing (FNP)\\nworkshop focuses on identifying cause-and-effect relationships in financial texts, a crucial\\naspect for generating meaningful financial summaries [98]. It shares two tasks: detecting\\na causal scheme in a given text and identifying cause-and-effect sentences.\\n Numerical Reasoning (NR): Datasets like FiNER-139 and ConvFinQA are designed\\nto test a models ability to perform calculations and understand financial contexts based\\non numerical data within texts.\\n Structure Recognition (SR): The FinTabNet [142] dataset, collected from earnings\\nreports, emphasizes the detection of table structures and the recognition of logical rela-\\ntionships within financial documents.\\n Multimodal Understanding (MM): Datasets like MAEC [95]) and MONOPOLY\\n(Mathur et al. [194] introduce multimodal data (audio, video, text, time series) from\\nearnings calls and monetary policy discussions, challenging models to integrate diverse\\ndata formats.\\n Machine Translation (MT) in Finance: MINDS-14 [112]) and MultiFin (Jrgensen\\net al. [282] datasets offer multilingual financial text, aiding in the development of models\\nthat can translate and comprehend financial information across languages.\\n Market Forecasting (MF): This task extends beyond stock movement prediction, fo-\\ncusing on broader market trend forecasting 36 using datasets that combine sentiment\\nanalysis, event detection, and multimodal cues 37.\\nRecent studies have shown that general purpose model can outperform fine-tuned models\\non some tasks. Still, they fail in some other cases carefully analyzed in Li et al. [292]. Some\\ninteresting results are shown in Table 9, Table 10, Table 11, Table 12. For example, in the\\nsentiment analysis task, FinMA-30B and GPT-4 achieved similar results (87% on F1) with a\\n5-shot prompting, while FLANG-ELECTRA achieved the best results (92% on F1) Lee et al.\\n[380], while GPT-4 could be the first choice for Sentiment Analysis and Relation Extraction\\ntasks.\\n36Market price, volatility, and risk\\n37Like StockEmotions [289]), EDT (Zhou, Ma, and Liu [143]), MAEC (Li et al. [95]) and MONOPOLY\\n(Mathur et al. [194]\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 33, 'page_label': '34'}, page_content='Data Model 50% Agreement 100% Agreement\\nAccuracy F1 score Accuracy F1 score\\nChatGPT(0) 0.78 0.78 0.90 0.90\\nChatGPT(5) 0.79 0.79 0.90 0.90\\nGPT-4(0) 0.83 0.83 0.96 0.96\\nGPT-4(5) 0.86 0.86 0.97 0.97\\nBloombergGPT(5) / 0.51 / /\\nGPT-NeoX(5) / 0.45 / /\\nOPT6B(5) / 0.49 / /\\nBLOOM176B(5) / 0.50 / /\\nFinBert 0.86 0.84 0.97 0.95\\nTable 9:Results on the Phrasebank dataset. The sub-script (n) following an LLM name represents the\\nnumber of shots. The best results are marked in bold. The results of other LLMs, like BloombergGPT,\\nare from the corresponding papers. / indicates the metrics were not included in the original study.\\nSource:Li et al. [292].\\nModel Category Weighted F1\\nChatGPT(0) OpenAI LLMs 75.90\\nChatGPT(5) OpenAI LLMs 78.33\\nGPT-4(9) OpenAI LLMs 87.15\\nGPT-4(5) OpenAI LLMs 88.11\\nBloombergGPT(5) Domain LLM 75.07\\nGPT-NeoX(5) Prior LLMs 50.59\\nOPT 6B(5) Prior LLMs 51.60\\nBLOOM 176B(5) Prior LLMs 53.12\\nRoBERTa-large Fine-tune 87.09\\nTable 10: Results for the sentiment analysis task on the FiQA dataset. Source: Li et al. [292].\\nModel Weighted F1\\nChatGPT(0) 71.78\\nChatGPT(5) 74.84\\nGPT-4(0) 84.17\\nGPT-4(5) 86.00\\nBloombergGPT(5) 82.20\\nGPT-NeoX(5) 73.22\\nOPT6B(5) 79.41\\nBLOOM176B(5) 76.51\\nBERT 95.36\\nTable 11: Results on the headline classification task. Source: Li et al. [292].\\nBloombergGPT The BloombergGPT model, developed by Wu et al. [350], is a special-\\nized LLM tailored for the financial domain. With its 50 billion parameters, is posited to be\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 34, 'page_label': '35'}, page_content='Model Entity F1\\nChatGPT(0) 29.21\\nChatGPT(20) 51.52\\nGPT-4(0) 36.08\\nGPT-4(20) 56.71\\nBloombergGPT(20) 60.82\\nGPT-NeoX(20) 60.98\\nOPT6B(20) 57.49\\nBLOOM176B(20) 55.56\\nCRFCoNLL 17.20\\nCRFFIN5 82.70\\nTable 12: Results of few-shot performance on the NER dataset. CRF CoNLL refers to the CRF model\\ntrained on general CoNLL data, and CRF FIN5 refers to the CRF model trained on FIN5 data. Source:\\nLi et al. [292].\\nModel FinQA ConvFinQA\\nChatGPT (0) 48.56 59.86\\nChatGPT (3) 51.22 /\\nChatGPT (CoT) 63.87 /\\nGPT-4 (0) 68.79 76.48\\nGPT-4 (3) 69.68 /\\nGPT-4 (CoT) 78.03 /\\nBloombergGPT(0) / 43.41\\nGPT-NeoX(0) / 30.06\\nOPT6B(0) / 27.88\\nBLOOM176B(0) / 36.31\\nFinQANet (fine-tune) 68.90 61.24\\nHuman Expert 91.16 89.44\\nGeneral Crowd 50.68 46.90\\nTable 13: Model performance (accuracy) on the question answering tasks. FinQANet here refers\\nto the best-performing FinQANet version based on RoBERTa-Large [154]. Due to its conservation\\nnature, few-shot and CoT learning cannot be executed on ConvFinQA.\\nthe apex of financial language models, having been trained on a comprehensive dataset of\\nan unprecedented scale within the financial domain. Wu et al. [350] detail the intricacies of\\nBloombergGPTs training regimen, which employed an amalgamation of financial texts, en-\\ncompassing a multitude of formats, and a general dataset to ensure versatility 38 as shown in\\nTable 14.\\nThe core of BloombergGPTs training material involved 363 billion tokens of finance-specific\\ndata, accompanied by a general corpus of 345 billion tokens. The datasets breadth is vast,\\nincorporating textual data spanning web sources, news articles, financial reports, and propri-\\netary content from Bloomberg terminals. This diversified data portfolio enables the model to\\nexpertly navigate the financial lexicon and nuances.\\n38FinPile, a comprehensive dataset consisting of a range of English financial documents including news,\\nfilings, press releases, web-scraped financial documents, and social media drawn from the Bloomberg archives\\naugmented with public available data.\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 35, 'page_label': '36'}, page_content='Dataset Docs C/D Chars C/T Toks T%\\nFINPILE 175,886 1,017 17,883 4.92 6,935 51.27%\\nWeb 158,250 933 14,768 4.96 2,978 42.01%\\nNews 10,040 1,665 1,672 4.44 376 5.31%\\nFilings 3,335 2,340 780 5.39 145 2.04%\\nPress 1,265 3,443 435 5.06 86 1.21%\\nBloomberg 2,996 758 227 4.60 49 0.70%\\nPUBLIC 50,744 3,314 16,818 4.87 3,454 48.73%\\nC4 34,832 2,206 7,683 5.56 1,381 19.48%\\nPile-CC 5,255 4,401 2,312 5.42 427 6.02%\\nGitHub 1,428 5,364 766 3.38 227 3.20%\\nBooks3 19 552,398 1,064 4.97 214 3.02%\\nPubMed Central 294 32,181 947 4.51 210 2.96%\\nArXiv 124 47,819 541 3.56 166 2.35%\\nOpenWebText2 1,684 3,850 648 5.07 128 1.80%\\nFreeLaw 349 15,381 537 4.99 108 1.80%\\nStackExchange 1,538 2,201 339 4.17 81 1.15%\\nDM Mathematics 100 8,193 82 1.92 43 0.60%\\nWikipedia (en) 590 2,988 176 4.65 38 0.53%\\nUSPTO Backgrounds 517 4,339 224 6.18 36 0.51%\\nPubMed Abstracts 1,527 1,333 204 5.77 35 0.50%\\nOpenSubtitles 38 31,055 119 4.90 24 0.34%\\nGutenberg (PG-19) 3 399,351 112 4.89 23 0.32%\\nUbuntu IRC 1 539,222 56 3.16 18 0.25%\\nEuroParl 7 65,053 45 2.93 15 0.21%\\nYouTubeSubtitles 17 19,831 33 2.54 13 0.19%\\nBookCorpus2 2 370,384 65 5.36 12 0.17%\\nHackerNews 82 5,009 41 4.87 8 0.12%\\nPhilPapers 3 74,827 23 4.21 6 0.08%\\nNIH ExPorter 92 2,165 20 6.65 3 0.04%\\nEnron Emails 2 1,882 20 3.90 3 0.04%\\nWikipedia (fr/1/22) 2,218 3,271 76 3.06 237 0.32%\\nTOTAL 226,631 1,531 34,701 4.89 7,089 100.00%\\nTable 14:Breakdown of the full training set used to train BLOOMBERGGPT. The statistics provided\\nare the average number of characters per document (C/D), the average number of characters per\\ntoken (C/T), and the percentage of the overall tokens (T%). Source: Wu et al. [350].\\nWu et al. [350] proffer insights into their methodological choices and their repercussions\\non model performance. The authors used parallel tokenizer training strategies because the\\nUnigram tokenizer was found to be inefficient for processing the entire Pile dataset. So the\\ndataset was split into domains, and each domain was further split into chunks. Every chunk\\nwas tokenized by a separate tokenizer, and then the tokenizer from each domain was merged\\nhierarchically using a weighted average of the probabilities of corresponding tokens. The result\\nwas cut from a tokenizer with 7 million tokens to only 2 17 tokens, dropping tokens with the\\nsmallest probabilities.\\nThe BloombergGPT model is a decoder-only causal language model based on BLOOM [349].\\nThe model contains 70 layers of transformer decoder blocks defined as follows:\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 36, 'page_label': '37'}, page_content='h = h1 + SA(LN(h1))\\nh = h + FFN(LN(h))\\nwhere SA is multi-head self-attention, LN is layer-normalization, and FFN is a feed-forward net-\\nwork with 1 hidden layer. Inside FFN, the non-linear function is GELU [30]. ALiBi positional\\nencoding is applied through additive biases at the self-attention component of the transformer\\nnetwork [180]. The input token embeddings are tied to the linear mapping before the final\\nsoftmax. The model also has an additional layer of normalization after token embeddings.\\nBloombergGPTs prowess was rigorously benchmarked against a suite of established LLMs\\nassessments, financial-specific benchmarks, and a series of internally devised tests. The model\\nexhibited a remarkable ability to outperform existing models on financial NLP tasks, a testa-\\nment to the efficacy of its specialized training as shown in Table 15, Table 16, and Table 17.\\nBloombergGPTs performance on standard, general-purpose benchmarks was also evaluated,\\ndemonstrating its versatility and proficiency across a range of NLP tasks.\\nOverall, while BloombergGPT falls behind the much larger PaLM540B (10parameters) and\\nBLOOM176B (3.5x parameters), it is the best-performing among similarly sized models. In fact,\\nits performance is closer to BLOOM 176B than it is to either GPT-NeoX or OPT 66B.\\nIn sum, according to benchmarks in Wu et al. [350], developing finance-specific BloombergGPT\\ndid not come at the expense of its general-purpose abilities.\\nBLOOMBERGGPT GPT-NeoX OPT 66B BLOOM176B\\nConvFinQA 43.41 30.06 27.88 36.31\\nFiQA SA 75.07 50.59 51.60 53.12\\nFPB 51.07 44.64 48.67 50.25\\nHeadline 82.20 73.22 79.41 76.51\\nNER 60.82 60.98 57.49 55.56\\nAll Tasks (avg) 62.51 51.90 53.01 54.35\\nAll Tasks (WR) 0.93 0.27 0.33 0.47\\nTable 15: Results on financial domain tasks. Source: Wu et al. [350].\\nBLOOMBERGGPT GPT-NeoX OPT 66B BLOOM176B\\nEquity News 79.63 14.17 20.98 19.96\\nEquity Social Media 72.40 66.48 71.36 68.04\\nEquity Transcript 65.06 25.08 37.58 34.82\\nES News 46.12 26.99 31.44 28.07\\nCountry News 49.14 13.45 17.41 16.06\\nAll Tasks (avg) 62.47 29.23 35.76 33.39\\nAll Tasks (WR) 1.00 0.00 0.67 0.33\\nTable 16: Results on internal aspect-specific sentiment analysis datasets. BLOOMBERGGPT far\\noutperforms all other models on sentiment analysis tasks. Source: Wu et al. [350].\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 37, 'page_label': '38'}, page_content='BLOOMBERGGPT GPT-NeoX OPT 66B BLOOM176B\\nNER\\nBFW 72.04 71.66 72.53 76.87\\nBN 57.31 52.83 46.87 59.61\\nFilings 58.84 59.26 59.01 64.88\\nHeadlines 53.61 47.70 46.21 52.17\\nPremium 60.49 59.39 57.56 61.61\\nTranscripts 75.50 70.62 72.53 77.80\\nSocial Media 60.60 56.80 51.93 60.88\\nAll Tasks (avg) 62.63 59.75 58.09 64.83\\nAll Tasks (WR) 0.57 0.29 0.19 0.95\\nNER+NED\\nBFW 55.29 34.92 36.73 39.36\\nBN 60.09 44.71 54.60 49.85\\nFilings 66.67 31.70 65.63 42.93\\nHeadlines 67.17 36.46 56.46 42.93\\nPremium 64.11 40.84 57.06 42.11\\nTranscripts 73.15 23.65 70.44 34.87\\nSocial Media 67.34 62.57 70.57 65.94\\nAll Tasks (avg) 64.83 39.26 58.79 45.43\\nAll Tasks (WR) 0.95 0.00 0.67 0.38\\nTable 17: Results on internal NER and NED datasets. On NER, while the much larger\\nBLOOM176b model outperforms all other models, results from all models are relatively close, with\\nBLOOMBERGGPT outperforming the other two models. On NER+NED, BLOOMBERGGPT out-\\nperforms all other models by a large margin. Source: Wu et al. [350].\\n2.4.3 LLMs in Education\\nThe advent of LLMs has significantly impacted education. LLMs can be leveraged to create\\neducational content tailored to individual student needs, providing explanations, generating\\npractice problems, and even offering feedback.\\nIntegrating LLMs into educational frameworks offers a rich tapestry of potential enhance-\\nments to teaching and learning experiences. The transformative influence of such technology is\\nparticularly marked in tasks that can benefit from automation, such as grading and personal-\\nized feedback on student work. Through their nuanced understanding of language, LLMs can\\nprovide insightful assessments that highlight the strengths and weaknesses in student assign-\\nments, which may span essays, research papers, and various other forms of written submissions.\\nAn additional benefit is LLMs capacity to detect plagiarism, bolsters academic evaluations\\nintegrity by mitigating the risk of academic dishonesty. This ability to provide quick and pre-\\ncise feedback can afford educators more time to address individual student needs, leading to a\\nmore targeted and effective teaching approach.\\nLLMs can achieve student-level performance on standardized tests [370] in a variety of\\nsubjects of mathematics (e.g., physics, computer science) on both multiple-choice and free-\\nresponse problems. Additionally, these models can assist in language learning, both for native\\nspeakers and language acquisition, due to their deep understanding of linguistic structures and\\nidiomatic expressions.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 38, 'page_label': '39'}, page_content='In the realm of intelligent tutoring systems, LLMs can be applied to simulate one-on-one\\ninteraction with a tutor, adapting to the students learning pace, style, and current level of\\nknowledge. These systems can engage in dialogue, answer student queries, and provide expla-\\nnations, much like a human tutor would [305, 217].\\nFurthermore, LLMs have the capacity to automate the grading process by evaluating open-\\nended responses in exams and assignments. This approach can free up time for educators to\\nfocus on more personalized teaching methods and direct student engagement.\\nThe intersection of LLMs and education also extends to research, where these models can\\naid in summarizing literature, generating hypotheses, and even writing research proposals or\\npapers, albeit with careful oversight to ensure academic integrity.\\nIn administrative and support roles, LLMs can streamline communication with students,\\nhandle routine inquiries, and manage scheduling and reminders, enhancing the overall educa-\\ntional experience for students and faculty.\\nTo tap into the full potential of LLMs in education, it is crucial to address challenges such\\nas ensuring the reliability of the information provided, avoiding biases, and maintaining privacy\\nand security, especially in data-sensitive environments like schools and universities.\\n2.4.4 LLMs in Law\\nThe legal sector is another domain that the advent of LLMs has significantly impacted. A num-\\nber of tasks in the legal field, such as legal document analysis [253]), legal judgment prediction\\n(Trautmann, Petrova, and Schilder [222]), and legal document writing (Choi et al. [264], can\\nbe solved by LLMs with high accuracy and efficiency.\\nFigure 13: Prompts used in Blair-Stanek, Holzenberger, and Durme [253] to pose SARA test cases\\nto GPT-3. The top boxes, in orange, contain statutes (optional). Example cases are in blue; in zero-\\nshot, no example cases exist. At the bottom, in green, are test cases. The text highlighted in yellow\\nis generated by GPT-3. If GPT-3s first response is unclear, the second prompt with Therefore the\\nanswer is is used, following Kojima et al. [285]. Source: Trautmann, Petrova, and Schilder [222].\\nBlair-Stanek, Holzenberger, and Durme [253] evaluates the capacity of OpenAIs GPT-3\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 39, 'page_label': '40'}, page_content='model, specifically text-davinci-003, to perform statutory reasoning39, a fundamental skill in le-\\ngal practice, on an established dataset known as SARA (StAtutory Reasoning Assessment). The\\ninvestigation includes several approaches like dynamic few-shot prompting, chain-of-thought\\nprompting, and zero-shot prompting (examples in Figure 13).\\nThe model surpasses previous benchmarks yet still exhibits considerable room for improve-\\nment, especially when handling simple synthetic statutes, revealing limitations in its current\\nstatutory reasoning capabilities even though GPT-3 has some prior knowledge of the U.S. Code.\\nMethod Constitutional Law Taxation Torts Total\\nSimple 21/25 24/60 6/10 51/95\\nCoT 21/25 18/60 5/10 44/95\\nRank Order 20/25 21/60 6/10 47/95\\nTable 18: Comparison of Multiple Choice Methods. Source: Choi et al. [264].\\nChoi et al. [264] explored ChatGPTs ability to write law school exams at the University of\\nMinnesota Law School, encompassing multiple choice and essay questions across four courses.\\nChatGPT generated answers for Constitutional Law, Employee Benefits, Taxation, and Torts\\nexams, with varying question formats across these subjects. These answers were blindly graded\\nin line with the standard grading process. ChatGPT managed to pass all four classes, averaging\\na C+ grade, demonstrating better performance on essay questions compared to multiple-choice,\\nwith notable strengths in organizing and composing essays (Table 18).\\nDespite its overall passing performance, ChatGPT ranked at or near the bottom in each\\nclass. The models essays showcased a strong grasp of basic legal rules but struggled with issue\\nspotting and detailed application of rules to facts. The findings suggest that while ChatGPT\\ncan assist in legal education and potentially in legal practice, it currently lacks the nuanced\\nunderstanding and depth of reasoning required for high-level legal analysis.\\nRecent studies on the latest GPT-4 model have shown that it can achieve a top 10% score\\nin a simulated bar exam compared with human test-takers [370], while Nay [202] and exhibit\\npowerful abilities of legal interpretation and reasoning. To further improve the performance\\nof LLMs in the law domain, specially designed legal prompt engineering is employed to yield\\nadvanced performance in long legal document comprehension and complex legal reasoning [364].\\n2.4.5 LLMs in Scientific Research\\nLLMs in scientific research can be employed across various stages of the research process, from\\nliterature review to hypothesis generation, brainstorming, data analysis, manuscript drafting,\\nproofreading, and peer review. Empirical evidence underscores the aptitude of LLMs in man-\\naging tasks dense with scientific knowledge, such as those presented by PubMedQA [69] and\\nBioASQ [178]. It is particularly true for LLMs pre-trained on scientific corpora, including, but\\nnot limited to, Galactica [220] and Minerva [182].\\nDue to their capabilities, LLMs are poised to play an integral role as supportive tools\\nthroughout the scientific research process [362]. During the initial stages of research, such as\\nbrainstorming, LLMs can assist in generating novel research ideas and hypotheses, thereby\\nfostering creativity and innovation40.\\n39Statutory reasoning is the application of legal rules written by legislative bodies to facts that are also in\\nnatural language\\n40Randomness and creativity of a model can be tuned using specific parameters. For example temperature is\\na OpenAI ChatGPT, GPT-3 and GPT-4 models parameter that govern the randomness and thus the creativity\\nof the responses.\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 40, 'page_label': '41'}, page_content='In the literature review phase, LLMs can perform exhaustive reviews, encapsulating the\\nstate of advancement within specific scientific disciplines [274, 146] providing explanations for\\nscientific texts and mathematics with follow-up questions.\\nProgressing to the phase of research ideation, LLMs have displayed potential in formulat-\\ning compelling scientific hypotheses [311]. In Park et al. [311], the authors shows the ability\\nof GPT-4 to generate hypotheses in the field of materials science, showcasing the models ca-\\npacity to propose research directions. Through examining conversations, it was evident that\\nGPT-4 generates richer and more specific information than the prompts provided, disproving\\nthe mirroring hypothesis. While checking for verbatim copying was more challenging, GPT-4\\ndoes seem to reflect current academic trends to an uncanny degree. However, it also combines\\ndisciplines and innovates concepts, leading to both errors and genuine creative insights. The\\nauthors compared the process to how cosmic rays can drive biological evolution through muta-\\ntions: radiations break DNA strands and cause cancer and deaths, but can also drive mutations\\nand evolution of the biosphere. Given the highlighted limitation, LLMs can be used to generate\\nhypotheses for further human evaluation and refinement.\\nIn the subsequent data analysis stage, LLMs can be harnessed for automating the examina-\\ntion of data attributes, including exploratory data analysis, visualization, and the extraction of\\nanalytical inferences [261]. In Hassan, Knipper, and Santu [276], the authors demonstrate the\\nutility of GPT-4 in automating data analysis tasks, such as data cleaning, feature engineering,\\nand model selection, thereby streamlining the data science workflow.\\nRegarding proofreading, LLMs can enhance the quality of scientific manuscripts by identify-\\ning grammatical errors, improving readability, and ensuring adherence to academic conventions.\\nIn addition, LLMs can go beyond helping users check grammar and can further generate reports\\nabout document statistics, vocabulary statistics, etc, change the language of a piece to make it\\nsuitable for people of any age, and even adapt it into a story [177]. While ChatGPT has some\\nusability issues when it comes to proofreading, such as being over 10 times slower than DeepL\\nand lacking in the ability to highlight suggestions or provide alternative options for specific\\nwords or phrases [306], it should be noted that grammar-checking is just the tip of the iceberg.\\nChatGPT can also be valuable in improving language, text restructuring, and other writing\\naspects.\\nFurthermore, in the manuscript drafting phase, the utility of LLMs extends to aiding sci-\\nentific writing endeavors [279, 251], offering a multitude of services such as condensing existing\\nmaterials and refining the written prose [255]. As explained in Buruk [255] and Hussam Alkaissi\\n[279], LLMs can assist in generating abstracts, introductions, and conclusions, thereby enhanc-\\ning the overall quality of scientific manuscripts.\\nFinally, in the peer review process, LLMs can contribute to automating the peer review pro-\\ncess, undertaking tasks like error identification, compliance with checklists, and prioritization\\nof submissions [298].\\nLLMs utility spans beyond the aforementioned domains, with their deployment also being\\nexplored in the psychological sphere. Here, studies have argued LLMs for human-like traits,\\nencompassing self-perception, Theory of Mind (ToM) 41, and emotional cognition [287, 248].\\nKosinski [287] employs classic false-belief tasks 42, revealing a marked improvement in ToM ca-\\n41The ability to impute unobservable mental states to others\\n42A false-belief task is a psychological test used to assess an individuals ability to understand that others\\ncan have beliefs about the world that are different from their own and that these beliefs can be incorrect.\\nThis ability is a crucial component of the Theory of Mind (ToM), which is the capacity to attribute mental\\nstatesbeliefs, intents, desires, emotions, knowledge, etc.to oneself and others and to understand that others\\nhave beliefs, desires, and intentions that are different from ones own.\\nThe classic example of a false-belief task is the Sally-Anne test, used primarily with children. The test involves\\ntwo dolls, Sally and Anne. Sally has a basket, while Anne has a box. In the presence of Sally, a marble is\\nplaced in Sallys basket. Sally then leaves the room, and while shes away, Anne takes the marble from Sallys\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 41, 'page_label': '42'}, page_content='pabilities in more recent versions of GPT-3. Specifically, the davinci-002 version solved 70% of\\nToM tasks, while the davinci-003 version achieved a 93% success rate, demonstrating perfor-\\nmances akin to seven and nine-year-old children, respectively. Notably, GPT-3.5s performance\\nin ToM assessments parallels that of nine-year-olds, suggesting nascent ToM capabilities in\\nLLMs. The study hypothesizes that ToM-like abilities might emerge spontaneously in AI with-\\nout explicit programming, especially in LLMs trained in human language. In the context of\\nAI, particularly in LLMs like GPT-3, the ability to perform well on false-belief tasks suggests\\na sophisticated level of language understanding and a rudimentary form of Theory of Mind,\\nalbeit not conscious or sentient like in humans. It is unsurprising that the initial enthusiasm\\nsurrounding the anecdotal performance of LLMs on reasoning tasks has somewhat waned due to\\na wave of recent studies questioning the robustness of these abilitieswhether in planning [333,\\n379], basic arithmetic and logic [266], theory of mind [331, 386], or broader mathematical and\\nabstract benchmarks [269, 307].\\nMoreover, the application of LLMs in software engineering is also gaining traction, with\\ninitiatives in code suggestions [323], code summarizations [325], and automated program re-\\npairs [352].\\n3 Foundations of Large Language Models\\nLarge Language Models (LLMs) have revolutionized the field of Natural Language Processing\\n(NLP) by achieving state-of-the-art performance on a wide range of tasks, such as text gener-\\nation, text classification, and machine translation. These models are trained on vast amounts\\nof text data to learn the underlying structure of the language and capture the relationships\\nbetween words.\\nIn the following sections, we will explore the key concepts and techniques that underpin the\\ndevelopment of LLMs, including pre-training strategies and major datasets used for training\\nand evaluation, as well as the Transformer architecture, which forms the basis of many modern\\nLLMs.\\nAfter that, we will discuss some model adaptation techniques for fine-tuning LLMs for\\nspecific tasks or domains.\\nFinally, we will discuss the tuning and quantization of LLMs, techniques used to reduce\\nthe models size and computational complexity, making it more efficient for deployment on\\nresource-constrained devices.\\n3.1 Pre-training\\nPre-training constitutes a foundational phase in developing Large Language Models (LLMs).\\nIt allows the model to capture the relationships between words and generate coherent and\\ncontextually relevant text, laying the groundwork for its subsequent performance on specific\\nNLP tasks [65, 88]. This phase involves training a language model on a vast corpus of text\\ndata before fine-tuning it on a smaller, task-specific dataset, such as text generation or text\\nclassification, to improve its performance on that task. Moreover, the extensive pre-training\\non diverse corpora enables LLMs to develop a broad understanding, making them adaptable\\nto a wide range of domains and languages [73, 75]. Despite its advantages, LLM pre-training\\nbasket and puts it in her box. The child is then asked where Sally will look for the marble when she returns.\\nThe correct answer is Sallys basket, where she left the marble. A child who can correctly predict where Sally\\nwill look for the marble demonstrates an understanding that Sally holds a false belief about the location of the\\nmarble. Successfully completing a false-belief task indicates that the individual can understand that others can\\nhold false beliefs and that these beliefs can influence their actions, a critical step in the development of social\\ncognition and empathy.\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 42, 'page_label': '43'}, page_content='is not without its challenges. The process requires substantial computational resources and\\nenergy, raising concerns about its environmental impact [80]. Additionally, the data used for\\npre-training can influence the models biases and sensitivities, necessitating careful curation of\\nthe training corpus to mitigate potential ethical and fairness issues [106].\\nThe field is evolving towards more efficient pre-training methods, such as transfer learn-\\ning, where a pre-trained model is adapted to new tasks or languages with minimal additional\\ntraining [76]. Moreover, emerging approaches aim to enhance LLMs contextual awareness and\\nethical sensitivity during the pre-training phase, addressing the challenges of bias and fairness.\\nSeveral pre-training strategies have been used to train large language models, including un-\\nsupervised, supervised, and semi-supervised pre-training. Lets explore each of these strategies\\nin more detail.\\n3.1.1 Unsupervised pre-training\\nUnsupervised pre-training is a pre-training strategy involving training a model on a large corpus\\nof text data without labels or annotations.\\nThe model is trained to predict the next word, given the previous words in the sequence [88].\\nThis is done using a technique called Autoregressive Language Modeling (ALM), where the\\nmodel is trained to predict the probability distribution over the next word in the sequence\\ngiven the previous words in the sequence in a unidirectional manner.\\nModels like GPT-3 and its variants use this autoregressive language modelling objective to\\npre-train over large text corpora and learn the relationships between words in the language.\\nThe main idea behind ALM is to predict the next token in a sequence based on the tokens\\nthat precede it. The computational realization of this modelling approach is typically achieved\\nthrough neural networks, particularly transformers, which leverage self-attention mechanisms\\nto encapsulate dependencies across varying distances in the input sequence [334].\\nDuring the generation process, a token is sampled based on the probability distribution pre-\\ndicted by the model for the next token position, appended to the sequence, and this augmented\\nsequence is then fed back into the model iteratively to generate subsequent tokens [88]. Despite\\nits prowess, the autoregressive nature of these models imbues them with an intrinsic limitation:\\nthe inability to leverage future context in token prediction, constraining their context compre-\\nhension to a unidirectional scope.\\nBERT and its variants, on the other hand, employ a masked language model (MLM) objec-\\ntive, where random words in a sentence are masked, and the model is trained to predict these\\nmasked words based on their context, integrating both preceding and succeeding context in\\nrepresentation learning [65].\\n3.1.2 Supervised pre-training\\nSupervised pre-training is a pre-training strategy that involves training a model on a large\\ncorpus of text data with labels or annotations. This paradigm contrasts with unsupervised pre-\\ntraining, where models learn from raw text without explicit labels. The supervised approach\\nenables models to learn representations more closely aligned with the end tasks, potentially\\nenhancing their performance and efficiency [89].\\nIn supervised pre-training, LLMs are exposed to a vast array of labelled data across various\\ndomains. This training regime involves teaching the model to predict the correct output given\\nan input under the supervision of known input-output pairs. This approach helps in learning\\ngeneral language representations and imbues the model with domain-specific knowledge, which\\nis particularly beneficial when the subsequent fine-tuning task is closely related to the pre-\\ntraining data [74].\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 43, 'page_label': '44'}, page_content='Figure 14: Using only the minimal labelled data points available, a supervised model may learn a\\ndecision boundary that will generalize poorly and be prone to misclassifying new examples. Source:\\nBergmann [252].\\nOne significant advantage of supervised pre-training is its potential to reduce the labelled\\ndata required for fine-tuning over specific tasks. By learning robust representations during pre-\\ntraining, LLMs can achieve high performance on downstream tasks even with relatively smaller\\ndatasets, a concept known as transfer learning [76]. Moreover, supervised pre-training can lead\\nto improvements in model generalization, making LLMs more adept at handling unseen data\\nor tasks that diverge from their initial training corpus.\\nThe reliance on large labelled datasets introduces concerns regarding the cost and feasibility\\nof data annotation, especially in specialized domains where expert knowledge is required.\\nFurthermore, as shown in Figure 14, the risk of overfitting to the pre-training data is non-trivial,\\nnecessitating careful regularization and validation to ensure the models generalizability [45].\\n3.1.3 Semi-supervised pre-training\\nSemi-supervised pre-training emerges as a compelling paradigm in the evolution of Large Lan-\\nguage Models (LLMs), blending the strengths of supervised and unsupervised learning method-\\nologies. This hybrid training approach leverages a combination of labelled and unlabeled data,\\noptimizing the utilization of available information and enhancing the models learning efficacy\\nand adaptability [10, 14].\\nSemi-supervised pre-training involves the initial training of models using a vast corpus of\\nunlabelled data akin to unsupervised pre-training. This phase allows the model to capture a\\nbroad understanding of language structures and patterns. Subsequently, the model undergoes\\nfurther training or fine-tuning on a smaller labelled dataset, instilling task-specific knowledge\\nand nuances [76, 42]. The rationale behind this approach is to exploit the abundance of readily\\navailable unlabeled data to develop a comprehensive language model, which is then refined\\nusing the more scarce labelled data to achieve superior performance on target tasks.\\nVarious techniques underpin semi-supervised pre-training in LLMs. One prominent method\\ninvolves self-training, where the model, initially trained on labelled data, generates pseudo-\\nlabels for the unlabeled dataset. These pseudo-labeled data points are then incorporated into\\nfurther training cycles, iteratively enhancing the models accuracy and robustness [19].\\nAnother notable technique is the use of consistency regularization, which ensures that the\\nmodel produces similar outputs for perturbed versions of the same input data, enhancing the\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 44, 'page_label': '45'}, page_content='models stability and generalization capabilities [34].\\nOther key techniques in semi-supervised learning include transductive and inductive learn-\\ning, with practical methods like label propagation and active learning aiding in leveraging\\nunlabeled data. These approaches are instrumental in refining the models decision-making\\ncapabilities [252].\\nTransductive learning, a concept primarily attributed to Vapnik [3], focuses on predicting\\nspecific examples from the training set without attempting to generalize beyond those. In\\ntransductive inference, the model is directly applied to the specific test set to infer the correct\\nlabels for the given unlabeled data. The key characteristic distinguishing transductive learning\\nfrom other machine learning methods is its focus on the particular sample rather than a general\\nrule applicable to new, unseen instances. One of the main applications of transductive learning\\nis in the realm of support vector machines (SVMs), where it is employed to predict labels for a\\ngiven, fixed set of test data, optimizing the margin not only for the training data but also for\\nthe test data, despite their labels being unknown [4].\\nConversely, inductive learning aims to build a general model that predicts outcomes for\\nnew, unseen data based on the patterns learned from the training data. Label propagation\\n(Figure 15) is a common technique in inductive learning, where the model infers the labels of\\nunlabeled data points based on the labels of their neighbours in the feature space.\\nFigure 15: LEFT: original labelled and unlabeled data points. RIGHT: using label propagation, the\\nunlabeled data points have been assigned pseudo-labels. Source: Bergmann [252].\\nActive learning is another inductive learning method that involves iteratively selecting the\\nmost informative data points for labelling and optimizing the models performance with minimal\\nlabelled data. This approach is more general than transductive learning and underpins most\\nsupervised learning algorithms. The objective is to infer a function that can generalize well\\nacross unseen samples, not just the examples provided during the training phase. Inductive\\nlearning is fundamental to numerous machine learning algorithms, from linear regression to\\ndeep neural networks, where the model learns an underlying function that maps input data\\nto output predictions, with the hope that this function will perform accurately on data not\\npresent in the training set [2].\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 45, 'page_label': '46'}, page_content='The semi-supervised approach is predicated on certain assumptions about the underlying\\nstructure and distribution of the data, which facilitate the effective integration of unlabeled\\ndata into the learning process.\\n Cluster Assumption: The cluster assumption posits that data points within the same\\ncluster are more likely to share a label. This assumption underpins the idea that data\\npoints in high-density regions of the input space belong to the same class, while low-\\ndensity regions denote boundaries between classes [14]. This principle guides the model\\nin generalizing from labelled data points to nearby unlabeled ones within the same cluster.\\n Continuity Assumption: Also known as the smoothness assumption, this posits that\\nif two points in the input space are close to each other, then their corresponding outputs\\nare also likely to be similar [9]. In practical terms, if two data points are close in the\\nfeature space, they will likely share the same label.\\n Manifold Assumption: The manifold assumption suggests that high-dimensional data\\nlie on a low-dimensional manifold. This assumption implies that the data points are situ-\\nated on a manifold of much lower dimensionality embedded within the higher-dimensional\\nspace, and learning can be simplified if this manifold structure is discovered and ex-\\nploited [11]. The manifold assumption often complements the cluster and continuity\\nassumptions, providing a geometric interpretation of the datas distribution.\\n Low-Density Separation Assumption: This assumption posits that the decision\\nboundary between different classes should lie in regions of low data density [14]. Es-\\nsentially, there is expected to be a natural separation or gap between classes, and the\\nlearning algorithm should prefer hypotheses that place the decision boundary in regions\\nwith few data points.\\n3.2 Data sources\\nLarge Language Models (LLMs) strongly depend on extensive, high-calibre data for pre-training,\\nwith their efficacy closely tied to the nature and preprocessing of the utilized corpora. The pri-\\nmary sources of data for training and evaluating LLMs can be broadly categorized into general\\nand specialized datasets, each serving distinct purposes in enhancing the models capabili-\\nties [364].\\n3.2.1 General Data\\nThis category typically encompasses web content, literary works, and conversational texts,\\nprized for their voluminous, varied, and accessible nature, thereby bolstering LLMs language\\nmodelling and generalization prowess. Including general data, such as web pages and books,\\noffers a rich lexicon spanning various themes, essential for the comprehensive training of LLMs.\\nAs shown in Figure 16, general-purpose data are among the most commonly used general data\\nsources for training LLMs.\\nThree important general data sources are:\\n Webpages: Web content, extracted from the internet, is a valuable source of diverse\\nand up-to-date text data, encompassing news articles, blog posts, and forum discussions.\\nThis data is instrumental in training LLMs to gain different linguistic knowledge and\\nenhance generalization capabilities [88, 99]. Crawled web data tends to contain a mix\\nof high-quality and noisy text, necessitating careful preprocessing to ensure the datas\\nquality and relevance.\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 46, 'page_label': '47'}, page_content='Figure 16:Commonly-used data sources for training and evaluating Large Language Models (LLMs).\\nSource: Zhao et al. [364].\\n Conversation text: Conversation text, including chat logs and social media interac-\\ntions, provides a rich source of informal language and colloquial expressions, enabling\\nLLMs to capture the nuances of human communication [241]. This data is particularly\\nuseful for training LLMs on question answering [155] and sentiment analysis tasks [82].\\nConversational data often involve multiple speakers, so an effective way is to transform the\\nconversation into a tree structure, where the utterance is linked to the one it is replying to.\\nThe tree can be divided into multiple subtrees, each one representing a sub-conversation,\\nwhich can be collected in the pre-training corpus. Overtraining on conversational data can\\nlead to the model to a performance decline since the declarative instructions and direct\\ninterrogatives can be erroneously interpreted as the beginning of a conversation [241].\\n Books: Books, comprising novels, essays, and scientific literature, offer a rich source\\nof long structured and coherent text data, enabling LLMs to learn complex language\\nstructures and thematic nuances [27]. This data is instrumental in training LLMs on\\nliterary text generation tasks and enhancing their proficiency in narrative comprehension\\nand storytelling [75].\\n3.2.2 Specialized Data\\nTailored to refine LLMs proficiency in particular tasks, specialized datasets encompass mul-\\ntilingual text, scientific literature, and programming code. Specialized datasets are useful to\\nimprove the specific capabilities of LLMs on downstream tasks. Next, we introduce three kinds\\nof specialized data.\\n Multilingual text: Multilingual text data, spanning multiple languages and dialects,\\nis crucial for training LLMs to understand and generate text in diverse linguistic con-\\ntexts [364]. This data is instrumental in enhancing the models cross-lingual capabil-\\nities and enabling them to perform translation tasks across different languages [364].\\nBLOOM [349] and PaLM [155] are two models that have been trained on multilingual\\ntext data to improve their performance on cross-lingual tasks. They have impressive\\nperformances on translation, multilingual question answering, and cross-lingual summa-\\nrization tasks, and they achieve comparable or superior results to models fine-tuned in\\nspecific languages.\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 47, 'page_label': '48'}, page_content=' Scientific literature: Scientific literature, encompassing research papers, patents, and\\ntechnical documents, provides a rich source of domain-specific text data essential for train-\\ning LLMs on scientific text generation and reasoning tasks [364, 220, 182]. Existing efforts\\nto build the scientific corpus for training LLMs mainly collect arXiv papers, scientific text-\\nbooks, math web pages, and other related scientific resources. Data in scientific fields are\\ncomplex, commonly including mathematical symbols and protein sequences, so specific\\ntokenization and preprocessing techniques are required to transform these different data\\nformats into a unified form that language models can process.\\n Code: Code, which includes source code snippets and software documentation, serves as\\na critical source of structured text data for training LLMs in tasks such as code generation\\nand completion [364, 203]. Typically, this data is gathered from open-source platforms\\nlike GitHub and StackOverflow to enable LLMs to generate code snippets, complete par-\\ntial code, and perform code summarization tasks. Studies [108, 105] demonstrate that\\nmodels trained on code data can achieve high accuracy and efficiency in generating code,\\nsignificantly enhancing code completion performance. Generated code has shown the\\nability to pass expert-designed unit tests [108] and solve competitive programming prob-\\nlems [184]. Two primary types of code corpora are generally utilized: question-answering\\ndatasets, such as those from Stack Exchange [235], and public software repositories like\\nGitHub [108], which provide code, comments, and docstrings for training purposes.\\n3.2.3 Commonly-used data sources.\\nThe development and evaluation of Large Language Models (LLMs) rely heavily on the avail-\\nability of high-quality datasets that span diverse domains and languages. The datasets in\\nTable 19 serve as the foundation for pre-training and fine-tuning LLMs, enabling researchers to\\nassess the models performance on a wide range of tasks, from text generation to translation.\\nCorpora Size Source Update Time\\nBookCorpus [27] 5GB Books Dec-2015\\nGutenberg [392] - Books Dec-2021\\nC4 [99] 800GB CommonCrawl Apr-2019\\nCC-Stories-R [55] 31GB CommonCrawl Sep-2019\\nCC-NEWS [73] 78GB CommonCrawl Feb-2019\\nREALNEWS [82] 120GB CommonCrawl Apr-2019\\nOpenWebText [67] 38GB Reddit links Mar-2023\\nPushift.io [87] 2TB Reddit links Mar-2023\\nWikipedia [393] 21GB Wikipedia Mar-2023\\nBigQuery [390] - Codes Dec-2023\\nthe Pile [111] 800GB Other Dec-2020\\nROOTS [179] 1.6TB Other Jun-2022\\nTable 19: Statistics of commonly-used data sources. Source: Zhao et al. [364]\\nThis section will explore some of the most commonly used data sources for training and\\nevaluating LLMs. Based on their content types, we categorize these corpora into six groups:\\nBooks, CommonCrawl, Reddit links, Wikipedia, Code, and others.\\n Books: BookCorpus [27] and Gutenberg [392] are two prominent datasets that contain\\ntext from a wide range of books spanning various genres and topics. These datasets\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 48, 'page_label': '49'}, page_content='are valuable for training LLMs on literary text and assessing their performance on text\\ngeneration tasks.\\nBookCorpus is a dataset consisting of text from over 11,000 books (e.g., novels and\\nbiographies), while Gutenberg is a collection of over 70,000 free ebooks, including novels,\\nessays, poetry, drama, history, science, philosophy, and other types of works, in the public\\ndomain.\\nBookCorpus is commonly used in previous small-scale models (e.g., GPT [51] and GPT-\\n2 [75]), while Gutenberg is used in more recent large-scale models (i.e., Llama [330]).\\nBook1 and Book2 used in GPT-3 [88] are much larger than BookCorpus but have not\\nbeen publicly released.\\n CommonCrawl: CommonCrawl [391] is a vast web corpus that contains data from\\nbillions of web pages covering diverse topics and languages. Due to noise and redundancy\\nin the data, researchers often extract subsets of CommonCrawl for training LLMs. The\\nmain subsets used for training LLMs are C4 43 [99], CC-Stories-R [55], CC-NEWS [73],\\nand REALNEWS [82].\\n Reddit links: Reddit is a social media platform where users can submit links and posts\\nand upvote or downvote them. Posts with a high number of upvotes are often\\nconsidered useful and can be used to create high-quality datasets. OpenWebText [67]\\nand Pushshift.io [87] are datasets that contain text data extracted from Reddit. These\\ndatasets are useful for training LLMs on social media text and assessing their performance\\non text generation and sentiment analysis tasks.\\n Wikipedia: Wikipedia [393] is a widely-used dataset containing text from various ar-\\nticles. Its an online encyclopedia with a large volume of high-quality articles. Most of\\nthese articles are composed in an expository style of writing (with supporting references),\\ncovering a wide range of languages and fields. Typically, the English-only filtered ver-\\nsions of Wikipedia are widely used in most LLMs (e.g., GPT-3 [88], and LLaMA [330]).\\nWikipedia is available in multiple languages and can be used in multilingual settings.\\n Code: Two major sources are GitHub, for open-source licensed code, and StackOver-\\nflow, for code-related question-answering platforms.\\nGoogle has publicly released BigQuery [390], a dataset that contains code snippets from\\nvarious programming languages. This dataset is useful for training LLMs (i.e., Code-\\nGen [203]) on code text and assessing their performance on code generation and code\\ncompletion tasks.\\n Others: The Pile [111] and ROOTS [179] are datasets that contain text data from\\nvarious sources, such as books, articles, and websites.\\nThe Pile contains 800GB of data from multiple sources, including books, websites, codes,\\nscientific papers, and social media platforms. Its widely used in training LLMs with\\ndifferent sizes (e.g., CodeGen(16B) [203] and Megatron-Turing NLG(530B) [214]).\\nROOTS comprises various smaller datasets (a total of 1.61 TB of text) in 59 different\\nlanguages (containing natural languages and programming languages). Its been used for\\ntraining BLOOM [349].\\nA mixture of these datasets is often used to train LLMs, as they provide a diverse range\\nof text data (Figure 16). The choice of datasets depends on the specific task and domain of\\ninterest and the computational resources available for training the model. Furthermore, to\\n43Colossal Clean Crawled Corpus\\n49'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 49, 'page_label': '50'}, page_content='train LLMs that are adaptative to specific tasks or domains, it is also important to consider\\nthe data sources that are relevant to them.\\n3.3 Data preprocessing\\nAfter collecting the data, the next step is to preprocess it to ensure that it is clean, consistent,\\nand ready for training Large Language Models (LLMs), removing noise and irrelevant or po-\\ntentially toxic information [155, 131, 299]. In Chen et al. [257], the authors propose a new data\\npreprocessing system, DataJuicer, that can be used to improve the quality of the processed\\ndata.\\nA typical pipeline for data preprocessing involves several steps, as shown in Figure 17:\\nFigure 17: Common data preprocessing steps for training Large Language Models (LLMs). Source:\\nZhao et al. [364].\\n3.3.1 Quality Filtering.\\nThe first step in data preprocessing is quality filtering, where the data is cleaned to remove ir-\\nrelevant or low-quality content. Existing works mainly adopt two strategies for quality filtering:\\nclassifier-based and heuristic-based filtering.\\nThe former approach involves training a classifier to distinguish between high-quality and\\nlow-quality data, using well-curated data (e.g., Wikipedia pages) as positive examples and noisy\\ndata (e.g., spam or irrelevant content) as negative examples. Rae et al. [131] and Du et al.\\n[161] find that classifier-based filtering may remove high-quality data in dialect, colloquial, and\\nsociolectal44 languages, which potentially leads to bias in the pre-training data and diminishes\\nthe corpus diversity.\\nOn the other hand, heuristic-based filtering involves setting predefined rules to identify and\\nremove noisy data [349, 131]. The set of rules can be summarized as follows:\\n Language based filtering. Remove data that is not in the target language.\\n Metric based filtering. Remove data that does not meet certain quality metrics, e.g., per-\\nplexity, readability, or coherence. Perplexity (PPL) is one of the most common metrics for\\nevaluating language models. This metric applies specifically to classical language models\\n(sometimes called autoregressive or causal language models) and is not well-defined for\\nmasked language models like BERT [65]. Perplexity is defined as the exponential average\\nnegative log-likelihood of a sequence.\\n44In sociolinguistics, a sociolect is a form of language or a set of lexical items used by a socioeconomic\\nclass, profession, age group, or other social group. Sociolects involve both passive acquisition of particular\\ncommunicative practices through association with a local community, as well as active learning and choice\\namong speech or writing forms to demonstrate identification with particular groups. Source: Wikipedia [393]\\n50'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 50, 'page_label': '51'}, page_content='If we have a tokenized sequence X = x1, x2, . . . , xt, the perplexity of the sequence is\\ndefined as:\\nP P L(X) = exp\\n(\\n1\\nt\\ntX\\ni\\nlog p(xi|x<i)\\n)\\n(6)\\nwhere log p(xi|x<i) is the log-likelihood of the token xi given the previous tokens x<i in\\nthe sequence. Intuitively, it can be thought of as an evaluation of the models ability to\\npredict uniformly among the set of specified tokens in a corpus 45 [314].\\n Statistic based filtering. Statistical features like punctuation distribution, symbol-to-word\\nratio, and sentence length can be used to filter out low-quality data.\\n Keyword based filtering . Remove data that contains specific keywords that are noisy,\\nirrelevant or toxic, like HTML tags, URLs, boilerplate text, or offensive language.\\n3.3.2 Deduplication.\\nThe next step in data preprocessing is deduplication, where duplicate data are removed to\\nreduce redundancy and improve the diversity of the training data. Moreover, Hernandez et al.\\n[171] found that duplication may cause instability in the training process, leading to overfitting\\nand poor generalization performance. Therefore, deduplication is essential to ensure the model\\nis exposed to diverse text data during training.\\nIt can be done at various granularities, such as at the document, paragraph, or sentence\\nlevel. Low-quality sentences containing repeated words or phrases can be removed to improve\\nthe data quality. At the document level, the deduplication can be done by computing the over-\\nlap ratio of surface features (e.g., words and n-grams overlap) between documents and removing\\nthe duplicates that contain similar contents [330, 131, 349, 181]. To avoid the contamination\\nproblem, the deduplication process should be done before the data is split into training, vali-\\ndation, and test sets [155]. Chowdhery et al. [155] and Carlini et al. [150] have shown that the\\nthree deduplication strategies should be used in conjunction to improve the training of LLMs.\\n3.3.3 Privacy reduction.\\nPrivacy reduction is another important step in data preprocessing, especially when dealing with\\nsensitive or personal information. Since data is often collected from the web and contains user-\\ngenerated content, the risk of privacy breaching is high [107]. This step involves anonymizing\\nor obfuscating sensitive data to protect individuals privacy. Common techniques for privacy\\nreduction include masking personally identifiable information (PII), such as names, addresses,\\nand phone numbers, and replacing them with generic placeholders or tokens [179].\\nPrivacy attacks on LLMs can be attributed to duplicated PII data in the pre-training, which\\ncan be used to extract the original PII data [181]. Therefore, de-duplication can also reduce\\nprivacy risks to some extent.\\n3.3.4 Tokenization.\\nTokenization is a crucial step in data preprocessing, where the text data is converted into tokens\\nthat can be processed by the model. The choice of tokenization method can significantly impact\\nthe models performance, as different tokenization strategies can affect the models ability to\\ncapture the underlying structure of the language.\\n45This means that the tokenization procedure has a direct impact on a models perplexity, which should\\nalways be taken into consideration when comparing different models.\\n51'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 51, 'page_label': '52'}, page_content='Common tokenization techniques include word-based tokenization, subword-based tokeniza-\\ntion, and character-based tokenization. Word-based tokenization splits the text into individual\\nwords, while subword-based tokenization breaks down the text into subword units, such as\\nprefixes, suffixes, and roots. Character-based tokenization, on the other hand, tokenizes the\\ntext into individual characters. Word-based tokenization is the predominant method used in\\ntraditional NLP research [5].\\nHowever, word-based tokenization can be problematic for languages with complex mor-\\nphology or limited vocabulary, as it may result in a large vocabulary size and sparse data\\nrepresentation. In some other languages, like Chinese, Japanese, and Korean, word-based tok-\\nenization is unsuitable because these languages do not have explicit word boundaries 46. Thus,\\nseveral neural network-based models employed subword-based tokenization, such as Byte Pair\\nEncoding (BPE) [35], Unigram [46], and WordPiece [36], to address these challenges.\\nByte Pair Encoding (BPE) is a type of data compression technique that has been effectively\\nadapted for natural language processing tasks, particularly in the domain of tokenization for\\nlarge language models (LLMs). The BPE algorithm operates by iteratively merging the most\\nfrequent pair of bytes (or characters in the context of text) in a given dataset into a single, new\\nbyte (or character). It repeats this process until a specified number of merges has been reached\\nor another stopping criterion has been met. The application of BPE in the field of NLP was\\npopularized by Sennrich, Haddow, and Birch [35] in the context of neural machine translation.\\nThey demonstrated that using BPE allowed for efficient handling of rare and unknown words,\\ncommonplace in languages with rich morphology or specialized vocabularies, such as scientific\\ntexts or code. By splitting words into subword units, BPE balances the granularity of characters\\nand the semantic units of full words, enabling models to represent a wide vocabulary with a\\nlimited set of tokens. BPE has been fundamental in the architecture of influential language\\nmodels, such as OpenAIs GPT series, BART and LLaMA.\\nWordPiece tokenization is a tokenization method that segments text into subword units,\\nbalancing the flexibility of character-based models and the efficiency of word-based models.\\nOriginating from speech processing [36], this method has found significant application in natural\\nlanguage processing, particularly within neural network-based models such as BERT and its\\nvariants. In WordPiece tokenization, a base vocabulary is first constructed with individual\\ncharacters, and then more frequent and meaningful sub-word units are incrementally added.\\nThis construction process is guided by a criterion that maximises the language model likelihood\\non a training corpus, thus ensuring that the resulting tokens are optimal representations of the\\ngiven data. The WordPiece algorithm iteratively merges the most frequently co-occurring\\npairs of tokens to form new sub-word units until a specified vocabulary size is reached. This\\ntokenization strategy has effectively reduced out-of-vocabulary issues, as the model can use\\nsmaller sub-word units when encountering unfamiliar words. Moreover, by capturing sub-\\nword regularities, WordPiece facilitates learning meaningful representations for morphologically\\nrich languages within large language models. This is particularly advantageous for handling\\nagglutinative languages, where words often comprise a series of affixed morphemes 47.\\n46It can yield different segmentation results for the same input.\\n47Agglutinative languages are a type of morphological linguistic classification in which words are formed\\nby adding discrete units, each carrying a specific grammatical meaning. These discrete units are known as\\nmorphemes, which are the smallest grammatical units in a language. In agglutinative languages, morphemes\\nare concatenated so that each morpheme represents a single grammatical function, such as tense, number, case,\\nor aspect. For example, in Turkish  an agglutinative language  a single word can be made up of a base or root\\nword with several affixes attached to it to modify its meaning. These affixes remain relatively invariant; they\\ndont undergo significant changes in form when theyre combined with other morphemes. Heres an illustrative\\nexample from Turkish:\\nev means house\\nevler means houses (plural)\\nevlerim means my houses (possessive plural)\\n52'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 52, 'page_label': '53'}, page_content='Unigram tokenization is a statistical method that employs a unigram language model to\\nsegment text into tokens probabilistically. This technique, standing in contrast to the deter-\\nministic nature of Byte Pair Encoding, involves constructing a unigram model from a large\\ninitial vocabulary and iteratively refining it to maximize the likelihood of the observed cor-\\npus [46]. The essence of Unigram tokenization lies in its iterative pruning process, wherein\\nless probable tokens are systematically eliminated from the vocabulary. The unigram language\\nmodel is estimated using an Expectation-Maximization (EM) algorithm: in each iteration, it\\nfirst identifies the optimal tokenization of words based on the current language model and then\\nupdates the model by re-estimating the unigram probabilities. Dynamic programming algo-\\nrithms, such as the Viterbi algorithm, are employed during this process to efficiently determine\\nthe optimal decomposition of a word based on the language model [364]. This probabilistic\\napproach is adept at handling the linguistic complexities and variations found across different\\nlanguages and domains. It particularly excels in the context of language models that require\\na nuanced understanding of morphological structures and sub-word variations. Unigram tok-\\nenization has been pivotal in developing the SentencePiece [46] tokenization library, renowned\\nfor its application in T5 and mBART. The adaptability and language-agnostic properties of\\nUnigram tokenization make it a preferred choice for LLMs tasked with processing multilingual\\ndata [46].\\n3.4 LLM Adaptation\\nThe adaptation of Large Language Models (LLMs) is a critical aspect of their deployment in\\nreal-world applications. It enables the models to be fine-tuned on specific tasks or domains\\nafter pre-training, enhancing their performance by minimizing the loss of generalization ca-\\npabilities. Adaptation can be achieved through various techniques, such as instruction tuning\\nand alignment tuning, which allow LLMs to enhance (or unlock) their abilities48 and align their\\nbehaviours with human values or preferences [364].\\n3.4.1 Instruction Tuning\\nInstruction tuning is a technique that leverages natural language instructions to fine-tune pre-\\ntrained LLMs [231], which is highly related to supervised fine-tuning [205] and multi-task\\nprompted training [209]. Instruction tuning enhances LLMs ability to follow and comprehend\\nnatural language instructions. Unlike traditional fine-tuning, which adapts models to specific\\ntasks, instruction tuning employs a more generalized approach that broadens the models utility\\nacross a variety of tasks through an instruction-following paradigm (Figure 18).\\nFLAN49 [231] is noted for substantially improving zero-shot learning capabilities when com-\\npared to traditional models like GPT-3 [88] (Figure 19).\\nChung et al. [156] have shown instruction-tuned (Figure 20) PaLM 50 enhance model per-\\nformance on various tasks (i.e., MMLU, BBH, TyDiQA and MGSM) when the model size is at\\nEach suffix attached to ev is a separate morpheme that changes the words meaning, indicating plurality\\nand possession without ambiguity. This is in contrast to fusional languages, where a single affix can represent\\nmultiple grammatical categories, or isolating languages, where words generally do not change form at all, and\\ngrammatical relations are indicated by word order or separate words.\\n48Some experiments on fine tuning general LLMs on specific tasks (e.g., planning) havent shown significant\\nimprovements, leading to the conclusion that fine tuning is just a way convert the task into a memory-based\\napproximate retrieval problem rather than a way to improve or unlock the models reasoning capabilities.\\nSometimes, in common sense domains, or with enough fine tuning, the reasoning part may also be obviated by\\nhaving seen a case that pretty much corresponds to the problem that needs to be solved [378].\\n49Finetuned LAnguage Net, a 137B parameter pre-trained language model and instruction tune it on over 60\\nNLP datasets verbalized via natural language instruction templates\\n50Also called Flan-PaLM 540B-parameter model\\n53'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 53, 'page_label': '54'}, page_content='Table 20: A detailed list of available collections for instruction tuning.\\nCategories Collections Time #Examples\\nTask Nat. Inst. [199] Apr-2021 193K\\nFLAN [231] Sep-2021 4.4M\\nP3 [147] Oct-2021 12.1M\\nSuper Nat. Inst.\\n[229]\\nApr-2022 5M\\nMVPCorpus [219] Jun-2022 41M\\nxP3 [200] Nov-2022 81M\\nOIG [334] Mar-2023 43M\\nChat HH-RLHF [148] Apr-2022 160K\\nHC3 [272] Jan-2023 87K\\nShareGPT [65] Mar-2023 90K\\nDolly [94] Apr-2023 15K\\nOpenAssistant [286] Apr-2023 161K\\nSynthetic Self-Instruct [228] Dec-2022 82K\\nAlpaca [327] Mar-2023 52K\\nGuanaco [110] Mar-2023 535K\\nBaize [357] Apr-2023 158K\\nBELLE [281] Apr-2023 1.5M\\nFigure 18: Overview of instruction tuning. Source: Zhao et al. [364].\\nleast 62B, though a much smaller size might suffice for some specific tasks (e.g., MMLU).\\nInstruction tuning has been widely applied also in other models like Instruct-GPT [205]\\nand GPT-4 [316]. Other experiments in Wei et al. [231] have shown that instruction tuning of\\nLaMDA-PT started to significantly improve performance on zero-shot tasks when the model\\nsize is at least 68B.\\nLets look at the construction of instruction-formatted instances essential for instruction\\ntuning. An instruction-formatted instance typically includes a task description (referred to\\nas the instruction), accompanied by a set of input-output examples and, optionally, a few\\ndemonstrations. There are three main approaches to constructing instruction-formatted in-\\nstances: formatting task datasets, formatting daily dialogues, and formatting synthetic data as\\nrepresented in Figure 21.\\nHistorically, datasets encompassing tasks like text summarization, classification, and trans-\\nlation were used to create multi-task training datasets [219, 72, 103]. These datasets have\\n54'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 54, 'page_label': '55'}, page_content='Figure 19: Top: overview of instruction tuning and FLAN. Instruction tuning finetunes a pre-\\ntrained language model on a mixture of tasks phrased as instructions. Evaluation of unseen task type\\nat inference time (i.e., evaluate the model on natural language inference (NLI) when no NLI tasks\\nwere seen during instruction tuning).\\nBottom: performance of zero-shot FLAN, compared with zero-shot and few-shot GPT-3, on three\\nunseen task types where instruction tuning improved performance substantially out of ten evaluated.\\nNLI datasets: ANLI R1R3, CB, RTE. Reading comprehension datasets: BoolQ, MultiRC, OBQA.\\nClosed-book QA datasets: ARC-easy, ARC-challenge, NQ, TriviaQA. Source: Wei et al. [231].\\nbecome crucial for instruction tuning, particularly when formatted with natural language de-\\nscriptions that clarify the task objectives of the LLMs. This augmentation helps the models\\nunderstand and execute the tasks more effectively [209, 205, 231, 229]. For instance, each exam-\\nple in a question-answering dataset might be supplemented with a directive like Please answer\\nthis question which guides the LLM in its response generation. The effectiveness of such in-\\nstruction tuning is evident as LLMs demonstrate improved generalization to unfamiliar tasks\\nwhen trained with these enriched datasets [231]. The decline in performance observed when\\ntask descriptions are omitted from training underscores the importance of these instructions.\\nPromptSource [147], a crowd-sourcing platform, has been proposed to aid in the creation,\\nsharing, and verification of task descriptions for datasets. This platform enhances the utility\\nof instruction tuning by ensuring a wide variety of well-defined task descriptions. Several\\nstudies [209, 219, 300] also tried to invert the input-output pairs of existing instances to create\\nnew instances using specially designed task descriptions (e.g., Please generate a question given\\nthis answer).\\nTalking about formatting daily chat data, Instruct-GPT has been fine-tuned using real user\\nqueries submitted to the OpenAI API to fill the significant gap in the data used for training\\n55'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 55, 'page_label': '56'}, page_content='Figure 20: Overview of FLAN instruction tuning with and without exemplars (i.e., zero-shot and\\nfew-shots) and with and without CoT. Following evaluation on unseen tasks. Source: Chung et al.\\n[156].\\nFigure 21: Three main approaches to construct instruction-formatted instances. Source: Zhao et al.\\n[364].\\nmodels  most training instances come from public NLP datasets that often lack instructional\\ndiversity and do not align well with actual human needs. This approach helps to harness\\nthe models capability to follow instructions effectively. To further enhance task diversity\\nand real-life applicability, human labellers are employed to create instructions for various tasks,\\nincluding open-ended generation, open-question answering, brainstorming, and casual chatting.\\nAnother set of labellers then provides responses to these instructions, which are used as training\\noutputs. This method enriches the training data and aligns the models responses more closely\\nwith human-like conversational patterns. InstructGPT also employs these real-world tasks\\nformatted in natural language for alignment tuning (see Section 3.4.2). GPT-4 extends this\\n56'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 56, 'page_label': '57'}, page_content='approach by designing potentially high-risk instructions and guiding the model to reject these\\ninstructions through supervised fine-tuning for safety concerns. Recent efforts have also focused\\non collecting user chat requests as input data, with models like ChatGPT or GPT-4 generating\\nthe responses. A notable dataset in this realm is the conversational data from ShareGPT, which\\nprovides a rich source of real-world interactions for training and refining the performance of\\nLLMs.\\nSemi-automated methods [228] for generating synthetic data have also been explored to\\ncreate instruction-formatted instances, which helps alleviate the need for extensive human\\nannotation and manual data collection. One such method is the Self-Instruct approach, which\\nefficiently utilizes a relatively small initial dataset. With the Self-Instruct method, only about\\n100 examples are required to start the data augmentation process (Figure 21c). From this\\ninitial task pool, a few instances are selected randomly and used as demonstrations for an\\nLLM. The model is then prompted to generate new task descriptions and corresponding input-\\noutput pairs. This process expands the dataset and ensures a variety of training examples\\nby incorporating a diversity and quality check before adding the newly synthesized instances\\nback into the task pool. This synthetic approach to data generation is portrayed as both\\ncost-effective and efficient, providing a scalable solution for enriching LLM training datasets. It\\nleverages LLMs generative capabilities to create diverse and relevant training materials, thereby\\nenhancing the training process without the usual resource-intensive demands of manual data\\ncreation. Instruction tuning improves zero-shot learning and establishes new benchmarks in\\nfew-shot learning scenarios. The improvement is attributed to the instruction tuning across\\ndiverse datasets, which likely provides a richer context for model adaptation [231]. By using\\nsupervision to teach a model to perform tasks described via instructions, the model will learn\\nto follow instructions and do so even for unseen tasks.\\nTwo essential factors for the instance construction are:\\n Scaling the instructions. Increasing the number of tasks within training data can\\nsignificantly improve the generalization ability of LLMs, as evidenced by Wei et al. [231],\\nSanh et al. [77], and Chowdhery et al. [155]. The performance of LLMs typically increases\\nwith the number of tasks but plateaus after reaching a saturation point [99, 155]. It is\\nsuggested that beyond a certain threshold, additional tasks do not contribute to per-\\nformance gains [99]. The diversity in task descriptions, including length, structure, and\\ncreativity variations, is beneficial [231]. However, increasing the number of instances per\\ntask might lead to overfitting if the numbers are excessively high [155, 258].\\n Formatting design. The way instructions are formatted also plays a crucial role in the\\ngeneralization performance of LLMs [155]. Task descriptions, supplemented by optional\\ndemonstrations, form the core through which LLMs grasp the tasks [155]. Utilizing a\\nsuitable number of exemplars as demonstrations can notably enhance performance and\\nreduce the models sensitivity to instruction nuances [77, 99]. However, including ad-\\nditional elements like prohibitions, reasons, or suggestions within instructions may not\\neffectively impact or even negatively affect LLM performance [155, 199]. Recently, some\\nstudies suggest incorporating chain-of-thought (CoT) examples in datasets that require\\nstep-by-step reasoning, which has proven effective across various reasoning tasks [99, 174].\\nInstruction tuning is often more efficient since only a few instances are needed for training.\\nBeing considered as a supervised training process, differs from pre-training in several key as-\\npects [156], including the training objective (e.g., sequence-to-sequence loss) and optimization\\nconfiguration (e.g., smaller batch sizes and learning rates), necessitating careful consideration\\nin practice.\\nBalancing the proportion of different tasks during fine-tuning is crucial. A commonly used\\nmethod is the examples-proportional mixing strategy [99], ensuring that no single dataset over-\\n57'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 57, 'page_label': '58'}, page_content='whelms the training process [99, 231]. Additionally, setting a maximum cap on the number of\\nexamples from any dataset helps maintain this balance [99, 231].\\nTo enhance the stability and effectiveness of instruction tuning, integrating pre-training\\ndata into the tuning process is beneficial, serving as regularization [231]. Some models, such\\nas GLM-130B and Galactica, start with a mix of pre-training and instruction-tuned data,\\neffectively combining the strengths of both pre-training and instruction tuning [155].\\nA strategic approach involves multiple tuning stages, starting with extensive task-specific\\ndata and followed by less frequent types, such as daily chat instructions, to avoid forgetting\\npreviously learned tasks [99].\\nSome additional strategies to improve the instruction tuning process include:\\n Efficient training for multi-turn chat data. In a multiturn chat 51 dataset, each\\nconversation can be divided into multiple context-response pairs for training, where the\\nmodel is fine-tuned to generate appropriate responses for each corresponding context. To\\nsave computational resources, Chiang et al. [263] propose a method that fine-tunes the\\nmodel on the whole conversation but relies on a loss mask that only computes the loss\\non the chatbots responses for training.\\n Filtering low-quality instructions using LLMs. Filtering out low-quality instruc-\\ntions through advanced LLMs helps maintain high training standards and reduces unnec-\\nessary computational expenses [231].\\n Establishing self-identification for LLM. In real-world applications, it is important\\nfor LLMs to be able to identify themselves when asked. To achieve this, models like\\nGPT-4 are trained to recognize and respond to self-identification instructions [316].\\n Concatenate multiple examples to approach max length. To handle variable-\\nlength sequences during training, it is common practice to introduce padding tokens to\\nensure uniform sequence lengths. However, this approach can lead to inefficient use of\\nthe models capacity, as the padding tokens do not contribute to the learning process. By\\nconcatenating multiple examples to approach the maximum sequence length, the model\\ncan process more information in each training step, enhancing the training efficiency and\\nperformance [116].\\n Evaluate the quality of instructions. Cao et al. [256] introduced InstructMining\\nto autonomously select premium instruction-following data for finetuning LLMs by em-\\nploying a combination of data mining techniques and performance evaluation strategies.\\nThe quality of instruction data is primarily assessed through its impact on model per-\\nformance, quantified by the inference loss of a finetuned model on an evaluation dataset.\\nInstructMining correlates the values of the natural language indicators 52 A predictive\\nmodel that estimates data quality based on these indicators is created with the infer-\\nence loss. To identify the most effective subset of data for finetuning, InstructMining\\n51A conversation between a user and a bot\\n52Some of the indicators include:\\n Input and output length\\n Reward model scores\\n Perplexity\\n Measures of Textual Lexical Diversity (MTLD)\\n Approximate distance to nearest neighbours in embedding space\\n Scores for naturalness, coherence, and understandability from models like UniEval\\n58'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 58, 'page_label': '59'}, page_content='integrates an optimization technique called BlendSearch. This method helps deter-\\nmine the optimal size and composition of the data subset, leading to the best finetuning\\noutcomes. BlendSearch combines global and local search strategies to efficiently ex-\\nplore the complex search space, focusing on minimizing the models inference loss on a\\nhigh-quality evaluation set. Cao et al. [256] also accounts for the double descent phe-\\nnomenon observed in model training, where increasing the dataset size initially improves\\nperformance up to a point, after which performance declines before potentially improving\\nagain as more data is added. This observation guides the selection process to focus on\\nan optimal point that balances data quality and quantity, improving model performance\\nefficiently.\\n Rewriting instructions into more complex ones. Xu et al. [355] introduces a\\nmethod termed Evol-Instruct, which significantly enhances the instruction-following\\ncapabilities and overall performance of large language models (LLMs). It is a systematic\\napproach for automatically generating complex instruction data using LLMs instead of\\nhuman input. This method involves iterative evolution and refinement of initial, simple\\ninstructions into more complex and diverse variants. These evolved instructions are then\\nused to fine-tune LLMs, specifically targeting their ability to effectively understand and\\nexecute more complex tasks. Starting with a basic set of instructions, Evol-Instruct\\nemploys a two-pronged strategy  In-Depth Evolving and In-Breadth Evolving.\\n/textbfIn-Depth Evolving enhances the complexity and depth of instructions by adding\\nconstraints, increasing reasoning demands, or introducing more detailed contexts. /textbfIn-\\nBreadth Evolving expands the variety and coverage of topics and skills addressed by the\\ninstructions, aiming to fill gaps in the LLMs training data and increase its general ro-\\nbustness across different types of tasks.\\nThroughout the evolution process, ineffective or poorly structured instructions are filtered\\nout to ensure only high-quality data is used for model training. This step is crucial for\\nmaintaining the integrity and effectiveness of the training dataset. The process repeats\\nseveral cycles, allowing the system to gradually refine the instruction set to maximize com-\\nplexity and utility while ensuring the instructions remain understandable and executable\\nby the LLM. By training with the complex instructions generated by Evol-Instruct, LLMs\\nlike the WizardLM demonstrate significant improvements in several key areas:\\n Enhanced Generalization: The model can handle a wider variety of tasks beyond\\nthe scope of its original training data.\\n Improved Complexity Handling: The LLM performs better in understanding and\\nexecuting tasks requiring higher levels of reasoning or multiple steps to complete.\\n Competitive Performance: Compared to models like OpenAIs ChatGPT and other\\ncontemporary LLMs, WizardLM trained with Evol-Instruct data exhibits competi-\\ntive or superior performance, especially on complex instruction-following tasks.\\nThe main effects of instruction tuning are:\\n Performance Improvement. Instruction tuning significantly enhances LLMs, proving\\neffective across models of various scales from 77M to 540B parameters. Smaller models\\nsubjected to instruction tuning can surpass larger models that havent been fine-tuned,\\nshowcasing the techniques broad applicability and cost-effectiveness [135, 231]. This\\napproach boosts model performance as parameter scale increases and demonstrates im-\\nprovements across different architectures, objectives, and adaptation methods [99].\\n59'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 59, 'page_label': '60'}, page_content=' Task Generalization. Instruction tuning endows LLMs with the capability to under-\\nstand and execute tasks based on natural language instructions. This method is particu-\\nlarly effective in generalizing across both familiar and novel tasks, significantly enhancing\\nperformance without direct prior exposure [155, 135]. Notably, models like BLOOMZ-P3,\\nfine-tuned on English-only tasks, demonstrate remarkable improvements in multilingual\\nsentence completion, indicating robust cross-lingual transfer capabilities [155].\\n Domain Specialization. Despite their prowess in general NLP tasks, LLMs often lack\\nthe domain-specific knowledge required for fields like medicine, law, and finance. Instruc-\\ntion tuning facilitates the transformation of general-purpose LLMs into domain-specific\\nexperts. For example, Flan-PaLM has been adapted into Med-PaLM for medical applica-\\ntions, achieving expert-level performance in medical tasks [99]. Similar adaptations have\\nbeen made in other domains, significantly enhancing LLMs effectiveness in specialized\\napplications [231].\\nIn summary, instruction tuning is a powerful technique that significantly enhances LLMs\\nperformance, generalization, and domain specialization. Instruction tunings effectiveness is\\nevident across models of various scales and architectures, demonstrating its versatility and\\nbroad applicability. Larger models, such as Llama 13B compared to Llama7B, generally perform\\nbetter, suggesting that increased model size enhances the models ability to follow instructions\\nand utilize knowledge more effectively. This is particularly evident in QA settings, where larger\\nmodels show markedly improved performance [364].\\nIncreasing the complexity and diversity of the Self-Instruct-52K dataset enhances Llamas\\nperformance in both chat and QA settings. For example, improving instruction complexity\\nsignificantly boosts performance on QA tasks, which typically involve complex queries. Merely\\nincreasing the number of instructions or attempting to balance instruction difficulty does not\\nnecessarily yield better outcomes. In some cases, such as scaling up instruction numbers without\\nfocusing on quality, it can even degrade performance [364].\\n3.4.2 Alignment Tuning\\nLLMs may sometimes generate outputs inconsistent with human values or preferences (e.g.,\\nfabricating false information, pursuing inaccurate objectives, and producing harmful, mislead-\\ning, or biased content) [205, 115]. To avoid such undesirable outcomes, alignment tuning\\nensures that LLMs outputs align with specified ethical guidelines or desired behaviors [364].\\nUnlike pre-training and fine-tuning, which focus on optimizing model performance, alignment\\ntuning aims to optimize the models behaviour to conform to human values and norms [364].\\nAlignment may harm the general abilities of LLMs to some extent, which is called alignment\\ntax [104].\\nThree primary criteria for regulating the behaviour of large language models (LLMs) are\\nhelpfulness, honesty, and harmlessness. These criteria have become standard in the literature\\nand are benchmarks for aligning LLMs with desirable human-like behaviours. Its possible\\nto adapt these criteria based on specific needs, such as substituting honesty with correct-\\nness [165]. Helpfulness refers to the models ability to assist users effectively and efficiently,\\nanswering queries or solving tasks concisely. It should also engage in deeper interaction when\\nnecessary, asking relevant questions and demonstrating sensitivity and awareness. Honesty\\ninvolves providing accurate information and transparency about the models uncertainty and\\nlimitations. This criterion is seen as more objective, potentially requiring less human interven-\\ntion to achieve alignment than the other criteria. Harmlessness involves avoiding generating\\noffensive or discriminatory language and being vigilant against being manipulated into harmful\\nactions. Determining what constitutes harm can vary significantly depending on cultural and\\nindividual differences and the context in which the model is used.\\n60'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 60, 'page_label': '61'}, page_content='Zhao et al. [364] notes the subjectivity of these criteria, rooted in human judgment, making\\nthem challenging to incorporate directly as optimization objectives in LLM training. Nonethe-\\nless, various strategies, such as red teaming 53, are employed to meet these criteria by inten-\\ntionally challenging LLMs to provoke harmful outputs and then refining them to prevent such\\nbehaviours.\\nDuring the pre-training phase on a large-scale corpus, the subjective and qualitative evalua-\\ntions of LLM outputs by humans cannot be taken into account. Human feedback is essential for\\nalignment tuning, as it provides the necessary supervision to guide the model towards desirable\\nbehaviours.\\nDominant strategies for generating human feedback data is human annotation [205, 165,\\n85]. This highlights the importance of labellers in the alignment tuning process, as they play\\na crucial role in providing feedback on the models outputs. Ensuring that labellers have\\nadequate qualifications is vital; despite stringent selection criteria, mismatches in intentions\\nbetween researchers and labellers can still occur, potentially compromising feedback quality\\nand LLM performance [106]. To address this, the InstructGPT initiative includes a screening\\nprocess to select labellers whose evaluations closely align with those of researchers [205]. In\\nsome studies, using super raters ensures the highest quality of feedback by selecting the most\\nconsistent labellers for critical tasks [165].\\nThree primary methods are used to collect human feedback and preference data:\\n Ranking-based approach. Human labellers evaluate model outputs in a coarse-\\ngrained fashion, often choosing only the best output without considering finer details.\\nThis method could lead to biased or incomplete feedback due to the diversity of opinions\\namong labellers and the neglect of unselected samples. To improve this, later studies\\nintroduced the Elo rating system to establish a preference ranking by comparing outputs,\\nthereby providing a more nuanced training signal [165, 85].\\n Question-based approach. This method involves labellers providing detailed feed-\\nback by answering specific questions designed to assess alignment criteria and additional\\nconstraints. For example, in the WebGPT project, labellers evaluate the usefulness of\\nretrieved documents to answer given inputs, helping to filter and utilize relevant informa-\\ntion [124].\\n Rule-based approach. This approach involves the use of predefined rules to generate\\ndetailed feedback. For instance, Sparrow uses rules to test whether responses are helpful,\\ncorrect, and harmless. Feedback is generated both by comparing responses and assessing\\nrule violations. Additionally, GPT-4 uses zero-shot classifiers to automatically determine\\nif outputs violate set rules [165, 316].\\nOne approach to alignment tuning is to use a reward model to evaluate the quality of\\ngenerated outputs. RLHF utilizes reinforcement learning (RL) techniques, such as Proximal\\nPolicy Optimization (PPO), to fine-tune LLMs based on human feedback, aiming to enhance\\nmodel alignment on criteria like helpfulness, honesty, and harmlessness. This process involves\\nseveral components and steps to effectively train and optimize LLMs. Key components of RLHF\\ninclude a pre-trained language model (LM), a reward model (RM), and an RL algorithm (e.g.,\\nPPO) [364]. The LM is initialized with parameters from existing LLMs, such as OpenAIs\\nGPT -3 or DeepMinds Gopher. The reward model provides guidance signals reflecting human\\npreferences. It could be a fine-tuned LM or a newly trained LM using human preference data.\\nRMs often differ in parameter scale from the LLM being aligned. The main steps in RLHF\\ninclude supervised fine-tuning, reward model training, and RL fine-tuning [364].\\n53Red teaming might involve trying to induce biased or harmful outputs from the model, to test its resistance\\nto producing undesirable content under adversarial conditions.\\n61'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 61, 'page_label': '62'}, page_content='Supervised fine-tuning involves collecting a supervised dataset with prompts and desired\\noutputs for initial fine-tuning.\\nReward model training trains the RM using human-annotated data where labellers rank\\noutputs, guiding the RM to predict human preferences. Studies suggest using large reward\\nmodels that align with the LLMs scale for better performance judgment and combining multiple\\nRMs focused on different alignment criteria for a nuanced reward signal.\\nRL fine-tuning treats alignment as an RL problem where the LM is optimized against the\\nRM using PPO, incorporating penalties like KL divergence to maintain closeness to the original\\nmodel behaviour. Practical strategies propose deploying the RM on a separate server and using\\nbeam search decoding to manage computational demands and enhance output diversity.\\nRLHF is a complex but promising approach to improving LLM alignment with human\\nvalues. It involves sophisticated training regimes and multiple feedback mechanisms to ensure\\nthe models outputs are ethical and practical.\\nThat being said, RLHF is memory-intensive (it needs to train multiple LMs), and the\\nPPO algorithm is somewhat complex and often sensitive to hyperparameters. Thus, increasing\\nstudies are exploring alternative methods to align LLMs with human values using supervised\\nfine-tuning without reinforcement learning.\\nThe main idea behind alignment tuning without reinforcement learning is to use high-quality\\nalignment datasets directly. LLMs aligned with human-written safety principles or refining\\nexisting examples through editing operations may create the alignment dataset. Additionally,\\nreward models can be reused to select highly rated responses from existing human feedback data,\\nenriching the datasets quality and relevance. Non-RL alignment methods employ supervised\\nlearning strategies similar to those used in original instruction tuning. These methods may also\\nintegrate auxiliary learning objectives, such as ranking responses or contrasting instruction-\\nresponse pairs, to further enhance LLMs alignment accuracy and performance.\\n3.5 Architecture\\nThe architecture of Large Language Models (LLMs) plays a pivotal role in determining their\\nperformance, efficiency, and scalability.\\nGenerally speaking, we can identify some key components that define different LLM archi-\\ntectures: the encoder and the decoder. The encoder is an essential component in LLMs. It\\nprocesses input sequences and maps them to a higher-dimensional space, capturing the contex-\\ntual information in the data. The structure of an encoder in LLMs typically involves a stack of\\nidentical layers, each comprising two main sub-layers: a multi-head self-attention 54 mechanism\\nand a position-wise fully connected feed-forward network [334].\\nOn the other hand, the decoder is responsible for generating output sequences based on the\\nencoded representations. The decoder in models such as GPT-3 [88] and its successors operate\\non the principle of autoregressive modelling, where each subsequent token is predicted based\\non the previously generated tokens. A key feature of decoders in LLMs is causality, which\\nensures that the prediction for the current token can only attend to previous tokens, not future\\nones. This is implemented through masked attention mechanisms in the transformers decoder\\nlayers [334].\\nFor example, in a translation task, the encoder processes the source sentence and produces\\na set of vectors representing its content. At the same time, the decoder uses cross-attention\\nto decide which words (or phrases) in the source sentence are most relevant for predicting the\\nnext word in the target language. In code generation, decoders can create syntactically correct\\ncode snippets given comments or docstrings as input, as demonstrated by Codex [108].\\n54See Section 3.5.4 for more details on self-attention mechanisms.\\n62'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 62, 'page_label': '63'}, page_content='Based on the components and the way they are connected, LLMs can be categorized into\\nthree main types: encoder-only 55, decoder-only and encoder-decoder models. All of these are\\nsequence-to-sequence models (often referred to as seq2seq models).\\nFigure 22: Some of the mainstream LLMs models by type.\\nMainstream architectures can be further categorized into three major types: encoder-\\ndecoder, casual decoder and prefix decoder, as shown in Figure 23. Both casual decoder and\\nprefix decoder are decoder-only architectures, but they differ in how they generate tokens.\\n3.5.1 Encoder-decoder\\nThe vanilla version of the Transformer architecture introduced by Vaswani et al. [334] belongs\\nto this category, which consists of an encoder and a decoder.\\nThe encoder transforms an input sequence into a set of representations that capture its\\nsemantic and syntactic properties.\\nOn the other hand, the decoder is tasked with generating an output sequence from the\\nencoded representations. It predicts each token by conditioning on the previously generated\\ntokens and the encoded input, a process that has significantly improved with the integration\\nof cross-attention modules. The encoder-decoder architecture enables a flexible approach to\\ndiverse language tasks by segregating the understanding (encoding) and generation (decoding)\\nprocesses.\\n55We refer to BERT-style methods as encoder-only; the description encoder-only may be misleading since\\nthese methods also decode the embeddings into output tokens or text during pretraining. In other words,\\nboth encoder-only and decoder-only architectures are decoding. However, the encoder-only architectures,\\nin contrast to decoder-only and encoder-decoder architectures, are not decoding in an autoregressive fashion.\\nAutoregressive decoding refers to generating output sequences one token at a time, conditioning each token\\non the previously generated tokens. Encoder-only models do not generate coherent output sequences in this\\nmanner. Instead, they focus on understanding the input text and producing task-specific outputs, such as labels\\nor token predictions [317].\\n63'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 63, 'page_label': '64'}, page_content='So far, there are only a small number of models that use the encoder-decoder architecture\\n(Figure 22), such as BART [94] and T5 [99].\\nFigure 23: A comparison of the attention patterns in three mainstream architectures. Here, the\\nblue, green, yellow and grey rounded rectangles indicate the attention between prefix tokens, attention\\nbetween prefix and target tokens, attention between target tokens, and masked attention, respectively.\\nSource: Zhao et al. [364].\\n3.5.2 Casual decoder\\nA causal decoder predicts each token based on the preceding tokens. This ensures that the\\ngeneration process is unidirectional and prevents the model from using future tokens in the\\nprediction process [334]. This mechanism is akin to how humans produce language, one word\\nat a time, building upon what has already been said without access to future words.\\nThe architecture typically employs self-attention mechanisms where the attention distribu-\\ntion is masked to prevent tokens from attending to subsequent positions in the sequence (i.e.,\\nunidirectional attention mask). This masking is instrumental in preserving the autoregressive\\nproperty within the transformer-based models [75].\\nThe GPT series56 of language models by OpenAI are prominent examples that utilize causal\\ndecoder architectures, where the ability to generate coherent and contextually relevant text has\\nbeen demonstrated effectively [88].\\nThe causal decoder architecture is well-suited for tasks requiring sequential generation, such\\nas text completion, language modelling, and text generation. It has been widely adopted as the\\narchitecture of choice for many large-scale language models, such as OPT [241], BLOOM [349],\\nand Gopher [131].\\n3.5.3 Prefix decoder\\nThe prefix decoder architecture 57 enables partial conditioning of generated sequences, revising\\nthe masking mechanism of causal decoders, to allow performing bidirectional attention over the\\nprefix tokens [66] and unidirectional attention only on generated tokens.\\nIn other words, this architecture allows the model to generate tokens based on both the input\\nprefix and the target prefix, which can be helpful in tasks that require generating sequences with\\nspecific prefixes or constraints. In practice, a prefix decoder is implemented by feeding a fixed\\n56GPT-3 [88] showed amazing in-context learning capability, whereas GPT-1 [51] and GPT-2 [75] didnt. It\\nseems that scaling plays an important role in increasing the model capacity of this model architecture\\n57Also called non-causal decoder [240]\\n64'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 64, 'page_label': '65'}, page_content='sequence of tokens 58 into the decoder alongside the tokens generated so far. The model then\\nextends the prefix by generating subsequent tokens that logically follow the context provided\\nby the prefix.\\nUnlike the causal decoder, which strictly adheres to a unidirectional generation pattern, the\\nprefix decoder allows for a predefined context or prefix to guide the generative process [119].\\nThis is particularly useful in tasks such as machine translation, where the prefix can be a\\npart of the already known or hypothesized translation. Still, the flexibility provided by the\\nprefix decoder makes it suitable for a range of applications, from controlled text generation to\\ntask-oriented dialogue systems, where maintaining context and coherence is crucial [183].\\nThis architecture has been utilized in various language models to improve text generation\\ncontrol and enhance the models ability to handle specific formats or styles [99].\\n3.5.4 Transformer Architecture\\nThe Transformer architecture has emerged as the de facto standard for LLMs, owing to its abil-\\nity to capture long-range dependencies and model complex language structures effectively [334],\\nmaking it possible to train models with billions or even trillions of parameters [88, 330].\\nThis architecture usually consists of stacked Transformer layers (Figure 24), each com-\\nprising a multi-head self-attention sub-layer and a position-wise fully connected feed-forward\\nnetwork [334]. Residual connection [29] and layer normalization [28] are applied for both sub-\\nlayers individually.\\nFigure 24: The full model architecture of the transformer. Source: Weng [57].\\nThe position-wise FFN sub-layer is a two-layer feed-forward network with a ReLU activation\\nfunction between the layers. Given a sequence of vectors h1, h2, . . . , hn, the computation of a\\nposition-wise FFN sub-layer on any hi, as shown in Equation 7.\\nFFN(hi) = ReLU(hiW1 + b1)W2 + b2 (7)\\n58Also known as the prefix\\n65'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 65, 'page_label': '66'}, page_content='where W1, W2, b1, and b2 are learnable parameters of the FFN sub-layer.\\nBesides the two sub-layers described above, the residual connection and layer normalization\\nare also key components of the Transformer. Different orders and configurations of the sub-\\nlayers, residual connection and layer normalization in a Transformer layer lead to variants of\\nTransformer architectures as shown in Table 21.\\nModel Category Size Normalization PE Activation Bias #L #H dmodel MCL\\nGPT3 [88] Causal\\ndecoder\\n175B Pre LayerNorm Learned GeLU Y 96 96 12288 2048\\nPanGU- [139] Causal\\ndecoder\\n207B Pre LayerNorm Learned GeLU Y 64 128 16384 1024\\nOPT [241] Causal\\ndecoder\\n175B Pre LayerNorm Learned ReLU Y 96 96 12288 2048\\nPaLM [155] Causal\\ndecoder\\n540B Pre LayerNorm RoPE SwiGLU N 118 48 18432 2048\\nBLOOM [349] Causal\\ndecoder\\n176B Pre LayerNorm ALiBi GeLU Y 70 112 14336 2048\\nMT-NLG [214] Causal\\ndecoder\\n530B - - - - 105 128 20480 2048\\nGopher [131] Causal\\ndecoder\\n280B Pre RMSNorm Relative - - 80 128 16384 2048\\nChinchilla [172] Causal\\ndecoder\\n70B Pre RMSNorm Relative - - 80 64 8192 -\\nGalactica [220] Causal\\ndecoder\\n120B Pre LayerNorm Learned GeLU N 96 80 10240 2048\\nLaMDA [221] Causal\\ndecoder\\n137B - Relative GeGLU - 64 128 8192 -\\nJurassic-1 [121] Causal\\ndecoder\\n178B Pre LayerNorm Learned GeLU Y 76 96 13824 2048\\nLlama [330] Causal\\ndecoder\\n65B Pre RMSNorm RoPE SwiGLU Y 80 64 8192 2048\\nLlama 2 [329] Causal\\ndecoder\\n70B Pre RMSNorm RoPE SwiGLU Y 80 64 8192 4096\\nFalcon [312] Causal\\ndecoder\\n40B Pre LayerNorm RoPE GeLU N 60 64 8192 2048\\nGLM-130B [239] Prefix\\ndecoder\\n130B Post DeepNorm RoPE GeGLU Y 64 96 12288 2048\\nT5 [99] Encoder-\\ndecoder\\n11B Pre RMSNorm Relative ReLU N 24 128 1024 512\\nTable 21:Model cards of several selected LLMs with public configuration details. PE denotes position\\nembedding, #L denotes the number of layers, #H denotes the number of attention heads, dmodel denotes\\nthe size of hidden states, and MCL denotes the maximum context length during training. Source: Zhao\\net al. [364].\\nConfigurations Since the introduction of the Transformer architecture, several variants and\\nconfigurations have been proposed to improve the performance and efficiency of LLMs. The\\nconfiguration of the four major parts of the Transformer architecture includes normalization,\\nposition embeddings, activation functions, and attention and bias, as shown in Table 22.\\nNormalization Methods Normalization methods are crucial for stabilizing the training\\nprocess and improving the convergence of LLMs. In the vanilla Transformer [334] architecture,\\nLayerNorm [28] is the most commonly used normalization method, which normalizes the hidden\\nstates across the feature dimension. Before LayerNorm was introduced, BatchNorm [26] was\\nwidely used in convolutional neural networks. Still, it was found to be less effective in sequence\\nmodels due to the varying batch sizes and sequence lengths. LayerNorm addresses this issue\\nby normalizing the hidden states across the feature dimension, making it more suitable for\\nsequence models. Specifically, LayerNorm normalizes the hidden states using the mean and the\\nvariance of the summed inputs within each layer.\\nRMSNorm [83] is another normalization method that has been proposed to improve the\\ntraining speed of LayerNorm. RMSNorm normalizes the hidden states by dividing them by the\\n66'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 66, 'page_label': '67'}, page_content='root mean square of the squared hidden states, which has been shown to improve the training\\nspeed and performance [125]. ChinchiLLa [172] and Gopher [131] are examples of LLMs that\\nuse RMSNorm as the normalization method.\\nDeepNorm [225] is a novel normalization method that combines LayerNorm with a learnable\\nscaling factor to stabilize the training process of deep Transformer models. With DeepNorm,\\nTransformer models can be scaled up to hundreds of layers without additional normalization\\nlayers, making it an effective method for training large-scale LLMs [225]. It has been used in\\nmodels such as GLM-130B [239].\\nConfiguration Method\\nNormalization position Post Norm [334]\\nPre Norm [75]\\nSandwich Norm [109]\\nNormalization method LayerNorm [28]\\nRMSNorm [83]\\nDeepNorm [225]\\nActivation function ReLU [16]\\nGeLU [56]\\nSwish [41]\\nSwiGLU [100]\\nGeGLU [100]\\nPosition embedding Absolute [334]\\nRelative [99]\\nRoPE [134]\\nAlibi [206]\\nTable 22: Detailed formulations for the network configurations. Source: Zhao et al. [364]\\nNormalization Position The position of the normalization layer (Figure 25) in the Trans-\\nformer architecture can significantly impact the models performance and convergence. The\\nthree main configurations proposed in different studies are pre-LN59, post-LN60, and Sandwich-\\nLN.\\nIn the pre-LN configuration, the normalization layer is placed inside the residual blocks,\\nwhile in the post-LN configuration, it is placed after them. In Ding et al. [109], the normalization\\nlayer is placed before and after the residual blocks, referred to as the Sandwich-LN configuration.\\nPost-LN is used in the vanilla Transformer architecture [334], where the normalization layer\\nis placed between the residual blocks. This sequence allows the model to first process the\\ninput through a sublayer, such as a Multi-Head Attention (MHA) or Feed-Forward Network\\n(FFN), and then apply normalization to the output of the sublayer combined with the residual\\nconnection. In particular, to train the model from scratch, any gradient-based optimization\\napproach requires a learning rate warm-up stage to stabilize the training process [334]. Existing\\nworks found that training of Transformer models with post-norm tends to be unstable due to\\nlarge gradients near the output layer [101].\\nPre-LN [62] is another configuration where the normalization layer is placed inside the\\nresidual blocks. It makes it possible to remove the warm-up stage, requiring significantly less\\ntraining time and hyper-parameter tuning on a wide range of applications. The Transformers\\nwith pre-LN have shown to be more stable during training but have worse performance [97].\\n59Pre-Layer Normalization\\n60Post-Layer Normalization\\n67'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 67, 'page_label': '68'}, page_content='Figure 25:Illustration of different LayerNorm structures in Transformers. Source: Ding et al. [109].\\nSandwich-LN [109] is a configuration that combines the advantages of both pre-LN and\\npost-LN by placing the normalization layer both before and after the residual blocks. This\\nconfiguration has been shown to improve the performance of Transformer models by providing\\nbetter stability during training and faster convergence [109]. In Zeng et al. [239], the authors\\nfound that the Sandwich-LN configuration sometimes fails to stabilize the training of LLMs\\nand may lead to the collapse of training.\\nActivation Functions Activation functions play a crucial role in the training and perfor-\\nmance of LLMs by introducing non-linearity into the model 61. LLMs most commonly used\\nactivation functions are ReLU, GeLU, Swish, SwiGLU, and GeGLU.\\nReLU62 [16] is a simple and widely used activation function that introduces non-linearity\\nby setting negative values to zero.\\nReLU(x) = max(x, 0) (8)\\nOne of the first activation functions to be used in deep learning, ReLU has been shown to\\nbe effective in training deep neural networks by preventing the vanishing gradient problem [17].\\nThis non-linear activation function introduces sparsity in the networks activations, which can\\nlead to faster training and better performance due to its simplicity and efficiency. However,\\nReLU can suffer from the dying ReLU problem, where neurons can become inactive and stop\\nlearning if the input is negative [20].\\nGeLU [31] is a Gaussian Error Linear Unit activation function used to model uncertainties\\nin neural networks. It was introduced to improve upon ReLU by considering the stochastic\\nregularisation techniques. The smoothness of the GELU function can be advantageous in\\ndeep neural networks with many layers, as it can help prevent the problem of dying ReLU\\n61In the feed-forward layer\\n62Rectified Linear Unit\\n68'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 68, 'page_label': '69'}, page_content='and improve the flow of gradients through the network. The GELU activation function is\\nmathematically described as follows:\\nGeLU(x) = x  (x) (9)\\nwhere (x) is the cumulative distribution function of the standard Gaussian distribution. This\\ncan also be approximated as:\\nGeLU(x)  0.5x(1 + tanh[\\np\\n2/(x + 0.044715x3))] (10)\\nAlternatively, the GELU function can be expressed as a scaled version of the sigmoid func-\\ntion, as shown below:\\nGeLU(x)  x  (1.702x) (11)\\nThe GELU function allows the input to control its gate, deciding whether to pass through\\nor be dampened. When x is large, GELU approximates to x, acting like a linear unit. When x\\nis close to zero or negative, it squashes the output, making it closer to zero. In other words, the\\nGELU function would produce outputs smoothed around zero rather than sharply cut off as\\nwith ReLU. Many deep learning models use The GELU activation function, including GPT-3\\nand BERT.\\nThe Swish [41] activation function is a smooth, non-monotonic function developed to over-\\ncome some limitations of ReLU and was found to perform better in deeper models. It is defined\\nas\\nSwish = x   (x) (12)\\nwhere x is the input to the activation function and sigmoid is the logistic function(x) = 1\\n1+ex .\\nThe Swish function allows small and negative values to pass through, which can benefit gradient\\nflow in deep models. It has been empirically demonstrated to work well for deeper models and\\nis computationally efficient.\\nSwiGLU [100] is a variant of the Swish activation function that combines the Swish function\\nwith the Gated Linear Unit (GLU) function. The SwiGLU activation function is defined as\\nSwiGLU (x, W, V, b, c, ) = Swish (xW + b)  (xV + c) (13)\\nHere, x is the input to the neuron, W and V are weight matrices, b and c are bias vectors, and\\nis a constant. The  symbol denotes element-wise multiplication, while Swish is the activation\\nfunction described in Equation 12. This function allows the network to learn which input\\nparts should be retained (gated) for further layers, combining the advantages of non-saturating\\nfunctions and dynamic gating mechanisms.\\nGeGLU [100] is another variant of the GLU activation function that combines the GeLU\\nfunction with the Gated Linear Unit (GLU) function. The GeGLU activation is formulated as\\nfollows:\\nGeGLU (x, W, V, b, c) = GeLU (xW + b)  (xV + c) (14)\\nAfter the output of the GeLU function is calculated, it is multiplied element-wise with a second\\nmatrix. This second matrix is calculated by multiplying the input matrix x with another matrix\\nW and adding a bias term b. The output of this multiplication is then passed through a second\\nmatrix V and added to a scalar term c.\\n69'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 69, 'page_label': '70'}, page_content='Position Embeddings Position embeddings are a crucial component of the Transformer\\narchitecture. They allow the model to capture the sequential order of tokens in the input\\nsequence. Several types of position embeddings are used in LLMs, including absolute, relative,\\nRoPE, and Alibi embeddings.\\nAbsolute position embeddings [334] were proposed in the original Transformer model. The\\nabsolute positional embeddings are added to the input embeddings at the bottoms of the\\nencoder and the decoder. There are two variants of absolute position embeddings: sinusoidal\\nand learned position embeddings, the latter of which is commonly used in existing pre-trained\\nlanguage models.\\nThe formulation for adding absolute position embeddings is straightforward:\\nEtotal(i) = Etoken(i) + Eposition(i) (15)\\nwhere Etotal(i) is the final embedding vector for token i, Etoken(i) is the initial token embedding\\nfor token i, and Eposition(i) is the position embedding vector for token i. This technique allows\\nthe model to use the order of words to understand meaning and context, which is especially\\nimportant for tasks involving sequence modelling and generation.\\nRelative position embeddings [52] are an alternative to absolute position embeddings that\\ncapture the relative distance between tokens in the input sequence. This allows the model\\nto learn more flexible and adaptive representations of the input sequence, which can improve\\nperformance on tasks that require capturing long-range dependencies and complex relation-\\nships between tokens. Relative position embeddings are incorporated into the self-attention\\nmechanism of Transformer models. Instead of considering only the absolute position of tokens,\\nthe attention scores are adjusted based on their relative distances. The formulation for the\\nattention mechanism with relative position embeddings is given by:\\nAttention(Q, K, V) = softmax\\n\\x12Q(K + R)T\\ndk\\n\\x13\\nV (16)\\nwhere Q, K, and V are the query, key, and value matrices, respectively, R is the relative\\nposition embedding matrix, and dk is the dimension of the key vectors. The relative positions\\nare calculated as Rij = Rpos[i]pos[j], where pos[i] and pos[j] are the positions of tokens i and j\\nin the input sequence, respectively.\\nRoPE63 [134] is a type of position embedding that uses rotational matrices to capture the\\nrelative positions of tokens in the input sequence. Unlike traditional position embeddings that\\nadd or concatenate position information, RoPE encodes position information through rotation\\nin the embedding space, enabling models to preserve positional relationships effectively. The key\\nidea of RoPE is to bind the position encoding with the word embedding in a way that preserves\\nthe rotational relationship between embeddings. It uses a rotation matrix to modulate the\\nembedding based on its position, thereby aligning words by their relative positions instead of\\ntheir absolute positions. The formula for the Rotary Position Embedding is:\\nErot(xi, pi) = Rotate(xi, pi) = xi cos(pi) + (W xi) sin(pi) (17)\\nwhere xi is the token embedding, pi is the position embedding, and W is a learnable weight\\nmatrix. Rotary Position Embeddings were introduced by Su et al. [134] and have been shown\\nto improve the performance of LLMs on a range of tasks.\\nALiBi64 [206] position embeddings offer an alternative mechanism for incorporating po-\\nsition information into Transformer models. Unlike traditional absolute or relative position\\n63Rotary Position Embeddings\\n64Attention with Linear Biases\\n70'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 70, 'page_label': '71'}, page_content='embeddings, ALiBi introduces biases directly into the self-attention mechanism to handle po-\\nsitional dependencies. ALiBi introduces a linear bias based on the distance between tokens\\nin the attention scores. Similar to relative position embedding, it biases attention scores with\\na penalty based on the distances between keys and queries. Different from the relative posi-\\ntional embedding methods like T5 [139], the penalty scores in ALiBi are pre-defined without\\nany trainable parameters. This bias is subtracted from the attention logits before the softmax\\noperation, helping the model to prioritize nearby tokens over distant ones, which is crucial in\\nmany sequential tasks. The modified attention score with ALiBi can be represented as:\\nAttention(Q, K, V) = softmax\\n\\x12QKT\\ndk\\n bias(i, j)\\n\\x13\\nV,\\nbias(i, j) = b|i  j|\\n(18)\\nwhere Q, K, and V are the query, key, and value matrices, respectively, and b is a learnable\\nscalar parameter that controls the strength of the bias, and |i  j| is the absolute distance\\nbetween tokens i and j, and dk is the dimension of the key vectors.\\nIn Press, Smith, and Lewis [206], the authors found that ALiBi has better extrapolation\\nperformance than traditional position embeddings, and it can also improve the stability and\\nconvergence of Transformer models during training [349].\\nAttention Mechanisms Attention mechanisms are a key component of the Transformer ar-\\nchitecture. They allow the model to capture long-range dependencies and complex relationships\\nbetween tokens in the input sequence.\\nAn attention function can be described as mapping a query and a set of key-value pairs to\\nan output, where the query, keys, values, and output are all vectors. The output is computed\\nas a weighted sum of the values, where the weight assigned to each value is computed by a\\ncompatibility function of the query with the corresponding key. The two most commonly used\\nattention functions are additive attention [23] and dot-product (multiplicative) attention. The\\nFigure 26: (left) Scaled Dot-Product Attention. (right) Multi-head attention consists of several\\nattention layers running in parallel. Source: Vaswani et al. [334].\\nscaled dot-product attention function used in Vaswani et al. [334] is defined as follows:\\n71'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 71, 'page_label': '72'}, page_content='Attention(Q, K, V) = softmax\\n\\x12QKT\\ndk\\n\\x13\\nV (19)\\nwhere Q, K, and V are the query, key, and value matrices, respectively, and dk is the dimension\\nof the key vectors. While for small values of d k the two mechanisms perform similarly, additive\\nattention outperforms dot product attention without scaling for larger values of d k [37].\\nA multi-head attention function is implemented by splitting the query, key, and value vectors\\ninto multiple heads and computing the attention function in parallel, yielding d v-dimensional\\noutput values. These are concatenated and once again projected, resulting in the final values,\\nas depicted in Figure 26. The multi-head attention mechanism allows the model to jointly\\nattend to information from different representation subspaces at different positions, enhancing\\nthe models capacity to capture complex relationships in the data.\\nMultiHead(Q, K, V) = Concat(head1, . . . ,headh)WO,\\nheadi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\n(20)\\nwhere WQ\\ni , WK\\ni , and WV\\ni are the weight matrices for the query, key, and value projections of\\nthe i-th head, respectively, and WO is the final output projection matrix.\\nWe can categorize the attention mechanisms into full attention, sparse attention, multi-\\nquery/grouped-query attention, Flash attention, and Paged attention. The Full attention mech-\\nanism is the standard attention mechanism used in the vanilla Transformer architecture [334],\\nwhere each token attends to all other tokens in the sequence. It adopts the scaled dot-product\\nwe discussed in Equation 19. This mechanism is computationally expensive and has a quadratic\\ncomplexity regarding the number of tokens, which can limit the models scalability to longer\\nsequences. To address this issue, several studies have proposed alternative attention mecha-\\nnisms.\\nIn the Sparse attention mechanism, tokens only attend to a subset of other tokens according\\nto a predefined pattern (e.g., local windows). This mechanism reduces the computational\\ncomplexity of the attention operation and allows the model to scale to longer sequences.\\nSparse Attention(Q, K, V) = softmax((QKT Mdk\\n))V (21)\\nwhere M is a sparse attention mask that defines the pattern of attention between tokens.\\nVarious sparse attention mechanisms have been proposed in the literature, such as Peng,\\nLi, and Liang [128], Zaheer et al. [102] and Child et al. [64]. It a useful in tasks involving very\\nlong documents or sequences, such as document classification and genomic sequence analysis.\\nThe multi-query/grouped-query attention mechanism [78] is an extension of the standard\\nattention mechanism, where the keys and values are shared across all of the different attention\\nheads, significantly reducing the size of these tensors and hence the memory bandwidth re-\\nquirements of incremental decoding. This mechanism is handy in tasks requiring large amounts\\nof data, such as machine translation and summarization. It can significantly reduce the compu-\\ntational cost of the attention operation with small sacrifices in model quality. Palm [155] and\\nStarcoder [247] are examples of LLMs that use the multi-query attention mechanism. A trade-\\noff between multi-query and multi-head attention grouped-query (GQA) has been explored in\\nAinslie et al. [245]. In GQA, heads are grouped together, and each group shares the same trans-\\nformation matrices. This mechanism has been adopted and empirically tested in the Llama 2\\nmodel [329].\\nFlash attention [159] is an approach that proposes to optimize the speed and memory con-\\nsumption of attention modules on the GPUs. Modern GPUs have different memory types, and\\n72'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 72, 'page_label': '73'}, page_content='Flash attention takes advantage of this by organizing the input block on the faster memory 65.\\nThe updated version, FlashAttention-2 [187], has been introduced to further enhance the per-\\nformance of the attention module on GPUs by optimizing the partitioning of GPU thread blocks\\nand warps, achieving approximately a 2 speedup compared to the original FlashAttention.\\nPagedAttention [335] is based on the observation that GPU memory is bottlenecked by\\ncached attention keys and value tensors. These cached key and value tensors are often referred\\nto as KV cache. The KV cache is large and highly dynamic depending on the sequence length.\\nAuthors find that existent systems waste 60%-80% of the memory due to fragmentation and\\nover-reservation. PagedAttention proposed techniques inspired by virtual memory manage-\\nment66 to manage the KV cache, partition sequences to sub-sequences allocating corresponding\\nKV caches into non-contiguous physical blocks as shown in Figure 27.\\nFigure 27: PagedAttention: KV Cache is partitioned into blocks. Source: vLLM: Easy, Fast, and\\nCheap LLM Serving with PagedAttention [335].\\nPaging increases the GPU memory utilization and enables efficient memory sharing in par-\\nallel sampling (Figure 28).\\nFigure 28: PagedAttention: example of parallel sampling. Source: vLLM: Easy, Fast, and Cheap\\nLLM Serving with PagedAttention [335].\\nTo put all these discussions together, Zhao et al. [364] summarize the suggestions from\\nexisting literature for detailed configuration. For stronger generalization and training stability,\\nthe pre-RMSNorm should be chosen for layer normalization and SwiGLU or GeGLU as the\\n65SRAM has fast IO, while HBM is slower\\n66Paging\\n73'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 73, 'page_label': '74'}, page_content='activation function. In addition, LN may not be used immediately after embedding layers,\\nwhich is likely to incur performance degradation. As for position embeddings, RoPE or ALiBi\\nis a better choice since it performs better on long sequences.\\n3.5.5 Emerging architectures\\nSeveral emerging architectures have been proposed to address specific challenges or improve the\\nperformance of Transformers. One of the main issues with the vanilla Transformer architec-\\nture is the quadratic complexity regarding the number of tokens, which can limit the models\\nscalability to longer sequences. To address this performance issue, several studies proposed al-\\nternative architectures, such as parameterized state space models (e.g., S4 [167], GSS [195], and\\nH3 [160]]), long convolutions(e.g., Hyena [315]), and recursive update mechanisms (RWKV [313]\\nand RetNet [326]).\\nParameterized state space models are a class of models that use a parameterized state space\\nto represent the hidden states of the model. However, this method has prohibitive computation\\nand memory requirements, rendering it infeasible as a general sequence modelling solution. To\\naddress this issue, S4 [167] proposed a novel parameterized state space model that uses a fixed-\\nsize state space to represent the hidden states of the model. This approach significantly reduces\\nthe models computational and memory requirements while maintaining high performance on\\na range of tasks. In Gu, Goel, and R e [167], the authors found that S4 can be trained quickly\\nand efficiently compared to Transformer variants designed for long-range sequence modelling\\nas shown in Table 23.\\nLENGTH 1024 LENGTH 4096\\nSpeed Mem. Speed Mem.\\nTransformer 1x 1x 1x 1x\\nS4 1.58x 0.43x 5.19x 0.091x\\nTable 23: Benchmarks vs. efficient Transformers\\nLong Range Arena(LRA) [136] is a benchmark suite that evaluates the performance of LLMs\\non a range of tasks that require capturing long-range dependencies. It contains six tasks with\\nlengths of 1K-16K steps, encompassing modalities and objectives that require similarity, struc-\\ntural, and visuospatial reasoning. Table 24 shows the performance of S4 and 11 Transformer\\nvariants from Tay et al. [136]. Notably, S4 solves the Path-X task, an extremely challenging task\\nthat involves reasoning about LRDs over sequences of length 128 128 = 16384. All previous\\nmodels have failed (i.e., random guessing) due to memory or computation bottlenecks or inabil-\\nity to learn such long dependencies. Other benchmarks in Gu, Goel, and R e [167] show that\\nMODEL ListOps Text Retrieval Image Pathfinder Path-X AVG\\nTransformer 36.37 64.27 57.46 42.44 71.40 X 53.66\\nS4 58.35 76.02 87.09 87.26 86.05 88.10 80.48\\nTable 24:(Long Range Arena) Accuracy on the full suite of LRA tasks. (Top) Original Transformer\\nvariants in LRA. Source: Gu, Goel, and R e [167].\\nS4 looks promising for long-range sequence modelling, achieving state-of-the-art performance\\non tasks requiring capturing long-range dependencies.\\n74'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 74, 'page_label': '75'}, page_content='Long convolutions are a class of models that use convolutional layers to capture long-range\\ndependencies in the input sequence. Poli et al. [315] proposed an operation-efficient architec-\\nture called Hyena defined by two recurring sub-quadratic operators: a long convolution and\\nan element-wise multiplicative gating (Figure 29). Compared to the attention operators in\\nTransformers, Hyena has a lower computational complexity and memory footprint, making it\\nmore efficient for long-range sequence modelling.\\nFigure 29: The Hyena operator is defined as a recurrence of two efficient subquadratic primitives:\\nan implicit long convolution h (i.e., Hyena filters parameterized by a feed-forward network) and mul-\\ntiplicative element-wise gating of the (projected) input. The depth of the recurrence specifies the size\\nof the operator. Source: Poli et al. [315].\\n3.6 Tuning and Optimization\\nSince LLMs consist of millions or billions of parameters, parameter tuning can be expensive\\nand time-consuming. In this section, we discuss model adaptation of parameters and memory.\\n3.6.1 Parameter-efficient model adaptation\\nIn the existing literature, several methods exist to adapt the model parameters to improve the\\nperformance of LLMs [114, 119, 118]. These methods aim to reduce the number of parameters\\nin the model while maintaining performance as much as possible. In the following sections, we\\ndiscuss some of the most popular methods for parameter-efficient model adaptation, such as\\nadapter tuning, prefix tuning, prompt tuning, and LoRA (illustrated in Figure 30).\\nFigure 30: An illustration of four different parameter-efficient fine-tuning methods. MHA and FFN\\ndenote the multi-head attention and feed-forward networks in the Transformer layer, respectively.\\nSource: Zhao et al. [364].\\nAdapter tuning Adapter tuning is a parameter-efficient technique for transferring a pre-\\ntrained model to multiple downstream tasks without re-training the entire model for each new\\ntask. This approach involves introducing small, trainable modules called adapters between\\n75'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 75, 'page_label': '76'}, page_content='the layers of a pre-trained network. This allows the original networks parameters to remain\\nfixed while adapting the model to new tasks with a minimal increase in the total number of\\nparameters. Adapter tuning is designed to address the inefficiency of fine-tuning large models\\nwhere each new task typically requires re-training the entire model. Instead, adapter tuning\\nuses a base pre-trained model and introduces small adapter layers that are trained for each\\nspecific task into the Transformer architecture [68, 277], as shown in Figure 31.\\nFigure 31: On the left, the architecture of the adapter module and its integration with the Trans-\\nformer. The adapter module is added twice to each Transformer layer.\\nOn the right, the adapter module consists of a feed-forward network with a bottleneck layer and a\\nresidual connection. During adapter tuning, the green layers are trained on the downstream data; this\\nincludes the adapter, the layer normalization parameters, and the final classification layer (not shown\\nin the figure). Source: Houlsby et al. [68].\\nThese adapter layers are typically much smaller than the main model layers, significantly\\nreducing the number of new parameters that need to be trained. The main idea is that the\\nadapter module first compresses the input representation to a lower-dimensional space (using\\na non-linear transformation) and then expands it back to the original dimension, allowing the\\nmodel to adapt to new tasks without changing the pre-trained parameters. This architecture\\nis also called bottleneck architecture 67, and it can be represented with dimensional reduction\\nusually achieved using a linear transformation D : Rd  Rm where m < d. This layer is\\nrepresented by a weight matrix W  Rmd and a bias vector b  Rm.\\ny = (Wdx + bd) (22)\\nwhere  is a non-linear activation function, x is the input vector, and y is the output vector of\\nreduced dimensionality. After processing through the reduced dimension, the representation is\\n67In neural network design, a bottleneck architecture refers to a specific configuration where the input spaces\\ndimensionality is reduced to a lower dimension before being projected back to the original dimension or higher.\\nThis architecture is commonly employed in deep learning models to reduce computational complexity, improve\\ntraining efficiency, and sometimes help extract more generalized features.\\n76'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 76, 'page_label': '77'}, page_content='usually projected back to the original dimension or higher using another linear transformation\\nU : Rm  Rd represented by Wu  Rdm and bu  Rd.\\nz = (Wuy + bu) (23)\\nwhere z is the output vector, ideally representing the reconstructed version of the input after\\npassing through the bottleneck.\\nAlternatively, parallel adapter [170] can also be used in Transformer layers, where the\\nadapter is added in parallel with the attention layer and the feed-forward layer accordingly.\\nDuring fine-tuning, the adapter modules are optimized according to the specific task goals,\\nwhile the parameters of the original language model are frozen. In this way, we can effectively\\nreduce the number of trainable parameters during fine-tuning.\\nAdapter tuning has been shown to achieve near state-of-the-art performance on various\\ntasks with significantly fewer parameters than full fine-tuning. For example, on the GLUE\\nbenchmark, adapter tuning approaches the performance of full fine-tuning with only about\\n3.6% of the parameters trained per task.\\nPrefix tuning Prefix-tuning is introduced as an efficient alternative to traditional fine-tuning\\nmethods for deploying large pre-trained language models (PLMs) across various tasks. Tra-\\nditional fine-tuning requires updating and storing a separate copy of the model for each task,\\nwhich becomes computationally expensive as the models size increases (e.g., GPT-3s 175 bil-\\nlion parameters). Prefix-tuning addresses this by optimizing only a small set of parameters,\\nreferred to as a prefix, significantly reducing the storage and computational overhead. The\\nmethod involves prefixing a sequence of continuous, task-specific vectors to the input, allowing\\nsubsequent tokens in the Transformer model to attend to these prefixes as if they were part of\\nthe input sequence, as shown in Figure 32.\\nFigure 32: Illustration of the prefix-tuning method, which freezes the Transformer parameters and\\nonly optimizes the prefix (the red prefix blocks). Consequently, it only needs to store the prefix for\\neach task, making prefix-tuning modular and space-efficient. Note that each vertical block denotes\\ntransformer activations at a one-time step. Source: Li and Liang [119].\\nAn approach to optimize prefix vectors involves using a re-parameterization technique, as\\ndescribed in the work by Li and Liang [119]. This method employs a multilayer perceptron\\n77'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 77, 'page_label': '78'}, page_content='(MLP) function to map a smaller matrix to the parameter matrix of the prefixes rather than\\ndirectly optimizing the prefixes themselves. This technique has proven effective for stabilizing\\nthe training process. Once optimization is complete, the mapping function is discarded, leaving\\nonly the refined prefix vectors tailored to enhance performance on specific tasks. This approach\\nleverages the inherent capabilities of the Transformer while only modifying a minimal set of\\nparameters, making it modular and space-efficient. Li and Liang [119] provides detailed em-\\npirical evaluations demonstrating that prefix-tuning achieves comparable performance to full\\nfine-tuning while only learning about 0.1% of the parameters. Evaluations are performed on\\ntasks like table-to-text generation and summarization using models such as GPT-2 and BART.\\nResults indicate that prefix-tuning reduces parameter count significantly and maintains com-\\npetitive performance with traditional fine-tuning in full-data settings and often outperforms\\nit in low-data scenarios. The approach effectively handles tasks with unseen topics during\\ntraining, showcasing better generalization capabilities [94].\\nPrompt tuning Prompt tuning primarily involves incorporating trainable vectors, called\\nprompt tokens, at the input layer of a model. Based on discrete prompting techniques, these\\ntokens augment the input text to assist models in performing specific tasks. In prompt tuning,\\nthese task-specific embeddings are combined with the original text embeddings and processed\\nby language models. Specifically, the method known as P-tuning employs a flexible approach\\nto integrate context, prompt, and target tokens. This method is adaptable for tasks involving\\nFigure 33: Illustration of the prompt tuning method, which only requires storing a small task-specific\\nprompt for each task and enables mixed-task inference using the original pre-trained model. With\\nmodel tuning, each copy of tuned models requires a copy of billions of parameters. In contrast, a\\ntuned prompt would only require thousands of parameters per taska reduction of over five orders of\\nmagnitude. Source: Lester, Al-Rfou, and Constant [118].\\nunderstanding and generating natural language and utilizes a bidirectional LSTM to learn\\nrepresentations of soft prompt tokens. Only these prompt embeddings are updated based\\non task-specific requirements during the training phase. The effectiveness of prompt tuning\\nmethods depends significantly on the computational power of the underlying language models,\\nas they generally involve a limited number of trainable parameters at the input layer.\\nLiu et al. [188] introduces P-Tuning v2, a method that extends prompt tuning by applying\\ncontinuous prompts across all layers of a language model, improving upon the conventional\\n78'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 78, 'page_label': '79'}, page_content='method where prompts are only used at the input layer. They address the limitations of tra-\\nditional prompt tuning, which underperforms significantly on complex sequence labelling tasks\\nwhen model size is below 10 billion parameters [118]. P-Tuning v2 modifies the conventional\\nprompt tuning by:\\n Utilizing continuous prompts at every layer of the model to increase tunable parameter\\ncount without significantly increasing overall parameter load.\\n Improving adaptability across both simple and complex tasks by modifying the interaction\\nof prompts with model architecture [119, 129].\\nP-Tuning v2 has been evaluated across various model scales (from 330M to 10B parameters)\\nand tasks, including classification and sequence labelling. The experiments demonstrate that\\nP-Tuning v2 provides comparable results to full model fine-tuning, requiring only 0.1%-3% of\\nthe parameters to be tuned. Liu et al. [188] concludes that P-Tuning v2 significantly narrows\\nthe performance gap between prompt tuning and full fine-tuning, offering a robust, scalable,\\nand efficient alternative for adapting large pre-trained models to diverse NLU tasks.\\nLoRA The technique called LoRA (Low-Rank Adaptation) is used for efficient fine-tuning\\nneural networks, particularly in adapting dense layers to downstream tasks with fewer trainable\\nparameters. LoRA strategically freezes the original parameter matrix W  Rmn and applies\\nupdates using a low-rank decomposition approach, which involves two smaller matrices A \\nRmk and B  Rnk where k is much smaller than m or n. This method significantly reduces\\nthe memory and storage requirements by limiting the trainable parameters to those in A and\\nB rather than the entire matrix W.\\nThe main advantage of LoRA is its ability to maintain a single large model while adapting\\nit to various tasks using different sets of low-rank matrices for each task, enhancing storage\\nefficiency and reducing computational costs. Advanced methods for determining the optimal\\nrank have been proposed, such as importance score-based allocation [363]  i.e., AdaLoRA \\nand search-free optimal rank selection [332]  DyLoRA. These methods help determine the\\noptimal rank for the low-rank decomposition, ensuring the model is adapted efficiently to the\\nspecific task requirements.\\nIn AdaLoRA 68, the idea is that adding more trainable parameters to the critical weight\\nmatrices can lead to better model performance. In contrast, adding more parameters to those\\nless important weight matrices yields very marginal gains or even hurt model performance.\\nGiven the parameter budget, i.e., the number of total trainable parameters, AdaLoRA always\\nprefers allocating more parameters to those essential modules. Distributing the budget evenly to\\nall weight matrices/layers, like LoRA and other methods (e.g., adapter and prefix tuning), often\\ngives suboptimal performance [363]. AdaLoRA operates by parameterizing the incremental\\nupdates in the form of singular value decomposition (SVD), allowing for selective pruning\\nof updates based on their assessed importance. This selective pruning targets the singular\\nvalues of unimportant updates, effectively reducing their parameter budget while avoiding\\nthe computational intensity of performing exact SVD calculations. SVD-based adaptation\\nis represented as:\\nW = W0 +  = W0 + PQ (24)\\nwhere W0 is the original parameter matrix,  is the update, P and Q are the left and right\\nsingular vectors, and  is the singular value matrix. Zhang et al. [363] substantiates the ef-\\nfectiveness of AdaLoRA through extensive experiments across various NLP tasks, including\\n68Adaptive Low-Rank Adaptation\\n79'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 79, 'page_label': '80'}, page_content='question-answering and natural language generation. These experiments demonstrate notable\\nperformance improvements, particularly in low-budget settings, compared to baseline methods\\nsuch as full fine-tuning and other parameter-efficient techniques like LoRA and adapter tun-\\ning. Key benchmarks from the paper highlight AdaLoRAs superior performance on standard\\ndatasets like GLUE and SQuAD. It consistently outperforms other approaches while utilizing\\nfewer parameters.\\nDyLoRA69 is a search-free method for determining the optimal rank for low-rank decompo-\\nsition in neural networks. The method is based on the observation that the optimal rank for\\nlow-rank decomposition varies across different layers and tasks. The main advantages of Dy-\\nLoRA over conventional LoRA include its ability to dynamically adapt to different rank sizes\\nduring inference, eliminating the need for exhaustive search and re-training across different\\nrank sizes. This is achieved by training the low-rank modules (LoRA blocks) across a spectrum\\nof ranks during the training phase, which allows the model to adjust to the best-performing\\nrank size at runtime without additional computational cost. This method is inspired by the\\nnested dropout technique but tailored to the needs of dynamic rank adaptation. The imple-\\nmentation involves sampling a rank size during each training step and adjusting the adapter\\nmodules accordingly, which allows the model to learn to perform efficiently under various rank\\nsize constraints. The main improvements of DyLoRA over LoRA include:\\n1. Dynamic LoRA Blocks: DyLoRA modifies the standard LoRA blocks to be dynamic,\\nallowing them to adjust their rank size during inference. This adaptation leads to more\\nflexible models that can perform well across a broader range of tasks without specific\\ntuning for each task.\\n2. Search-Free Adaptation: By avoiding the exhaustive search for the optimal rank size,\\nDyLoRA reduces the training and adaptation time significantly. The model can be trained\\nonce and used dynamically across different settings, making it highly efficient.\\n3. Performance: Experimental results show that DyLoRA matches or exceeds the perfor-\\nmance of traditional LoRA with a static rank across various NLP tasks. This is demon-\\nstrated in tasks such as sentiment analysis, question answering, and natural language\\ngeneration, indicating the robustness and versatility of DyLoRA.\\n3.6.2 Memory-efficient model adaptation\\nIn addition to parameter-efficient model adaptation, memory-efficient techniques have been\\nproposed to reduce the memory footprint of LLMs. These methods aim to reduce the mem-\\nory requirements of LLMs during inference, making them more suitable for deployment in\\nresource-constrained environments. This section discusses some of the most popular methods\\nfor memory-efficient model adaptation, i.e. model quantization.\\nQuantization Quantization techniques reduce memory and computational costs by repre-\\nsenting weights and activations with lower-precision data types, such as 8-bit integers (int8).\\nThis enables loading larger models that would typically be too large to fit into memory and\\nspeeds up inference. This process can substantially reduce the storage requirements and the\\ncomputational complexity of deploying LLMs, which is crucial for their application in resource-\\nconstrained environments.\\nQuantization can be done in two ways: post-training quantization and quantization-aware\\ntraining. Post-training quantization is done after the model has been trained, while quantization-\\naware training is done during training. Post-training quantization is easier to implement, but\\nquantization-aware training can lead to better results.\\n69Dynamic Search-Free Low-Rank Adaptation\\n80'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 80, 'page_label': '81'}, page_content='Main quantization techniques include uniform quantization, non-uniform quantization, and\\nmixed-precision quantization. Uniform quantization maps the floating-point values to a fixed set\\nof integer values, while non-uniform quantization uses non-linear mapping to better represent\\nthe data distribution. Mixed-precision quantization uses a combination of different precision\\ndata types to represent the weights and activations.\\nUniform quantization discretizes the values within a certain range into equal-sized intervals.\\nMathematically, it can be described as:\\nLinearQuant(x, bitwidth) = Clip(round( x\\nbitwidth)  bitwidth, minV, maxV) (25)\\nwhere minV and maxV are the minimum and maximum scale range respectively [39].\\nNon-uniform quantization, such as logarithmic quantization, allocates more fine-grained\\nintervals to values that are more frequent or sensitive to quantization errors. This method can\\nbe represented as:\\nLogQuant(x, bitwidth)(x) = Clip(AP2(x), minV, maxV) (26)\\nwhere AP2 is the approximate-power-of-2 function that maps the input to the nearest power\\nof two as defined in Hubara et al. [39]. This approach is particularly effective for distributions\\nwith a high dynamic range [32].\\nMixed-precision quantization leverages the strengths of both uniform and non-uniform quan-\\ntization by using different precision data types for different parts of the model. For example,\\nweights can be quantized to 8-bit integers while activations are quantized to 16-bit integers.\\nBit-width Storage Reduction Accuracy Loss\\n32 (Full Precision) 0% 0%\\n16 50% 1%\\n8 75% 2%\\n4 87.5% 5%\\nTable 25: Performance comparison of quantized LLM\\nAs per Table 25, lower bit-widths generally result in more significant storage savings, but\\nthey can also lead to higher accuracy losses [40].\\n4 Utilization Strategies and Techniques\\nIn this section, we will discuss the strategies and techniques for effectively utilizing large lan-\\nguage models. We will start by discussing the importance of context in utilizing large language\\nmodels and how it can be used to improve their performance. We will then move on to the\\nconcept of chain-of-thought prompting and how it can be used to guide text generation. Finally,\\nwe will discuss the LLMs ability to plan for complex tasks.\\n4.1 In-Context Learning\\n4.1.1 ICL strategy\\nIn-context learning is a special prompting technique, initially introduced by Brown et al. [88],\\nthat allows the model to learn from the context of the prompt (examples shown in Figure 34).\\nICL consists of the task description and/or a few examples of the task as demonstrations\\n81'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 81, 'page_label': '82'}, page_content='Approach Representative Work Key Point\\nIn-context Learning (ICL) KATE [186], EPR [208], SG-ICL [176], APE [368],\\nStructured Prompting [168], GlobalE & Lo-\\ncalE [190]\\nDemonstration selection (similar, k-NN)\\nDemonstration selection (dense retrieval; con-\\ntrastive learning)\\nDemonstration selection (LLM as the demonstra-\\ntion generator)\\nDemonstration format (automatic generation &\\nselection)\\nDemonstration format (grouped context encod-\\ning; rescaled attention)\\nDemonstration order (entropy-based metric;\\nprobing set generation with LLM)\\nChain-of-thought Prompting (CoT) Complex CoT [163], Auto-CoT [243], Selection-\\nInference [157], Self-consistency [227], DI-\\nVERSE [295], Rationale-augmented ensem-\\nbles [226]\\nDemonstration (complexity-based selection)\\nDemonstration (automatic generation)\\nGeneration (alternate between selection and in-\\nference)\\nGeneration (diverse paths; self-ensemble)\\nGeneration (diverse paths; Verification (step-wise\\nvoting))\\nGeneration (rationale sampling)\\nPlanning Least-to-most prompting [244], DECOMP [175],\\nPS [338], Faithful CoT [302], PAL [164], Hug-\\ngingGPT [321], AdaPlanner [324], TIP [301],\\nRAP [275], ChatCoT [260], ReAct [236], Reflex-\\nion [322], Tree of Thoughts [359], LLM-modulo\\nframework [379]\\nPlan generation (text-based; problem decomposi-\\ntion)\\nPlan generation (text-based; problem decomposi-\\ntion)\\nPlan generation (text-based)\\nPlan generation (code-based)\\nPlan generation (code-based; Python)\\nPlan generation (code-based; models from Hug-\\ngingFace)\\nPlan refinement (skill memory)\\nFeedback acquisition (visual perception)\\nFeedback acquisition (LLM as the world model;\\nPlan refinement (Monte Carlo Tree Search))\\nFeedback acquisition (tool); Plan refinement\\n(conversation between LLM and tools)\\nFeedback acquisition (tool); Plan refinement (syn-\\nergizing reasoning and acting)\\nFeedback acquisition (text-based self-reflection);\\nPlan refinement (dynamic memory)\\nFeedback acquisition (vote comparison); Plan re-\\nfinement (tree-based search)\\nTable 26: Typical LLM utilization methods and their key points for ICL, CoT, and planning. Note\\nthat the key points only highlight the most important technical contribution. Source: Zhao et al. [364]\\ncombined in a specific order to form natural language prompts with specifically designed tem-\\nplates [88]. Finally, the test instance is appended to the prompt to form the input for LLMs\\nto generate the output. LLMs can improve the performance to execute a new task without\\nexplicit gradient update based on task demonstrations. Formally, the in-context learning task\\ncan be defined as follows:\\nLLM(I, f(x1, y1), . . . , f(xk, yk)| {z }\\ndemonstrations\\n, f(xk+1| {z }\\ninput\\n, |{z}\\nanswer\\n))  yk+1 (27)\\nwhere I is a task description, f(xi, yi) function that converts task demonstration to natural\\nlanguage, xk+1 is a new input query, yk+1 is the prediction of the output generated. The actual\\nanswer yk+1 is left as a blank to be predicted by the LLM.\\nSince ICLs performance heavily relies on demonstrations, it is important to design them\\nproperly in the prompts. The three main aspects are a direct consequence of what is defined in\\nEquation 27: how to select the task demonstrations, convert them into natural language, and\\narrange demonstrations in a reasonable order.\\nDifferent training strategies enhance ICL capabilities, improving performance across various\\ntasks without specific task optimization during the pre-training phase (see Figure 36 under the\\nTraining branch). Main approaches include Supervised In-context Training, such as MetaICL70\\nand Symbol Tuning, and Self-supervised In-context Training, such as Self-supervised ICL and\\nPICL [265].\\nMetaICL [196] proposed to continually train LLMs on a wide range of tasks 71 with demon-\\nstration examples. This approach is related to other works that use multi-task learning for\\nbetter zero-shot performance at test time [196]. However, MetaICL is distinct as it allows\\n70Meta-training for InContext Learning\\n71Classification, question answering, natural language inference, paraphrase detection and more\\n82'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 82, 'page_label': '83'}, page_content='Figure 34: In-context learning contrasted with traditional fine-tuning. Source: Brown et al. [88]\\nlearning new tasks from k examples alone, without relying on task reformatting (e.g., reducing\\neverything to question answering) or task-specific templates (e.g., converting different tasks to\\na language modelling problem). MetaICL is based on the core idea of in-context learning by\\nconditioning on training examples (i.e., explicitly training on an in-context learning objective).\\nSymbol Tuning [346] instead fine-tunes language models on in-context input-label pairs,\\nsubstituting natural language labels (e.g., positive/negative sentiment) with arbitrary sym-\\nbols (e.g., foo/bar). As a result, symbol tuning demonstrates an enhanced capacity to utilize\\nin-context information for overriding prior semantic knowledge. Compared to MetaICL, which\\nconstructs several demonstration examples for each task, instruction tuning mainly considers\\nan explanation of the task and is easier to scale up.\\nSelf-supervised ICL leverages raw corpora to generate input/output pairs as training data.\\nPICL also utilizes raw corpora but employs a simple language modelling objective, promoting\\ntask inference and execution based on context. PICL has shown to be more effective in zero-shot\\nsettings and task generalization [265].\\nEffective demonstration design is crucial, involving selecting and ordering examples or using\\ninstruction induction and reasoning steps (as shown in Figure 36 under the Inference/Demonstration\\n83'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 83, 'page_label': '84'}, page_content='Figure 35: Two examples of in-context learning, where a language model (LM) is given a list of\\ntraining examples (black) and a test input (green) and asked to make a prediction (orange) by predicting\\nthe next tokens/words to fill in the blank. Source: Lab [288]\\nIn-context Learning\\nInference\\nScoring FunctionChannel prompt tuning [196],kNN-Prompting [354]\\nDemonstration Designing\\nOrganization\\nSelecting\\nKATE [186],EPR [208],PPL [166],SG-ICL [176],Self Adaptive [233],MI [215],Q-Learning [242],Informative Score [293],Topic [342],UDR [293]\\nOrdering GlobalE&LocalE [190]\\nFormatting\\nInstruction Instruction Induction [173],APE [244],Self-Instruct [228]\\nReasoning Steps\\nCoT [228],Complex CoT [163],AutoCoT [243],Self-Ask [206],MoT [294],SuperICL [356],iCAP [224],Least-to-Most Prompting [244]\\nTraining Warmup\\nSelf-supervised In-context TrainingSelf-supervised ICL [152],PICL [270]\\nSupervised In-context Training\\nMetaICL [196],OPT-IML [174],FLAN [231],Super-NaturalInstructions [229],Scaling Instruction [156],Symbol Tuning [346]\\nFigure 36: Taxonomy of in-context learning. The training and the inference stage are two main\\nstages for ICL. During the training stage, existing ICL studies mainly take a pre-trained LLM as the\\nbackbone and optionally warm up the model to strengthen and generalize the ICL ability. Towards\\nthe inference stage, the demonstration design and the scoring function selection are crucial for the\\nultimate performance. Source: Dong et al. [265]\\nDesigning branch). The selection aims to choose good examples for ICL using unsupervised 72\\nor supervised methods. For example, KATE [186] and EPR [208] select demonstrations based\\non similarity. Ordering the selected demonstrations is also an important aspect of demonstra-\\ntion design. Lu et al. [190] have proven that order sensitivity is a common problem and affects\\nvarious models. To address this problem, studies have proposed several training-free meth-\\nods for ordering demonstrations. Liu et al. [186] sorted examples based on similarity, while\\nGlobalE&LocalE [190] orders demonstrations based on global and local entropy.\\n72Based on pre-defined metrics\\n84'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 84, 'page_label': '85'}, page_content='A common representation of demonstrations is concatenating examples (x1, y1),  , (xk, yk)\\nwith a template T directly. However, this approach may not be optimal for all tasks (i.e., when\\nthe task is complex or requires multiple steps such as math word problems and common-sense\\nreasoning). In those cases, learning the mapping from xi to yi with only k demonstrations is\\nchallenging. Template engineering has been studied in Liu et al. [122] and Liu et al. [186] to\\ngenerate task-specific templates. Some researchers have proposed designing a better demon-\\nstration format by describing tasks with instructions and adding intermediate reasoning steps\\nbetween examples (xi, yi). Instructions depend heavily on human input, but they can be gen-\\nerated automatically as shown in Honovich et al. [173] given several demonstration examples.\\nZhou et al. [368] proposed APE for automatic instruction generation and selection. To further\\nimprove the quality of the automatically generated instructions, Wang et al. [228] proposed\\nSelf-Instruct, which can eliminate its own generations.\\nAdding intermediate reasoning steps between examples introduced in Wang, Zhu, and Wang\\n[342] is also called Chain-of-Thought prompting. We will delve into Chain-of-Thought prompt-\\ning in the next Section 4.2.\\nICL operates at inference stage  without explicit gradient updates  focusing on task recog-\\nnition and learning through demonstrations. Task recognition utilizes pre-trained knowledge to\\nsolve tasks identified in the demonstrations. A Probably Approximately Correct (PAC) [347]\\nframework has been proposed to evaluate ICLs learnability, suggesting that LLMs can recog-\\nnize tasks from minimal inputs.\\nOn the other hand, task learning involves LLMs learning new tasks through demonstrations,\\nakin to implicit fine-tuning through the attention mechanism, which generates meta-gradients.\\nWith the examples provided in ICL, LLMs can implement learning algorithms such as gradient\\ndescent or directly compute the closed-form solution to update these models during forward\\ncomputation. Under this explanation framework, it has been shown that LLMs can effectively\\nlearn simple linear functions and even some complex functions like decision trees with ICL [144].\\nDifferent model scales exhibit distinct capabilities; smaller models are adept at task recognition,\\nwhile larger models (at least 66 billion parameters) are necessary for task learning [309].\\nScoring Function Target Efficiency Task Coverage Stability\\nDirect M(yj |\\nC, x)\\n+++ + +\\nPPL PPL( Sj) + +++ +\\nChannel M(x |\\nC, yj)\\n+ + ++\\nTable 27: Summary of different scoring functions.\\nDespite its promises, ICL performance is known to be highly sensitive to input examples.\\nThus, a focal piece of ICL is the example selection based on scoring functions, which decides\\nhow to transform the LLMs predictions into an estimation of the likelihood of a specific answer.\\nA direct estimation method adopts the conditional probability of candidate answers and selects\\nthe higher probability as the final answer [88]. However, this method poses some restrictions\\non the template design. For example, the answer tokens should be placed at the end of the\\ninput sequences. Perplexity (PPL) is another commonly used metric that computes the PPL\\nof the entire input sequence:\\nSj = {C, s(x, yi, I)} (28)\\nwhere C are the tokens of the demonstration examples, x is the input query, and yi is the\\ncandidate label. As PPL is a global metric (i.e., it considers the entire input sequence), it\\n85'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 85, 'page_label': '86'}, page_content='removes the limitations of token positions but requires extra computation time. In generation\\ntasks such as machine translation, ICL predicts the answer by decoding tokens with the highest\\nsentence probability combined with diversity-promoting strategies such as beam search or Top-\\np and Top-k [91] sampling algorithms. Min et al. [197] proposed a channel scoring function that\\nestimates the likelihood of the input query given the candidate answer73, which is more efficient\\nand stable than the direct estimation method. In this way, language models are required to\\ngenerate every token in the input, which could boost the performance under imbalanced training\\ndata regimes. To calibrate the bias or mitigate the sensitivity via scoring strategies, some studies\\nadd additional calibration parameters to adjust the model predictions [141].\\n4.1.2 ICL performance and origins\\nKnowing and understanding the factors that influence ICL can help improve LLMs perfor-\\nmance. ICL has a close connection with instruction tuning (discussed in Section 3.4.1) in that\\nboth utilize natural language to format the task or instances. However, instruction tuning\\nneeds to fine-tune LLMs for adaptation, while ICL only prompts LLMs for utilization [364].\\nFurthermore, instruction tuning can enhance the ICL ability of LLMs to perform target tasks,\\nespecially in the zero-shot setting 74 [156].\\nStage Factor\\nPretraining\\nPretraining corpus domain [211]\\nPretraining corpus combination [211]\\nNumber of model parameters [232, 88]\\nNumber of pretraining steps [232]\\nInference\\nLabel space exposure [198]\\nDemonstration input distribution [198]\\nFormat of input-label pairing [198, 249]\\nDemonstration input-label mapping [198, 237, 346]\\nDemonstration sample ordering [190]\\nDemonstration-query similarity [190]\\nDemonstration diversity [249]\\nDemonstration complexity [249]\\nTable 28: Summary of factors that correlate relatively strongly to ICL performance. Source: Dong\\net al. [265]\\nSeveral factors correlate relatively strongly to ICL performance, as shown in Table 28.\\nICL ability may arise by putting multiple corpora together in the pre-training stage, and the\\ndomain source is more important than the corpus size [211]. In contrast, pre-train on corpora\\nrelated to downstream tasks and models with lower perplexity does not always perform better\\nin ICL [211]. Wei et al. [232] suggested that a pre-trained model suddenly acquires some\\nemergent ICL abilities when it achieves a large scale of pretraining steps or model parameters,\\nand Brown et al. [88] showed that the ICL ability grows as the parameters of LLMs increase from\\n0.1 billion to 175 billion. At the inference stage, the properties of the demonstrations influence\\nthe ICL performance, such as the label space exposure, the format of input-label pairing,\\nthe ordering of demonstration samples, and the complexity of demonstrations [198, 249, 190].\\nThere are contrasting results on the impact of input-label mapping related to ICL [198, 237].\\nAn interesting finding is that, when a model is large enough, it will show an emergent ability\\n73Compute the conditional probability in a reversed direction\\n74Using only task descriptions\\n86'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 86, 'page_label': '87'}, page_content='to learn input-label mappings, even if the labels are flipped 75 or semantically-unrelated76 [345].\\nSome general validated factors for the ICL demonstrations are that they should be diverse,\\nsimple, and similar to the test example in terms of the structure [249]. Lu et al. [190] indicated\\nthat the demonstration sample order is also an important factor. Liu et al. [186] found that the\\ndemonstration samples with closer embeddings 77 to the query samples usually perform better\\nthan those with farther embeddings 78.\\nThe reasons for the ICL ability have been investigated from different perspectives. Focusing\\non the pretraining data distribution, Chan et al. [151] showed that the ICL ability is driven by\\ndata distributional properties. The ICL ability emerges when the training data have examples\\nappearing in clusters and have enough rare classes. Xie et al. [234] explained ICL as implicit\\nBayesian inference79 and constructed a synthetic dataset to prove that the ICL ability emerges\\nwhen the pretraining distribution follows a mixture of hidden Markov models. The hypothe-\\nses is that LM learn to do Bayesian inference during pre-training. To predict the next token\\nduring pretraining, the LM must infer (locate) the latent concept 80 for the document using\\nevidence from the previous sentences. Later, if the LM infers also the latent concept prompt\\n(provided by the demonstrations), then the in-context learning ability occurs. Under the learn-\\ning mechanism, the ICL ability is explained by the ability of Transformers to encode effective\\nlearning algorithms to learn unseen linear functions according to demonstration samples, and\\nencoded learning algorithms can achieve a comparable error to that from the least squares\\nestimator [268]. Also Li et al. [296] showed the ability of Transformers to implement a proper\\nfunction class through implicit empirical risk minimization for the demonstrations. From an\\ninformation-theoretic perspective, Hahn and Goyal [273] showed an error bound for ICL under\\nlinguistically motivated assumptions to explain how next-token prediction can bring about the\\nICL ability. Another series of works attempted to build connections between ICL and gradient\\ndescent and found that Transformer-based in-context learners can implement standard fine-\\ntuning algorithms implicitly [144, 308, 296]. Looking at functional components, Olsson et al.\\n[204] found indirect evidence that Induction heads 81 might constitute the mechanism for the\\nmajority of all ICL in large transformer models.\\nIn-context learning (ICL) evaluation spans traditional tasks and newly proposed challenging\\ntasks, and it provides open-source tools for standardized evaluation. ICL has been tested\\nagainst established benchmarks, such as SuperGLUE and SQuAD, with mixed results. GPT-3,\\nfor example, exhibited comparable performance to state-of-the-art fine-tuning on some tasks\\n75Flipped-label ICL uses flipped targets, forcing the model to override semantic priors to follow the in-context\\nexemplars. For example, in the sentiment analysis task, the label Positive becomes Negative in ICL context\\nand viceversa\\n76The labels are semantically unrelated to the task(e.g., for sentiment analysis, it uses foo/bar instead of\\nnegative/positive)\\n77Using Classify Token (CLS) embeddings of a pre-trained RoBERTa to measure the proximity of two sen-\\ntences with the Euclidean distance. The CLS token is extensively used to capture the context and semantics of\\nthe input (e.g., the sentiment in sentiment analysis; category in classification tasks; etc.).\\n78Retrieving the input k nearest neighbourhoods ordered by ascending similarity measure\\n79Bayesian inference is a method of statistical inference in which Bayes theorem is used to update the\\nprobability for a hypothesis as more evidence or information becomes available. Fundamentally, Bayesian\\ninference uses prior knowledge, in the form of a prior distribution in order to estimate posterior probabilities.\\nP(H||E) = fracP (E|H)  P(H)P(E)), where P(H) is the prior probability of hypothesis H, P(E|H) is the\\nlikelihood of evidence E given hypothesis H, P(E) is the marginal likelihood of evidence, and P(H|E) is the\\nposterior probability of hypothesis H given evidence E [348].\\n80A latent variable that contains various document-level statistics. For example, a news topics concept\\ndescribes a distribution of words (news and their topics), a format (the way that news articles are written), a\\nrelation between news and topics, and other semantic and syntactic relationships between words. In general,\\nconcepts may be a combination of many latent variables that specify different aspects of the semantics and\\nsyntax of a document\\n81attention heads that implement a simple algorithm to complete token sequences like [ A][B] . . .[A]  [B]\\n87'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 87, 'page_label': '88'}, page_content='within SuperGLUE but lagged in most natural language understanding tasks. Scaling the\\nnumber of demonstration examples has shown potential but has yet to bridge the gap fully\\nbetween ICL and traditional fine-tuning methods [88, 168].\\nNew benchmarks have been introduced to assess the capabilities of large language models\\n(LLMs) beyond traditional fine-tuning. The BIG-Bench and BIG-Bench Hard focus on tasks\\nranging from linguistics to social behaviours, with models outperforming human raters on many\\nof these tasks [246, 218]. OPT-IML Bench has been designed to evaluate the generalization\\ncapabilities of LLMs across various held-out categories, emphasizing the models generalization\\ncapabilities [174]. OpenICL has been developed to provide a flexible and unified framework for\\nICL evaluation. This toolkit supports different LLMs and tasks, enabling consistent implemen-\\ntation and evaluation of ICL methods across various studies [351].\\nThe application of In-Context Learning (ICL) has transcended the domain of natural lan-\\nguage processing (NLP), influencing research in various modalities such as visual tasks, vi-\\nsion+language integration, and speech. Visual In-Context Learning explores how models gen-\\neralize learned visual concepts to new, unseen tasks by leveraging contextual demonstrations\\nakin to NLP-based ICL. Techniques such as image patch infilling and training models like\\nmasked autoencoders (MAE) exemplify this approach [149]. Noteworthy models like Painter\\nand SegGPT have been developed to handle multiple tasks or integrate various segmentation\\ntasks into a single framework [340, 341]. The Prompt Diffusion model introduced by Wang et al.\\n[343] represents a pioneering effort in diffusion-based models displaying ICL capabilities, partic-\\nularly when guided by textual prompts [343]. Integrating visual contexts with linguistic models\\nhas significantly improved vision-language tasks. Frozen and Flamingo models have demon-\\nstrated the feasibility of multi-modal, few-shot learning by combining vision encoders with large\\nlanguage models (LLMs). These models effectively perform ICL on multi-modal tasks when\\ntrained on large-scale multi-modal web corpora [137, 145]. Kosmos-1 and METALM extend\\nthese capabilities by demonstrating strong performance across various vision-language tasks,\\nunderpinned by a semi-causal language modelling objective [278, 169].\\n4.1.3 ICL future research\\nFuture research in ICL is expected to focus on several key areas, including the optimization\\nof pretraining objectives, the distillation of ICL abilities, the enhancement of ICL robustness,\\nthe improvement of ICL efficiency and scalability, the updating of knowledge within LLMs,\\nthe augmentation of models, and the expansion of ICL into multi-modal domains [265]. Op-\\ntimizing pretraining objectives to better align with ICL requirements could enhance model\\ncapabilities for ICL applications. Introducing intermediate tuning phases and tailoring pre-\\ntraining objectives to better align with ICL requirements could bridge this gap and enhance\\nmodel capabilities for ICL applications [211]. An important goal is to distill ICL capabilities\\nfrom larger models to smaller, more efficient ones, potentially enabling the deployment of ICL\\nin resource-constrained environments [193]. Another area of improvement is the robustness of\\nICL, which is highly susceptible to the format and permutation of demonstrations [141, 190],\\nwithout compromising accuracy or efficiency [374].\\nA more theoretical understanding of ICLs mechanisms could lead to more robust imple-\\nmentations. Moreover, the scalability of ICL is constrained by the input limitations of language\\nmodels and the computational cost associated with large numbers of demonstrations. Innova-\\ntive strategies like structured prompting [168] and dynamic prompting [336] are being explored\\nto address these challenges. The development of models with extended context capabilities [290]\\nindicates significant potential for progress in this area. Finally, the expansion of ICL into multi-\\nmodal domains is expected to yield new insights and applications, particularly in vision and\\nspeech [265].\\n88'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 88, 'page_label': '89'}, page_content='4.2 Chain-of-Thought\\n4.2.1 CoT strategy\\nChain-of-Thought (CoT) prompting is an enhanced strategy developed to augment the per-\\nformance of large language models (LLMs) on complex reasoning tasks such as arithmetic,\\ncommonsense, and symbolic reasoning [230, 123, 81]. This method integrates intermediate\\nreasoning steps within the prompts, providing a more structured path towards the solution.\\nTo some extent, CoT can be considered a special case of ICL, as it involves the generation\\nFigure 37: Chain-of-Thought reasoning for GSM8k math word problem. The prompt is coloured\\nblack, and the reasoning path produced by the language model is coloured teal. This reasoning path\\ncontains two reasoning steps. Source: Li et al. [295]\\nof prompts with a series of intermediate reasoning steps (Figure 38). Still, the ordering of\\ndemonstrations, in this case, has a relatively minor impact on the performance of LLMs [230].\\nWei et al. [230] and Wang et al. [227] have shown that language models, when large enough\\n(i.e., >100 billion parameters), can learn to perform complex reasoning tasks through CoT\\nprompting without explicit task-specific [232].\\nCoT can be effectively combined with In-context Learning (ICL) in both few-shot and zero-\\nshot settings:\\n Few-shot CoT. In the few-shot scenario, CoT augments standard input-output pairs\\nwith intermediate reasoning steps. The design of CoT prompts is crucial; incorporating\\ndiverse and complex reasoning paths has been shown to boost LLM performance signifi-\\ncantly. An automated approach, Auto-CoT, facilitates the generation of CoT sequences\\nwithout manual effort by clustering and selecting representative questions [243].\\n Zero-shot CoT. Unlike its few-shot counterpart, zero-shot CoT does not rely on an-\\nnotated demonstrations. Instead, it generates reasoning steps directly from a prompt,\\nsignificantly improving performance when scaled to larger models. This approach was\\npioneered by models like Flan-T5, which demonstrated improved zero-shot performance\\nthrough instruction tuning on CoT annotations [156].\\nTo apply these strategies effectively, it is essential to design CoT prompts that guide the\\nmodel through the reasoning process. In Li et al. [295], the authors have shown that using\\ndiverse CoTs (i.e., prompts with multiple reasoning paths for each problem) can significantly\\n89'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 89, 'page_label': '90'}, page_content='Figure 38: A comparative illustration of in-context learning (ICL) and chain-of-thought (CoT)\\nprompting. ICL prompts LLMs with a natural language description, several demonstrations, and a\\ntest query, while CoT prompting involves a series of intermediate reasoning steps in prompts. Source:\\nZhao et al. [364]\\nFigure 39: The DIVERSE approach for CoT. Source: Li et al. [295]\\nenhance the performance of LLMs on complex reasoning tasks. The proposed method, DI-\\nVERSE82, generates diverse CoTs by leveraging a self-ensemble approach that alternates be-\\ntween selection and inference. It has three main components: first, it generates diverse prompts\\nto explore different reasoning paths for the same question; second, it uses a verifier to filter out\\nincorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step\\nindividually instead of the whole chain (Figure 39). In the first step, the model generates mul-\\ntiple reasoning paths for each question, which are then used to create diverse prompts following\\nthe idea that All roads lead to Rome. As an improvement of Wang et al. [227], DIVERSE se-\\nlects M1 different prompts for each question and M2 reasoning paths for each prompt, resulting\\nin M1  M2 diverse prompts. Then, the verifier takes a question and a candidates reasoning\\npath and outputs the probability that the reasoning path leads to the correct answer. Different\\npredictions are aggregated using a voting verifier to obtain the final prediction:\\ny = arg max\\ny\\nM1X\\ni=1\\n1y=yi  f(xi, zi, yi) (29)\\nwhere 1y=yi is an indicator function that equals 1 if y = yi, and f() is the probability produced\\nby the verifier.\\n82Diverse Verifier on Reasoning Step\\n90'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 90, 'page_label': '91'}, page_content='Figure 40: A: Chain of thoughts (in blue) are intermediate reasoning steps towards a final answer.\\nThe input of CoT prompting is a stack of a few (often 8) CoT cases before a test question. Then,\\nthe language model will continue generating an output CoT for the test question. B: Chains of harder\\nreasoning complexity are chains with more reasoning steps (9 steps in this case, v.s. only 2 steps in\\nsubfigure A). Source: Fu et al. [163]\\nAnother intuitive idea is that prompting with more complex reasoning steps (i.e., chains\\nwith more reasoning steps) is more likely to elicit the reasoning ability of LLMs [163], which can\\nresult in generating correct answers (Figure 40). Other complexity indicators than the number\\nof reasoning steps, such as question lengths or the length of the underlying formula for solving\\na given problem, also exist, but improvements in performance are consistent across various\\ncomplexity indicators. Consequently, question length can be used as a proxy for complexity for\\ndatasets not annotated with reasoning steps to generate CoT prompts. In that way, annotating\\nonly the identified few-shot instances is possible, thus reducing the annotation cost [163]. To\\nexclude complexity correlated factors, Fu et al. [163] proposed prompts evaluation:\\n Simpler examples but the same number of reasoning steps.For instance, compar-\\ning 24 cases that each require 3 reasoning steps with 8 cases that each require 9 reasoning\\nsteps, both resulting in a total of 72 steps.\\n Prompts of the longest lengths but not necessarily the most steps. This ensures\\nthat the length is not the only factor being assessed.\\nIt turned out that the complexity of reasoning steps is the most important factor for the\\nperformance of LLMs on complex reasoning tasks [163]. Complexity-based prompting can be\\nfurther enhanced by using the output selection method called Complexity-based Consistency,\\nalleviating the possibility that the model can take shortcuts during reasoning 83. The method\\nexplicitly promotes outputs with more complex reasoning chains at inference time, similar to\\nthe self-consistency practice in Wang et al. [227]. A voting mechanism is used to select the final\\noutput among top K complex reasoning chains, as shown in Figure 41.\\nPreviously mentioned methods rely on two major paradigms: Zero-Shot-CoT and Manual-\\nCoT. Zero-Shot-CoT is a task-agnostic paradigm that generates reasoning steps directly from\\n83Relying on spurious correlations that inevitably exist in the training data and are not related to the reasoning\\nprocess as shown by Mudrakarta et al. [49], Lai et al. [117], and Sugawara et al. [54]\\n91'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 91, 'page_label': '92'}, page_content='Figure 41: Complexity-based Consistency for CoT. During decoding, it samples N reasoning chains\\nfrom the language model (N = 5 here) and takes the majority answer over the K (K = 3 here) most\\ncomplex generated chains. Source: Fu et al. [163]\\nFigure 42: Zero-Shot-CoT [285] (using the Lets think step by step prompt) and Manual-CoT[230]\\n(using manually designed demonstrations one by one) with example inputs and outputs of an LLM.\\nSource: Zhang et al. [243]\\nthe prompt, eliminating the need for annotated CoT datasets [285], adding a single prompt like\\nLets think step by step after the test question to facilitate the reasoning chains in LLMs. On\\nthe other hand, Manual-CoT uses manually designed demonstrations one by one, which can be\\nexpensive and time-consuming to create [230]. Since this prompting paradigm is task-agnostic\\nand does not need input-output demonstrations, it is called Zero-Shot-CoT (left of Figure 42).\\nWith Zero-Shot-CoT, LLMs have shown to be decent zero-shot reasoners.\\nThe other paradigm is few-shot prompting with manual reasoning demonstrations one by\\none [230]. Each demonstration has a question and a reasoning chain. A reasoning chain\\ncomprises a rationale (a series of intermediate reasoning steps) and an expected answer. With\\nall the demonstrations being manually designed, this paradigm is called Manual-CoT (right of\\nFigure 42).\\nTo mitigate the effect of reasoning chain mistakes from Zero-Shot-CoT, Zhang et al. [243]\\n92'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 92, 'page_label': '93'}, page_content='proposed the use of Auto-CoT, a method that generates demonstrations automatically since\\ntheir diversity is crucial for the performance of LLMs. It consists of two main components: a\\nclustering algorithm that groups similar questions and a representative selection algorithm that\\nselects the most representative questions from each cluster. The overall procedure is illustrated\\nin Figure 43. Diversity-based clustering may mitigate misleading by similarity effects 84, and\\nFigure 43: demonstrations (on the right) are automatically constructed one by one (total: k) using\\nan LLM with the Lets think step by step prompt. Source: Zhang et al. [243]\\nthe representative selection algorithm can select the most representative questions from each\\ncluster is used as demonstrations to generate reasoning chains for the test question. Auto-CoT\\nhas shown to be effective in generating diverse reasoning chains and improving the performance\\nof LLMs on arithmetic and symbolic reasoning [243].\\n4.2.2 CoT performance and origins\\nCoT is considered by many as an emergent ability [232], a capability that suddenly appears and\\ngreatly enhances the performance of LLMs when they reach a certain scale. Moreover, CoT is\\nonly effective for tasks that require step-by-step reasoning, such as arithmetic, commonsense,\\nand symbolic reasoning [230, 123, 81]. Whereas, for other tasks, CoT can be detrimental to\\nthe performance of LLMs with respect to standard prompting [226], e.g., MNLI-m/mm, SST-2,\\nand QQP from GLUE[56]. It seems that the effectiveness of CoT is inversely proportional to\\nthe effectiveness of standard prompting [230].\\nMain prompting components, e.g., symbols, patterns, and text, impact CoT. Studies have\\ndemonstrated that both patterns and text are crucial for CoT performance, as their removal\\ncan cause a significant decline in effectiveness: text enables LLMs to generate meaningful\\npatterns, while patterns help LLMs comprehend tasks and produce text that facilitates their\\nresolution [192].\\n84The retrieved demonstration questions are similar to the test question and ask how long will it take him\\nto cook the rest ? The reasoning chains generated by Zero-Shot-CoT produce answers regarding the total of\\ninstead of the rest. Following the demonstrations, the component that retrieves the top-k similar questions \\ncalled Retrieval-Q-CoT  also fails by misunderstanding the meaning of the rest.\\n93'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 93, 'page_label': '94'}, page_content='Figure 44: Program-of-Thoughts (PoT) for solving math word problems. The input is a math word\\nproblem, and the output is a program that can solve the problem. Source: Chen et al. [259]\\nThe origins of CoT ability are widely hypothesized to be elicited by training on code since\\nthose models have shown to be more effective in reasoning tasks [162, 185]. Intuitively, code\\ndata is well organized with algorithmic logic and programming flow, which may be helpful in\\nimproving the reasoning performance of LLMs. However, this hypothesis still lacks publicly\\nreported evidence of ablation experiments (with and without training on code). Well try to\\naddress this gap in the next section 5, by conducting a series of experiments to evaluate the\\neffectiveness of training on code data for reasoning tasks. In addition, instruction tuning seems\\nnot to be the main factor for CoT ability since the performance of LLMs on CoT tasks is not\\nsignificantly improved by instruction tuning [156].\\n4.3 Program-of-Thoughts\\nPoT uses a programmatic approach to prompt LLMs to solve complex reasoning tasks pro-\\nposed by Chen et al. [259]. It leverages models to generate text and programming languages\\nstatements, executing them to get the final answer. The approach is similar to CoT, but the\\nreasoning steps are expressed in a more structured way, resembling a program (see Figure 44).\\nCoT uses LLMs for both reasoning and computation, i.e., the language model not only needs\\nto generate the mathematical expressions but also needs to perform the computation in each\\nstep85. Whatever the case, LLMs are not ideal for actually solving these mathematical expres-\\nsions, because:\\n LLMs are very prone to arithmetic calculation errors, especially when dealing with large\\nnumbers.\\n LLMs cannot solve complex mathematical expressions like polynomial equations or even\\ndifferential equations.\\n LLMs are highly inefficient at expressing iteration, especially when the number of iteration\\nsteps is large.\\nPoT can overcome these limitations by using a programmatic approach, where the reasoning\\nsteps are expressed as Python programs that can be executed to get the final answer by a\\nPython interpreter. The programmatic approach is also different from generating equations\\ndirectly, that is found to be more challenging for LLMs [230]. It mainly differs from equation\\ngeneration for the following reasons:\\n PoT breaks down the reasoning process into a series of steps, each of which is expressed\\nas a Python statement;\\n85Some studies contradict the fact that LLMs can perform computations or reasoning tasks [207, 373, 283].\\n94'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 94, 'page_label': '95'}, page_content=' it binds semantic meaning to variables, which can elicit language models reasoning ca-\\npabilities and generate more accurate programs\\nIn zero-shot PoT, a caveat is that LLM can fall back to generating a reasoning chain in comments\\nrather than in the program. Therefore, Chen et al. [259] proposes to suppress # token logits\\nto encourage it to generate programs.\\nAs confirmed by our experiments in Section 5, PoT can significantly improve performance\\non math problems compared to CoT. Even though PoT is effective on highly symbolic math\\nproblems, it still struggles with AQuA dataset, which contains complex algebraic questions\\nmainly due to the diversity of questions, which the demonstration cannot possibly cover. For\\nsemantic reasoning tasks like commonsense reasoning (StrategyQA), probably PoT is not the\\nbest option. In contrast, CoT can solve more broader reasoning tasks.\\n4.4 Planning for complex tasks\\n4.4.1 Commonsense knowledge\\nICL and CoT are two simple yet general strategies for solving various tasks. However, they\\nstruggle with complex tasks that require long-term planning, such as mathematical word prob-\\nlems [207] and multi-hop question answering [373]. Commonsense knowledge 86 is essential for\\nNLP systems to understand and generate human-like language. Main categories are summa-\\nrized in Bian et al. [373]:\\n General commonsense: refers to knowledge that is widely shared and assumed to be\\ntrue by most people, such as the sun rises in the east and sets in the west.\\n Physical commonsense: involves intuitive knowledge about the physical world, such\\nas objects falling to the ground when dropped and water flowing downhill.\\n Social commonsense: involves knowledge about social norms, customs, and practices,\\nsuch as it is polite to say thank you when making requests.\\n Science commonsense: involves knowledge about basic scientific principles, such as\\ngravity pulling all objects on Earth to Earths centre.\\n Event commonsense: involves knowledge about the sequence of events and their causal\\nrelationships, such as if a glass is knocked over, the liquid inside will spill.\\n Numerical commonsense: involves knowledge about numbers, such as a human has\\ntwo hands and ten fingers.\\n Prototypical commonsense: involves knowledge about typical or prototypical exam-\\nples of concepts, such as a swallow is a kind of bird and a bird has wings.\\n Temporal commonsense: involves knowledge about time, such as travelling abroad\\nrequires a longer time than taking a walk.\\nA list of commonsense QA datasets commonly used in evaluating LLMs is shown in Table 29.\\nThese datasets encompass domains like general, physical, social, science, event, numerical, pro-\\ntotypical, and temporal commonsense. Table 30 shows the accuracy of GPT-3, GPT-3.5, and\\nChatGPT on these datasets. The ability of models to leverage commonsense is probably im-\\nproved by instruction tuning and human alignment, looking at the results of Instruct GPT and\\n86It includes knowledge about the spatial, physical, social, temporal, and psychological aspects of the typical\\neveryday life, as well as an awareness of social norms, beliefs, and values [8].\\n95'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 95, 'page_label': '96'}, page_content='Dataset Domain Example (Bold texts are the answers)\\nCommonsenseQA General Choose your answer to the question: Where are you likely to find\\na hamburger? A. fast food restaurant , B. pizza, C. ground\\nup dead cows, D. mouth, E. cow circus\\nOpenBookQA General Choose your answer to the question: If a person walks in the\\nopposite direction of a compass arrow they are walking A. west,\\nB. north, C. east, D. south\\nWSC General Choose sub-sentence A or B that completes the sentence: The\\ntrophy doesnt fit into the brown suitcase because A. the trophy\\nis too small. B. the suitcase is too small .\\nPIQA Physical Choose one that is correct: A. ice box will turn into a cooler\\nif you add water to it . B. ice box will turn into a cooler if\\nyou add soda to it.\\nSocial IQA Social Taylor taught math in the schools after studying to be a teacher.\\nChoose the most suitable answer for the question: What does\\nTaylor need to do before this? A. get a certificate , B. teach\\nsmall children, C. work in a school\\nARC Science Choose your answer to the question: Which technology was\\ndeveloped most recently? A. cellular telephone, B. television,\\nC. refrigerator, D. airplane\\nQASC Science Choose your answer to the question: What is described in terms\\nof temperature and water in the air? A. storms; B. climate;\\nC. mass; D. seasonal; E. winter; F. density; G. length\\nHellaSWAG Event Choose your answer to the question: We see a chair with a pillow\\non it. A. a man holding a cat does curling. B. a man holding a\\ncat starts hitting objects on an item. C. a man holding a cat is\\nwrapping a box. D. a man holding a cat sits down on the\\nchair.\\nNumerSense Numerical a square is a shape with maskequally length sides. (four)\\nProtoQA Prototypical Use simple words separated by commas to name something in\\nyour life that could cause you to lose weight. ( Eating less,\\nexercising more, stress.)\\nMC-TACO Temporal Select all feasible answers for the question: Carl Laemmle, head\\nof Universal Studios, gave Einstein a tour of his studio and in-\\ntroduced him to Chaplin. At what time did Einstein return\\nhome? A. 8:00 PM; B. a second later; C. a hour later\\nTable 29: Examples from commonsense QA datasets. Source: Bian et al. [373]\\nChatGPT versus GPT-3 in Table 30.) ChatGPT demonstrates strong capabilities in common-\\nsense QA tasks but has limitations in identifying necessary knowledge. It has been proved by\\nevaluating answers generated by ChatGPT on questions from each commonsense QA dataset\\nusing the following prompt:\\nWhat knowledge is necessary for answering this question?\\n{question} {answer choices(if applicable)} .\\nThis means that LLMs are inexperienced problem solvers who rely on memorizing a large\\namount of information to cover the answers[373]. Kambhampati [378] and Kambhampati et al.\\n[379] strongly argue that LLMs cant reason or plan autonomously. Techniques like Chain-of-\\nThought (CoT), ReACT, and fine-tuning, which are often used to enhance their capabilities,\\nstill do not enable sufficient generalization. LLMs struggle with self-verification because they\\nlack the ability to assess the accuracy of their outputs. A key question arises:\\n96'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 96, 'page_label': '97'}, page_content='Dataset GPT-3 Instruct GPT ChatGPT Human\\nCommonsenseQA 38 81 74 88.9\\nOpenBookQA 22 65 73 89.3\\nWSC 46 78 78 92.1\\nPIQA 48 77 78 94.5\\nSocial IQA 36 71 62 86.9\\nARC 27 88 94 \\nQASC 25 75 74 93.0\\nHellaSWAG 19 61 67 95.7\\nNumerSense 45 63 79 89.7\\nProtoQA 67.3 84.6 94.2 \\nMC-TACO 20 53 52 75.8\\nTable 30: Evaluation results (accuracy) of large language models on commonsense QA datasets.\\nSource: Bian et al. [373]\\nWhy does LLM respond in constant time, even for polynomial or exponential prob-\\nlems?\\nFor Kambhampati [378] and Kambhampati et al. [379] the answer lies in their nature as re-\\ntrievers, not true reasoners. LLMs can mimic planning by combining retrieved information but\\nlack true instance-level understanding required for accurate reasoning. LLMs excel at tasks\\ninvolving pattern recognition within a distribution87, but struggle with instance-specific88 tasks\\nlike formal planning or sequencing actions toward a goal. For example, even when fine-tuned\\nfor specific tasks like multiplication, LLMs falter with more complex variations, showing their\\nlimitations. Ultimately, while LLMs can replicate certain logical patterns, their planning abili-\\nties are superficial, relying heavily on memorized logic rather than true reasoning. Additionally,\\ninvolving humans to iteratively prompt LLMs introduces the risk of the Clever Hans effect89,\\nwhere the models responses are inadvertently influenced by subtle cues from the prompter,\\n87The distributional or style properties in various fields can be understood as the recurring patterns and\\ncharacteristics that define the general appearance or structure of an object or medium. In the realm of art,\\nthese properties might include brushstroke patterns, color palettes, and compositional rules that collectively\\ndefine an artists body of work or the broader characteristics of an art movement. These stylistic elements\\nenable the recognition of an artists work even when individual pieces differ in content.\\nSimilarly, in language, distributional properties pertain to the recurring patterns in word choice, sentence\\nstructure, and other linguistic elements that define a particular writing style or genre. These patterns help in\\nidentifying the genre or author of a text based on its overall style rather than its specific content.\\nIn computer vision, distributional properties refer to the consistent textures, lighting conditions, and geometric\\npatterns across images of a specific type of object or scene. For example, the overall shape of cars or the texture\\nof fur in animals represents such properties. These features allow models to recognize new instances of objects\\nthat share these common characteristics, even if the specific details differ from those previously encountered.\\n88Instance properties refer to the specific and unique features that distinguish one example from another\\nwithin a given category. In art, these properties are reflected in the distinct brushstrokes, intricate details, and\\nparticular color choices used in an individual painting. These elements contribute to the identity of a specific\\nartwork, differentiating it from others, even within the same artists portfolio.\\nIn language, instance properties manifest as the precise selection of words, the unique arrangement of sen-\\ntences, and the specific use of punctuation in a particular sentence or paragraph. These elements define the\\nuniqueness of a text, capturing the nuances of expression that distinguish one piece of writing from another,\\neven if they share the same overall style or genre.\\nIn the domain of computer vision, instance properties are found in the detailed characteristics of a specific\\nobject in an image, such as the color, make, and model of a particular car, as well as any unique markings it\\nmay have. These properties enable the recognition of a particular instance of an object, allowing for fine-grained\\nclassification and identification within a broader category.\\n89Clever Hans was a horse claimed to have performed arithmetic and other intellectual tasks. After a formal\\ninvestigation in 1907, psychologist Oskar Pfungst demonstrated that the horse was not actually performing\\n97'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 97, 'page_label': '98'}, page_content='rather than genuine understanding. While LLMs cant plan independently, they can assist in\\nplanning when combined with external solvers and verifiers in an LLM-Modulo framework. In\\nthis setup, LLMs support planning by suggesting plans, guessing domain models, elaborating\\non problem specifications, and translating formats, but they still rely on external systems for\\nverification and sequencing.\\nEven though we have seen surprising abilities of LLMs, Qian et al. [207] have shown ad-\\nditional limitations on certain basic symbolic manipulation tasks, such as copy, reverse and\\naddition, particularly when dealing with repeating symbols 90 and OOD 91 data. To address\\nthese limitations, Qian et al. [207] have proposed a series of methods to improve the perfor-\\nmance of LLMs on these tasks, such as positional markers, fine-grained computation steps,\\nand combining LMs with callable programs for basic operations. Positional markers 92 and\\nfine-grained computation steps 93 provide some improvement with repeating symbols but not\\nwith OOD. It clearly indicates the limitation of Transformers and pre-trained language models\\nin induction. Combining LMs with callable programs 94 for basic operations shows potential\\nbut still relies on the LMs ability to locate tokens accurately. The LM with tutor method 95\\ndemonstrates each task step, significantly improving accuracy and handling OOD scenarios,\\neffectively achieving 100% accuracy on all tasks.\\nWith the release of new models like Open AI o1 and o396 and Claude 3.5, the field is moving\\ntowards more powerful models that can potentially address some of the previous limitations.\\nWang et al. [387] explores the planning capabilities of OpenAIs o1 models, focusing on their\\nperformance across diverse tasks requiring feasibility, optimality, and generalizability. The o1-\\npreview model demonstrates improvements in generating feasible plans compared to earlier\\nlanguage models like GPT-4. However, the study identifies key challenges, such as the models\\nlimitations in following domain-specific constraints, which often misinterpret physical or logical\\nconstraints. Also, the model struggles to generate coherent plans. Although the individual steps\\nmay be valid, the model sometimes fails to sequence them into a coherent, goal-oriented plan.\\nMoreover, the models ability to interpret initial and goal states leads to errors, particularly in\\ntasks requiring multi-step reasoning. Regarding the optimality of plans, the model often fails\\nto generate optimal plans, instead producing suboptimal or inefficient solutions with duplicate\\nthese mental tasks, but was watching the reactions of his trainer. He discovered this artifact in the research\\nmethodology, wherein the horse was responding directly to involuntary cues in the body language of the human\\ntrainer, who was entirely unaware that he was providing such cues.\\n90Copy example with repeating symbols input: . . .989894 . . .answer: . . .9894 . . .\\n91Out-of-distribution refers to prompting the model to execute an operation on numbers with more digits\\nwith respect to numbers used for training. It demonstrates the ability to generalize on unseen data.\\n92LMs have implicit positional markers embedded in the architecture. Most Transformer-based LMs encode\\nthe positional information into positional vectors and add each of them to the corresponding word vector.\\nExplicit positional markers are added into input strings: input: . . .222 . . .output: . . .A2B2C2 . . .. Essen-\\ntially, adding explicit positional markers breaks the repeating numbers into a non-repeating input sequence.\\n93For example, in k-digit addition, the model is allowed to break it down into k simple 1-digit addition, and\\nthe model is allowed to generate k intermediate addition results to get the final answer.\\n94A callable function add(1,5) can be invoked and return the result in text: carry C: 0, result 6\\n95A tutor shows every single step visually and sometimes calls an already\\nlearned sub-module to complete a task. Instead of providing a training example:\\ncopy: 1 1 1 2 2 2 result: 1 1 1 2 2 2 , the tutor explicitly shows the model how to copy the\\ninput as follows: rmov, end=F, cpy, rmov, end=F, cpy, ..., rmov, end=T. where rmov is a function\\nthat moves the tape head to the right, cpy is a function that copies the current symbol, and end=F indicates\\nthat the end of the tape is not reached. This setup can be likened to a multi-tape Turing machine, where\\nstate transitions occur between the positions of tape heads, accompanied by read and write operations. The\\nTransformer is trained to model these state transitions, effectively simulating the programming of a Turing\\nmachine.\\n96There are still not enough details about the o3 model to provide a comprehensive analysis.\\n98'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 98, 'page_label': '99'}, page_content='or unnecessary steps. The model lacks mechanisms to incorporate domain-specific heuristics or\\noptimization techniques, resulting in suboptimal decision-making. Finally, the models gener-\\nalizability remains limited. It struggles with tasks that require reasoning over unseen scenarios\\nand symbolic reasoning, where action semantics diverge from natural language.\\nIn the following paragraphs, we will discuss the general framework of prompt-based planning,\\nplan generation, plan execution, and plan evaluation. After that we will present the most\\ncommon approaches to planning and their limitations.\\n4.4.2 Prompt and code based planning\\nPrompt-based planning has been proposed to break down complex tasks into simpler sub-tasks,\\nand generate a plan of actions to accomplish the task. The general framework of prompt-based\\nplanning is shown in Figure 45.\\nFigure 45: The general framework of prompt-based planning. Source: Zhao et al. [364]\\nIn this paradigm, there are three main components: the planner, the executor, and the\\nenvironment97. The first component is the planner, which generates a plan of action to solve\\nthe task. The plan can be generated in various forms, e.g., natural language, symbolic, or\\nprogrammatic [164, 244], that we will discuss in the next section 4.4.2. The memory mechanism\\ncan enhance the task planner, which stores intermediate results and reuses them in the future.\\nThe plan executor is responsible for executing the plan generated by the planner. It can be\\nimplemented as a separate LLM for textual tasks or as a program executor for programmatic\\ntasks [338, 164].\\n97Its similar to Reinforcement Learning, where the planner is the agent, the executor is the policy, and the\\nenvironment is the world, but the difference is that in RL they are typically interleaved in the agent, while in\\nprompt-based planning they are separated\\n99'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 99, 'page_label': '100'}, page_content='The environment is the world where the task is executed, which can be set up as the LLM\\nitself or an external system, e.g., a simulator or a virtual world like Minecraft [359, 337]. The\\nenvironment provides feedback to the task planner about the result of the actions, which can\\nbe used to update the plan, either in the form of natural language or from other multimodal\\nsignals [322, 301]\\n4.4.3 Plan generation\\nFor solving complex tasks, the planner needs to generate a long-term and multi-step plan, which\\nrequires the planner to be able to reason over long-term dependencies and develop a coherent\\nand consistent plan. First, it needs to understand the task and break it down into sub-tasks,\\nthen generate a plan that can accomplish the task by executing the sub-tasks in a proper order.\\nThe plan should be generated in an interpretable and executable way by the executor, which\\nacts according to the plan and interacts with the environment to accomplish the task. The\\nplanner can further incorporate the feedback from the environment to update and refine the\\nplan and achieve better performance.\\nThe most common form of plan generation is natural language, where the planner generates\\na sequence of natural language instructions that describe the plan. In this approach, LLMs\\nare prompted to generate a sequence of instructions that describe the plan, which the executor\\ncan execute to accomplish the complex task. For example, Plan-and-Solve [338] adds explicit\\ninstructions to the input of the LLM, which guides the model to generate a plan for solving the\\ntask (i.e., devise a plan) in a zero-shot setting, while Self-planning [377] and DECOMP [175]\\ngenerate the plan in a few-shot setting by providing a few examples to guide LLM through\\nICL. Other approaches consider incorporating extra tools or models when planning, such as\\nToolFormer [319] and HuggingGpt [321]. ToolFormer is a model trained to decide which APIs\\nto call when to call them, what arguments to pass, and how to best incorporate the results into\\nfuture token prediction. This is done in a self-supervised way, requiring nothing more than a\\nhandful of demonstrations for each API. It incorporates a range of tools, including a calculator,\\na Q&A system, two different search engines, a translation system, and a calendar. HuggingGpt\\nis an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models\\nin machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, it uses\\nChatGPT to conduct task planning when receiving a user request, select models according to\\ntheir function descriptions available in Hugging Face, execute each subtask with the selected\\nAI model, and summarize the response according to the execution results.\\nAlthough text-based plan approaches sound intuitive, they have limitations since the gen-\\nerated plans may lead to incorrect results due to the ambiguity of natural language, even when\\nthe plan is sound. To address this issue, code-based plan generation has been proposed. In\\nthis method, the planner generates a program the executor can execute to accomplish the task.\\nCompared to text-based plans, programmatic plans are more verifiable and less ambiguous, and\\nthey can directly be executed by interpreters or compilers (e.g., Python or PDDL 98) to accom-\\nplish the task. This approach involves prompting LLMs to first generate a program for solving\\nthe task, followed by using a deterministic solver to execute it. For instance, Faithful CoT [302]\\nand PAL [164] divide a reasoning task into two stages: in the first stage, the LLM generates\\na plan based on the query, and in the second stage, a deterministic solver executes the plan\\nto produce the final answer. Additionally, similar code-based approaches can be employed in\\nembedded agents, as demonstrated by methods like PROGPROMPT [212] and LLM+P [297].\\nIn the following paragraphs, we will elaborate on some notable approaches to natural lan-\\n98Planning Domain Definition Language defines the universal aspects of a problem. Essentially, these\\naspects do not change regardless of the specific situation were trying to solve. In PDDL, this is mostly the\\nobject types, predicates and actions that can exist within the model.\\n100'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 100, 'page_label': '101'}, page_content='guage and programmatic plan generation.\\nPlan-and-Solve (PS) prompting is a text-based plan generation approach that consists of\\ntwo components: devising a plan and carrying out the subtasks. The process includes:\\n1. Step 1: Prompting for Reasoning Generation . To meet the criteria for effective\\nproblem-solving, templates guide LLMs in devising and completing a plan with atten-\\ntion to calculations and intermediate results. For example: Lets first understand the\\nproblem, extract relevant variables, devise a plan, and solve the problem step by step.\\n2. Step 2: Prompting for Answer Extraction. Similar to Zero-shot-CoT, another\\nprompt extracts the final numerical answer from the reasoning text.\\nA comparison of prompting strategies is shown in Figure 46. The PS+ variant of Plan-and-Solve\\nis an extension that adds detailed instructions to improve reasoning quality.\\nFigure 46: Example inputs and outputs of GPT-3 with (a) Zero-shot-CoT prompting, (b) Plan-and-\\nSolve (PS) prompting, and (c) answer extraction prompting. While Zero-shot-CoT encourages LLMs\\nto generate multi-step reasoning with Lets think step by step, it may still generate wrong reasoning\\nsteps when the problem is complex. Unlike Zero-shot-CoT, PS prompting first asks LLMs to devise\\na plan to solve the problem by generating a step-by-step plan and carrying out the plan to find the\\nanswer. Source: Wang et al. [338]\\nMethod MultiArith GSM8k AddSub AQUA SingleEq SVAMP\\nZero-shot-CoT 83.8 56.4 85.3 38.9 88.1 69.9\\nPoT 92.2 57.0 85.1 43.9 91.7 70.8\\nPS (ours) 87.2 58.2 88.1 42.5 89.2 72.0\\nPS+ (ours) 91.8 59.3 92.2 46.0 94.7 75.7\\nTable 31: Accuracy comparison on math reasoning datasets. Source: Wang et al. [338]\\nCompared to Zero-shot-CoT, which suffers from pitfalls like calculation and missing-step\\nerrors, PS+ Prompting has shown to be more effective in addressing these issues [338]. The\\n101'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 101, 'page_label': '102'}, page_content='Method CSQA StrategyQA\\nFew-Shot-CoT (Manual) 78.3 71.2\\nZero-shot-CoT 65.2 63.8\\nZero-shot-PS+ 71.9 65.4\\nTable 32: Accuracy on commonsense reasoning datasets. Source: Wang et al. [338]\\nMethod Last Letter Coin Flip\\nFew-Shot-CoT (Manual) 70.6 100.0\\nZero-shot-CoT 64.8 96.8\\nZero-shot-PS+ 75.2 99.6\\nTable 33: Accuracy on symbolic reasoning datasets. Source: Wang et al. [338]\\nexperiments with GPT-3 show that PS+ consistently outperforms Zero-shot-CoT and is com-\\nparable to 8-shot CoT prompting on math reasoning problems. Self-consistency (SC) 99[227]\\nimproves performance by generating multiple reasoning paths and selecting the final answer by\\nmajority voting. PS+ with SC outperforms PS+ without SC and Zero-shot-CoT with SC.\\nLeast-to-Most Prompting is a text-based prompting strategy that aims to improve the\\nperformance of LLMs on complex reasoning tasks proposed by Zhou et al. [244]. Least-to-most\\nprompting consists of two stages:\\n1. Decomposition: The prompt contains examples demonstrating problem decomposition,\\nfollowed by the specific question to be decomposed.\\n2. Sub-problem Solving : The prompt consists of examples demonstrating sub-problem\\nsolving, previously answered subquestions and solutions, and the next question to be\\nanswered.\\nFigure 47 illustrates this approach.\\nLeast-to-most prompting significantly outperforms Chain-of-Thought prompting on the last-\\nletter-concatenation task 100 [230], especially on longer lists 101. Table 34 shows the accuracy\\ncomparison.\\nMethod Length 4 Length 6 Length 8 Length 10 Length 12\\nStandard Prompting 0.0 0.0 0.0 0.0 0.0\\nChain-of-Thought 84.2 69.2 50.2 39.8 31.8\\nLeast-to-Most 94.0 88.4 83.0 76.4 74.0\\nTable 34: Accuracies of different prompting methods on the last-letter-concatenation task. Source:\\nZhou et al. [244]\\n99It reduces randomness in LLMs output by generating N reasoning results and determining the final answer\\nby majority voting\\n100In this task, each input is a list of words, and the corresponding output is the concatenation of the last\\nletters of the words in the list. For example, thinking, machine outputs ge, since the last letter of thinking\\nis g and the last letter of machine is e.\\n101When the testing lists are much longer than the lists in the prompt exemplars.\\n102'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 102, 'page_label': '103'}, page_content='Figure 47: Least-to-most prompting teaches language models how to solve a complex problem by\\ndecomposing it to a series of simpler subproblems. It consists of two sequential stages: (1) decompo-\\nsition and (2) sequentially solving subproblems. The answer to the second subproblem is built on the\\nanswer to the first subproblem. The demonstration examples for each stages prompt are omitted in\\nthis illustration. Source: Zhou et al. [244]\\nLeast-to-most prompting also achieves 99.7% accuracy on the SCAN102 compositional gener-\\nalization benchmark with only 14 exemplars, compared to 16% with Chain-of-Thought prompt-\\ning. Table 35 shows the accuracy comparison. Least-to-most improves performance on GSM8k\\nand DROP benchmarks, particularly for problems requiring multiple solving steps. Table 36\\nshows the accuracy comparison.\\nMethod Code-davinci-002 Text-davinci-002 Code-davinci-001\\nStandard Prompting 16.7 6.0 0.4\\nChain-of-Thought 16.2 0.0 0.0\\nLeast-to-Most 99.7 76.0 60.7\\nTable 35: Accuracies of different prompting methods on the SCAN benchmark. Source: Zhou et al.\\n[244]\\nLeast-to-most prompting effectively generalizes to more complex problems than those seen\\nin the prompts. This approach can be combined with other prompting techniques, such as\\nchain-of-thought and self-consistency, to enhance performance further.\\n102it is probably the most popular benchmark for evaluating compositional generalization. It requires mapping\\nnatural language commands to action sequences [47].\\n103'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 103, 'page_label': '104'}, page_content='Method Non-football (DROP) Football (DROP) GSM8k\\nZero-Shot 43.86 51.77 16.38\\nStandard Prompting 58.78 62.73 17.06\\nChain-of-Thought 74.77 59.56 60.87\\nLeast-to-Most 82.45 73.42 62.39\\nTable 36: Accuracies of different prompting methods on GSM8k and DROP benchmarks. Source:\\nZhou et al. [244]\\nDECOMP is a text-based prompting strategy that decomposes complex tasks into simpler\\nsubtasks and generates a plan to solve the task, similar to Least-to-Most prompting. The\\ncore idea of Decomposed Prompting involves dividing a complex task into multiple simpler\\nsubtasks. Each subtask is addressed separately using LLMs, and their results are then combined\\nto produce the final outcome. Tasks are decomposed based on their inherent structure. For\\ninstance, a question-answering task might be split into subtasks involving information retrieval,\\ncomprehension, and synthesis. The model can process each step more effectively by focusing\\non these individual components.\\nFigure 48: The DECOMP framework. Source: Khot et al. [175]\\nIn DECOMP, the core is a decomposer LLM that tries to solve a complex task by generating\\na prompting program P. Each step of P directs a simpler sub-query to a function in an\\nauxiliary set of sub-task functions F available to the system. Given a query Q whose answer\\nis A, the program P is a sequence of the form (( f1, Q1, A1), . . . ,(fk, Qk, Ak)) where Ak is the\\nfinal answer predicted by P and Qi is a sub-query directed to the sub-task function fi  F. P\\nis executed by a high-level imperative controller, which passes the inputs and outputs between\\nthe decomposer and sub-task handler until a stopping condition in P is met and the final\\noutput is obtained. Using a software engineering analogy, the decomposer defines the top-\\nlevel program for the complex task using interfaces to more straightforward sub-task functions.\\nThe sub-task handlers serve as modular, debuggable, and upgradable implementations of these\\nsimpler functions, akin to a software library. Specialized prompts are designed for each subtask,\\nguiding the LLM to focus on specific aspects of the problem. This involves crafting precise and\\ncontextually relevant prompts that direct the models attention to the desired task component.\\nExtensive experiments demonstrate the efficacy of Decomposed Prompting. Key bench-\\nmarks and datasets were utilized to evaluate the performance gains achieved through this\\napproach (Figure 49).\\n104'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 104, 'page_label': '105'}, page_content='Figure 49: On the left: Exact Match results on the k-th letter concatenation task (k=3) using space\\nas a delimiter with different numbers of words in the input. On the right: Exact Match results on\\nreversing sequences. Incorporating CoT in DECOMP greatly increases the ability of the model to\\ngeneralize to new sequence lengths Source: Khot et al. [175]\\nProgram-Aided Language Models (PALMs) are a new class of code-based language\\nmodels that use the LLM to read natural language problems and generate programs as the\\nintermediate reasoning steps. Still, they offload the solution step to a runtime like a Python\\ninterpreter. These models are designed to perform complex reasoning tasks that require struc-\\ntured knowledge and logical reasoning, such as mathematical word problems, symbolic reason-\\ning, and program synthesis. Despite LLMs seeming to be adept at CoT prompting, LLMs often\\nmake mathematical and logical errors, even though the problem is decomposed correctly into\\nintermediate reasoning steps [164].\\nPaL is a model that belongs to this new class of models. It generates programs that\\ncan be executed by a Python interpreter and uses the programs output as the final answer.\\nPaL has been shown to outperform much larger LLMs using CoT (e.g., PaLM-540B) on\\nFigure 50: Example prompt for the mathematical reasoning tasks from the GSM8k benchmark.\\nSource: Gao et al. [164]\\nmathematical word problems and symbolic reasoning tasks [164] as shown in Table 37. PaL\\nModel GSM8k GSM-\\nHARD\\nSVAMP ASDIV SINGLEEQ SINGLEOP ADDSUB MULTIARITH\\nDirectCodex 19.7 5.0 69.9 74.0 86.8 93.1 90.9 44.0\\nCoTUL2-20B 4.1 - 12.6 16.9 - - 18.2 10.7\\nCoTLAMDA-137B17.1 - 39.9 49.0 - - 52.9 51.8\\nCoTCodex 65.6 23.1 74.8 76.9 89.1 91.9 86.0 95.9\\nCoTPaLM-540B 56.9 - 79.0 73.9 92.3 94.1 91.9 94.7\\nCoTMinerva\\n540B\\n58.8 - 79.4 79.6 96.1 94.6 92.5 99.2\\nPaL 72.0 61.2 79.4 79.6 96.1 94.6 92.5 99.2\\nTable 37: Problem solve rate (%) on mathematical reasoning datasets. The highest number on each\\ntask is in bold. The results for DIRECT and PaLM-540B are from Wei et al. [230], the results for\\nLAMDA and UL2 are from Wang et al. [227], the results for Minerva are from Lewkowycz et al. [182].\\nPAL ran on each benchmark 3 times and reported the average. Source: Gao et al. [164].\\nis even more effective with respect to other LLMs when tested on the GSM-HARD dataset\\n105'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 105, 'page_label': '106'}, page_content='Figure 51: An example for a PaL prompt in the Colored Objectstask. Source: Gao et al. [164]\\n a version of GSM8k contains larger numbers (i.e., up to 7 digits). Other interesting results\\ncome from symbolic reasoning tasks from BIG-Bench Hard: the Colored Objects103 and the\\nPenguins104 tasks as shown in Table 38. Gao et al. [164] have shown that PaL is not limited\\nModel COLORED\\nOBJECT\\nPENGUINS DATE REPEAT\\nCOPY\\nOBJECT\\nCOUNT-\\nING\\nDirectCodex 75.7 71.1 49.9 81.3 37.6\\nCoTLAMDA-137B - - 26.8 - -\\nCoTPaLM-540B - 65.1 65.3 - -\\nCoTCodex 86.3 79.2 64.8 68.8 73.0\\nPALCodex 95.1 93.3 76.2 90.6 96.7\\nTable 38: Solve rate on three symbolic reasoning datasets and two algorithmic datasets. In all\\ndatasets, PAL achieves a much higher accuracy than chain-of-thought. Results with closed models\\nLAMDA-137B and PaLM-540B are included if available to the public Wei et al. [230] and Suzgun\\net al. [218]. Source: Gao et al. [164].\\nto LMs of code. Still, it can work with LMs that were mainly trained for natural language\\nif they have a sufficiently high coding ability. Benefits come from the synergy between the\\nPython prompt and the interpreter. PaL avoids inaccuracy on arithmetic tasks and incorrect\\nreasoning by offloading the calculations and some of the reasoning to a Python interpreter,\\nwhich is correct by design, giving the right program.\\nSELF-PLANNING is a code-generation strategy using a planning-based approach. In this\\ncase, the planning is executed before the actual code generation, and the LLM itself generates\\nthe plan. In the first stage, the planning phase, the LLM is prompted to abstract and decompose\\nthe intent to obtain a plan for guiding code generation using few-shot prompting. The prompt\\nC is designed as k examples105 concatenated together\\nC\\n\\n= xe\\n1  ye\\n1\\nn\\nxe\\n2  ye\\n2\\nn\\n. . .\\nn\\nxe\\nk  ye\\nk (30)\\nwhere each example xe\\ni  ye\\ni  consists of the example intent xe\\ni and its associated plan ye\\ni to\\ndemonstrate the planning task. During inference, the test-time intent x will be concatenated\\n103It requires answering questions about coloured objects on a surface\\n104It requires to answer a question about the attributes of the penguins on a table (e.g., how many penguins\\nare less than 8 years old?). This task describes dynamics as well since the penguins can be added or removed.\\n105Note that k is a fairly low number.\\n106'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 106, 'page_label': '107'}, page_content='after the prompt, and C fx will be fed into the LLM M, which will attempt to do planning for\\nthe test-time intent. The output of the LLM is the test-time plan y for the test-time intent x.\\nFigure 52: Self-planning generation phases (i.e., planning and implementation phases). Source:\\nJiang et al. [377]\\nIn the second stage, the implementation phase, the plan generated in the first stage guides\\nthe code generation. The plan y is concatenated with intent x and fed into the LLM M to\\ngenerate the code z. The above two stages can be formalized as\\nP(z|x, C) =\\nX\\ny\\nP(z|y, x, C)  P(y|x, C),  P(z|y, x, C)  P(y|x, C) (31)\\nwhere y is any of all possible plans, and y denotes one of the plans generated by the LLM in\\nthe first stage. Jiang et al. [377] further simplifies the above equation by adopting the plan\\nwith the highest probability as y. Thus, the final equation becomes\\nP(z|x, C)\\n\\n= P(z|y, x, C)| {z }\\nImplementation phase\\n P(y|x, C)| {z }\\nPlanning phase\\n(32)\\nBenchmarking against various LLMs pre-trained on code, such as CodeGeex (13B) [366],\\nCodeGen-Mono (16.1B) [203], and PaLM Coder (560B) [156], reveals that SELF-PLANNING\\nsignificantly enhances performance across public code generation datasets. This improvement\\nis observed when comparing SELF-PLANNING with other prompting methods, including Di-\\nrect, Code Chain-of-Thought (CoT), and Few-shot approaches. Comparing the effectiveness\\nof SELF-PLANNING relative to model size, it is evident that SELF-PLANNING impact is\\nmore pronounced with larger models. As the model size reaches 13B, LLMs performance in\\ncode generation tasks begins to exhibit emerging ability, but self-planning ability is still rela-\\ntively low. Experiments show that incorporating code training data and RLHF can enhance\\nthe models self-planning capabilities and increase its size.\\n107'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 107, 'page_label': '108'}, page_content='4.4.4 Feedback and plan refinement\\nFeedback is an essential component in the plan-based reasoning paradigm, as it allows the\\nplanner to refine the plan based on the feedback from the environment following the planning-\\nexecution-refinement loop. Feedback sources are categorized into internal and external, based\\non their origin relative to the LLM-based planner.\\nInternal Feedback: Here, the LLM acts as a feedback source. One common method is to\\nassess the effectiveness of generated plans through structured prompts. For instance, Hao et al.\\n[275] evaluates the success potential of various plans by estimating their likelihood of achiev-\\ning the desired outcome. At the same time, Tree of Thoughts employs a comparative voting\\nmechanism among different plans. Additionally, LLMs can refine their feedback using interme-\\ndiate outcomes from plan execution, such as in Reflexion, where sparse outcomes like success\\nor failure are translated into detailed, actionable feedback. This feedback is then preserved in\\nthe LLMs long-term memory to enhance future planning.\\nExternal Feedback: Beyond the LLM, external tools and environments also contribute\\nto feedback. Tools like code interpreters in programming tasks offer immediate error feed-\\nback, while models like stable diffusion in multimodal tasks provide visual feedback. Virtual\\nenvironments like Minecraft offer a rich, interactive backdrop for feedback through immersive\\nexperiences. Moreover, projects like Generative Agents investigate the dynamics of multi-agent\\nsystems in simulated settings, where agents derive feedback from both environmental interac-\\ntions and inter-agent communication.\\nRegarding the plan refinement, the three main approaches are summarized in the next\\nparagraphs.\\nReasoning. When feedback data from the environment is not directly usable for plan refine-\\nment by LLMs, some approaches incorporate an explicit reasoning process to extract essential\\ninformation from the feedback [260, 236]. React prompts LLMs with demonstrations to gener-\\nate reasoning traces over feedback. Human intelligence uniquely integrates task-oriented actions\\nwith verbal reasoning or inner speech,  significantly contributing to cognitive functions like\\nself-regulation and working memory management. For example, in the kitchen, a person might\\nverbally strategize their next steps in a recipe (Now that everything is cut, I should heat up\\nthe pot of water), adapt to missing ingredients (I dont have salt, so let me use soy sauce\\nand pepper instead), or seek additional information online to enhance their cooking process.\\nThis ability to blend action with analytical thinking enables humans to swiftly learn new tasks\\nand make robust decisions, even in novel or uncertain situations. React has been widely used\\nin autonomous agent projects, such as AutoGPT, which can automatically reason over the\\nobserved feedback to revise the initial plan for solving various user requests. However, these\\napproaches typically fix the order of reasoning and planning.\\nChatCoT supports flexible switching between the two processes, unifying the tool-augmented\\nCoT reasoning framework into a multi-turn conversation between the LLM-based task planner\\nand the tool-based environment. At each turn, the LLM can freely interact with tools when\\nneeded; otherwise, it performs the reasoning by itself.\\nBacktracking. Initial planning techniques primarily focused on progressing with forward\\nactions within an existing plan, often resulting in locally optimal strategies based on short-\\nterm assessments. To address this limitation, the Tree of Thoughts approach [359] introduces\\nthe capability for backtracking through search techniques such as breadth-first and depth-first\\nsearches, enabling more comprehensive global planning strategies. This method iteratively\\nrefines the plan by returning to previous decision points and exploring alternative paths as\\ndepicted in Figure 55.\\n108'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 108, 'page_label': '109'}, page_content='Figure 53: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,\\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA [60] question. Source:\\nChen et al. [260]\\nFigure 54: ChatCoT strategy illustrated to solve a mathematical problem. The conversational knowl-\\nedge memory is initialized to provide tools, task and reasoning format knowledge. Then, the tool-\\naugmented reasoning step is iterated multiple times to perform step-by-step reasoning until the answer\\nis obtained. Source: Chen et al. [260]\\nIn developing such a method, Yao et al. [359] revisits foundational artificial intelligence and\\ncognitive science principles, framing problem-solving as navigating a tree-like combinatorial\\nspace. Within this framework, Yao et al. [359] introduced three novel challenges aimed at\\npushing the boundaries of state-of-the-art models such as GPT-4: the Game of 24 106, Creative\\n106The Game of 24 is a mathematical challenge where the objective is to manipulate four numbers using basic\\narithmetic operations +  to achieve a result of 24. For instance, from the numbers 4, 9, 10, 13 , one\\npossible solution could be (10  4)  (13  9) = 24 .\\n109'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 109, 'page_label': '110'}, page_content='Figure 55: Diagram demonstrating various problem-solving methodologies using LLMs. Each rect-\\nangle represents a distinct thought, forming an integral step towards resolving a problem. Source: Yao\\net al. [359]\\nWiring107, and Crosswords 108. These tasks necessitate a blend of deductive, mathematical,\\ncommonsense, and lexical reasoning skills, along with sophisticated systematic planning or\\nsearching capabilities. The Tree of Thoughts model demonstrates its versatility and efficacy\\nacross these diverse tasks by supporting varied levels of thought processes, multiple thought\\ngeneration and assessment methods, and adaptable search algorithms tailored to the specifics\\nof each challenge.\\nFurthermore, some studies [168, 344] utilize feedback signals to revise the entire plan since\\nthe initial plan generated by the LLM is often imperfect. For example, DEPS 109 [344] selects a\\nbetter plan according to feedback signals, while TIP 110 [301] adds feedback signals to prompts\\nfor the LLM-based planner to revise each step in the initial plan.\\nDEPS has been tested on Minecraft, an open world with abundant object types and complex\\ndependencies and relations. As a result, ground-truth plans typically involve a long sequence of\\nsub-goals with strict dependencies (e.g., obtaining a diamond requires 13 sub-goals with strict\\ndependencies). Another challenge in an open-ended world is the feasibility of the produced\\nplans. For example, the fastest way to craft a bed in Minecraft is to slaughter a sheep to obtain\\nwool, which can be used to craft beds or collect beds from a village. However, since no sheep\\nor village is reachable by the agent within 3 minutes of gameplay, to craft a bed efficiently, the\\nagent should choose to slaughter a spider and use materials (e.g., string) it drops to craft wool,\\nand then a bed. The key to solving the first challenge is effectively adjusting the generated plan\\nupon a failure. When the controller fails to complete a sub-goal, a descriptor will summarize the\\ncurrent situation as text and send it back to the LLM-based planner. Then, prompt the LLM\\n107In the Creative Wiring task, participants are given four random sentences and must craft a coherent narrative\\nconsisting of four paragraphs, each concluding with one of the provided sentences. This task tests creative\\nsynthesis and advanced planning.\\n108A 5  5 mini crosswords task is a harder search problem involving natural language. The goal is not to\\nsolve the problem since it can be solved with specialized NLP pipelines but to explore the limit of LM as a\\ngeneral-purpose solver.\\n109Describe, Explain, Plan, Select\\n110Text-Image Prompting\\n110'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 110, 'page_label': '111'}, page_content='Figure 56: Overview of the DEPS interactive plannet architecture. Source: Wang et al. [344]\\nas an explainer to locate the errors in the previous plan. Finally, a planner will refine the plan\\nusing the descriptor and explainer information. To improve the feasibility of generated plans\\nconditioned on the current state, which is the second identified challenge, Wang et al. [344] use\\na learned goal-selector to choose the most accessible sub-task based on the proximity to each\\ncandidate sub-goal. Developing multi-task agents that can accomplish a vast and diverse suite\\nof tasks in complex domains has been considered a key milestone towards generally capable\\nartificial intelligence.\\nMemorization Long-term memory is a crucial component in the planning process. It allows\\nmodels to store and retrieve information from past experiences and the short-term memory\\ncapabilities provided by in-context learning (ICL) in large language models (LLMs). Reflex-\\nion [322] introduces an innovative framework that enhances language agents through linguistic\\nfeedback rather than weight updates. Reflexion agents reflect verbally on task feedback, main-\\ntaining reflective text in an episodic memory buffer to improve decision-making in subsequent\\ntrials. This process mirrors how humans iteratively learn complex tasks by reflecting on previ-\\nous failures to develop improved strategies for future attempts.\\nReflexion can incorporate various types (scalar values or free-form language) and sources\\n(external or internally simulated) of feedback signals, significantly improving performance over\\na baseline agent across diverse tasks such as sequential decision-making, coding, and language\\nreasoning.\\nThe Reflexion framework consists of four main components: the Actor, the Evaluator, the\\nSelf-Reflection model, and the memory. The Actor, built upon an LLM, is specifically prompted\\nto generate necessary text and actions based on state observations. The Evaluator assesses the\\nquality of the Actors outputs by computing a reward score that reflects performance within the\\ngiven task context. The Self-Reflection model, also instantiated as an LLM, generates verbal\\nself-reflections to provide valuable feedback for future trials. Core components of the Reflexion\\n111'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 111, 'page_label': '112'}, page_content='Figure 57: Reflexion works on decision-making, programming, and reasoning tasks. Source: Shinn\\net al. [322]\\nFigure 58: (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm. Source: Shinn et al.\\n[322]\\nprocess are the notion of short-term and long-term memory. At inference time, the Actor\\nconditions its decisions on short and long-term memory, similar to how humans remember fine-\\ngrain recent details while also recalling distilled meaningful experiences from long-term memory.\\nIn the RL setup, the trajectory history serves as the short-term memory, while outputs from\\nthe Self-Reflection model are stored in long-term memory. These two memory components\\nwork together to provide specific context. Still, they are also influenced by lessons learned over\\nseveral trials, a key advantage of Reflexion agents over other LLM action choice works. Given a\\nsparse reward signal, such as a binary success status (success/fail), the current trajectory, and\\nits persistent memory mem, the self-reflection model generates nuanced and specific feedback.\\nThis feedback, which is more informative than scalar rewards, is then stored in the agents\\nmemory mem. For example, in a multi-step decision-making task, if the agent receives a\\nfailure signal, it can infer that a specific action ai led to subsequent incorrect actions ai+1 and\\n112'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 112, 'page_label': '113'}, page_content='ai+2. The agent can then verbally state that it should have taken a different action, ai, which\\nwould have resulted in correct actionsai+1 and ai+2, and store this experience in its memory. In\\nsubsequent trials, the agent can leverage past experiences to adapt its decision-making approach\\nat time t by choosing action ai. This iterative process of trial and error, self-reflection, and\\npersisting memory enables the agent to rapidly improve its decision-making ability in various\\nenvironments by utilizing informative feedback signals. For instance, Reflexion achieves a 91%\\npass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art\\nGPT-4, which achieves 80%.\\nFigure 59: Generative agent architecture. Agents perceive their environment, and all perceptions are\\nsaved in a comprehensive record of the agents experiences called the memory stream. Based on their\\nperceptions, the architecture retrieves relevant memories and uses those retrieved actions to determine\\nan action. These retrieved memories are also used to form longer-term plans and create higher-level\\nreflections, both of which are entered into the memory stream for future use. Source: Park et al. [310]\\nGenerative agents [310] are another example of models that leverage memory to improve\\nplanning where a sandbox environment is populated with 25 agents that focus on the ability to\\ncreate a small, interactive society of agents inspired by games such as The Sims. In particular,\\nthe generative agents leverage a memory stream mechanism for action planning and reflection,\\nsimulating human-like decision behaviour. The memory stream is a long-term memory module\\nthat records a comprehensive list of the agents natural language experiences. The reflection\\nand the planning components synthesize memories into higher-level inferences over time, en-\\nabling the agent to draw conclusions about itself and others, and recursively translates those\\nconclusions and the current environment into high-level action plans, as shown in Figure 59.\\nOther studies [324, 337] have also explored using memory called skill library mechanism to\\nstore successful plans, which can be reused and synthesized as complex plans for new tasks.\\nAdaPlanner [324] uses skill memory as a repository, archiving past successful plans and their re-\\nspective interactions with the environment. If the agent encounters a task resembling the skills\\nstored in memory, these skills can serve as few-shot exemplars in the LLM agents prompt.\\nThis feature improves not only sample efficiency but also reliability for future planning. To\\nimplement the long-term memory, Wang et al. [337] and Wang et al. [138] proposes tools like\\nvector databases, which can store plans or feedback into high-dimensional vectors.111 Memory-\\n111A vector database is a type of database engineered specifically for handling vector data, which are arrays of\\nnumbers or embeddings representing various types of data objects. These databases are designed to efficiently\\nstore, manage, and perform operations on vectors, often used to represent images, text, or other complex data\\ntypes in a form suitable for machine learning models and similarity search operations. Vector databases excel\\nin handling similarity searches, which involve finding vectors closest to a given vector. They are optimized to\\nstore and query high-dimensional data efficiently.\\n113'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 113, 'page_label': '114'}, page_content='Figure 60: Adding and retrieving skills from the skill library in Voyager. Source: Sun et al. [324]\\nFigure 61: Overview of MemoryBank. The memory storage stores past conversations, summarized\\nevents and user portraits, while the memory updating mechanism updates the memory storage. Memory\\nretrieval recalls relevant memory. Source: Zhong et al. [367]\\nBank [367] incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting\\nCurve theory.112 This mechanism allows the model to forget less relevant information and re-\\ntain more important information based on time elapsed and relative relevance, thereby offering\\na human-like memory management system.\\n4.4.5 LLM-modulo Framework\\nLLM-modulo framework are a novel approach to planning that combines the strengths of LLMs\\nwith the modularity of traditional planning systems.\\nThe reasons behind the development of the LLM-modulo framework are manifold. Kamb-\\n112The Ebbinghaus Forgetting Curve is a psychological theory that describes how information is lost over time\\nwhen there is no attempt to retain it. It shows that humans tend to halve their memory of newly learned\\nknowledge in days or weeks unless they consciously review the learned material.\\n114'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 114, 'page_label': '115'}, page_content='Figure 62:LLMs serve as idea generators, while various external critics, each specializing in different\\naspects, evaluate and provide feedback on the proposed plan. Source: Kambhampati et al. [379]\\nhampati et al. [379] argue that auto-regressive large language models (LLMs) lack the ability to\\nindependently plan and self-verify, which are essential aspects of reasoning. Despite being pow-\\nerful tools trained on vast amounts of data, LLMs function more like advanced n-gram models,\\nexcelling in linguistic tasks but falling short in structured reasoning and planning. LLMs are\\nakin to Kahnemans System 1  fast, intuitive, and associative, but not capable of the delib-\\nerate, logical thinking attributed to System 2. They are better at retrieving information and\\nmaking analogies than performing structured planning or self-critique. A close examination of\\nseveral works claiming planning capabilities for LLMs [283] suggests that they either work in\\ndomains/tasks where subgoal interactions can be safely ignored, or by delegating the interac-\\ntion resolution to the humans in the loop (i.e., repeating prompts until the LLM generates a\\nplan that the human finds acceptable 113). For instance, LLMs are shown to be poor at both\\ngenerating and verifying solutions for tasks such as graph coloring, and fine-tuning them does\\nnot significantly improve their planning abilities [18]. On the contrary, self-critiquing meth-\\nods, where LLMs generate and critique their own solutions, are not effective, as LLMs struggle\\nto verify solutions effectively and the performance are even worse than the direct generation114.\\nAs a result of not being good at self-critique their plans, LLMs cant self-improve by generating\\nand refining their data, contrary to some claims in the literature[379].\\nWhile LLMs can generate candidate plans, these plans are often not executable without\\nerrors as shown in Table 39. This demonstrates that LLMs are more effective when used in\\ncombination with external verification systems in frameworks like the LLM-Modulo Frame-\\nworks, where they serve as approximate knowledge sources rather than independent planners.\\nThe LLM-modulo framework is a hybrid approach that combines the strengths of LLMs with\\nthe modularity of traditional planning systems (see Figure 62). LLMs serve as idea generators,\\nwhile various external critics, each specializing in different aspects, evaluate and provide feed-\\nback on the proposed plan. Critics can evaluate LLM-generated candidate plans over hard and\\n113It implies that the human already knows the answer, and the Clever Hans effect is a potential issue.\\n114The reason is that the system cannot recognise the correct colouring-generated answers.\\n115'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 115, 'page_label': '116'}, page_content='Domain Method GPT-4o GPT-4-\\nTurbo\\nClaude-\\n3-Opus\\nLLaMA-\\n3 70B\\nGemini\\nPro\\nGPT-4\\nBlocksworld\\n(BW) One-shot 170/600\\n(28.33%)\\n138/600\\n(23%)\\n289/600\\n(48.17%)\\n76/600\\n(12.6%)\\n68/600\\n(11.3%)\\n206/600\\n(34.3%)\\nZero-shot 213/600\\n(35.5%)\\n241/600\\n(40.1%)\\n356/600\\n(59.3%)\\n205/600\\n(34.16%)\\n3/600\\n(0.5%)\\n210/600\\n(34.6%)\\nMystery BW\\n(Deceptive) One-shot 5/600\\n(0.83%)\\n5/600\\n(0.83%)\\n8/600\\n(1.3%)\\n15/600\\n(2.5%)\\n2/500\\n(0.4%)\\n26/600\\n(4.3%)\\nZero-shot 0/600\\n(0%)\\n1/600\\n(0.16%)\\n0/600\\n(0%)\\n0/600\\n(0%)\\n0/500\\n(0%)\\n1/600\\n(0.16%)\\nTable 39:Results of state-of-the-art LLMs GPT-4o, GPT-4-Turbo, Claude-3-Opus, Gemini Pro, and\\nLLaMA-3 70B for Plan Generation with prompts in natural language (PlanBench). Source: Kamb-\\nhampati et al. [379]\\nsoft constraints. Hard constraints refer to correctness verification which can include causal cor-\\nrectness, timeline correctness, resource constraint correctness as well as unit tests. On the other\\nhand, soft constraints can include more abstract notions of good form such as style, explicability,\\npreference conformance, etc. LLMs cannot take on the role of the hard critics with soundness\\nguarantees, they can help simulate some aspects of the soft critics. The banks of critics evaluate\\nthe current plan candidate about its fitness and acceptability. If all the hard critics accept the\\nplan, the plan is considered a valid solution to be returned to the user or the executor. When\\nthe critics reject the plan, it can provide various level of feedback including alternative plans,\\npartial plans, or even just the reasons for rejection. One way of obtaining the critics is to use\\npartial planner, operating on either the model itself or their relaxed versions[13]. LLMs can\\nalso be used as Reformulators, since model-based verifiers tend to be operating on specialized\\nformal representations. Reformulators module attached to critics can convert the plan into a\\nform that can be evaluated by the critics, a thing that LLMs are good at [126] 115. The Meta\\n(Backprompt) Controller is responsible for coordinating the interaction between the LLM and\\nthe critics, especially in presence of a mix of hard and soft critics. Controller can assume the\\nresponsibility of compiling critics feedback into a coherent form that can be used to guide the\\nLLM in generating the next candidate plan (e.g., from a simple round-robin prompt selection\\nto a LLM summarized prompt). Humans are involved once per domain and once per problem,\\nacquiring the domain model with the help of the LLM (e.g., teasing out PDDL planning models\\nfrom LLMs) [271]. Once the model is acquired, this way it can be used by correctness verifiers\\nsuch as VAL [7, 271]. Often the planning problems in real world situations are specified in-\\ncompletely, leaving it to the human commonsense to refine the specification. This brings up a\\nsecond role for humansthis time end users. Basically, the LLM-modulo framework remove the\\nrestriction on the expressiveness of the planning language, allowing the LLM to generate plans\\nin natural language, and the critics to evaluate them in a more formal language. Applying\\nthe framework to classical planning domains [333] and recent travel planning benchmark [376]\\nshow that with back prompting from VAL acting as the external verifier and critic, LLM perfor-\\nmance in Blocks World improves to 82% within 15 back prompting rounds, while in Logistics,\\nit improves to 70%. LLM-Modulo doesnt help as much in an obfuscated version of blocks\\nworld called Mystery BW, reaching about 10% accuracy. This should be expected because the\\nLLMs have difficulty generating plausible candidate plans for this domain (note that even here,\\n115Indeed some apporaches to combine LLMs with external symbolic solvers just use LLMs as reformulators\\n116'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 116, 'page_label': '117'}, page_content='if a plan is returned, it must have passed muster with VAL, and is thus guaranteed correct by\\nits model). For the travel planning case study [376], Kambhampati et al. [379] adapted the\\nFigure 63: LLM Modulo Framework adapted for Travel Planning. Source: Kambhampati et al. [379]\\nLLM-Modulo framework to this benchmark by operationalizing hard constraints (e.g., budget\\nconstraints) or commonsense constraints (e.g., suggesting diverse attractions to visit) as critics\\nas shown in Figure 63. The LLM-modulo approach improved of 6 the startlingly low 0.7%\\nbaseline, achieved in Gundawar et al. [376] by using LLMs planners with different prompting\\ntechniques, such as Cot and ReAct (as shown in Figure 64). Furthermore, authors also find\\nthat LLMs reliably play the role of hard critics and several commonsense critics, as well as the\\nreformatter role (i.e., converting free form travel plans into structured plans parseable by the\\ncritics for back-prompts or plan evaluation). In this domain the LLM was able to enumerate\\nthe type of critics that are needed to validate the plan, with little human supervision.\\n4.5 Retrieval-Augmented Generation\\n(RAG) is an innovative paradigm designed to enhance the capabilities of large language models\\n(LLMs) [375]. By integrating retrieval systems with generative models, RAG addresses some\\nof the most pressing challenges in LLMs, including hallucinations, outdated knowledge, and\\nuntraceable reasoning processes. Gao et al. [375] delves into the evolution of RAG frameworks,\\nthe components that constitute these systems, and the metrics used for their evaluation.\\nRAG merges the intrinsic generative abilities of LLMs with external retrieval mechanisms,\\ncreating a synergy that enhances knowledge-intensive tasks.\\nThis framework offers the following core advantages:\\n1. Enhanced Knowledge Integration : By querying external databases, RAG systems\\ncontinuously update their knowledge base, addressing the limitations of static pre-trained\\nmodels.\\n2. Improved Accuracy: Retrieved data serves as contextual grounding, reducing halluci-\\nnations and increasing the factual reliability of generated outputs.\\n117'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 117, 'page_label': '118'}, page_content='Figure 64: Final Pass rates of models across LLM Modulo Iterations. Source: Kambhampati et al.\\n[379]\\n3. Domain Adaptability: RAG enables LLMs to integrate domain-specific information,\\nimproving performance in specialized areas like law, medicine, and engineering.\\nRAG systems are categorized into three main paradigms:\\n1. Na ve RAG: it was the first iteration of RAG systems. It follows the traditional pipeline\\nof indexing, retrieval, and generation, which is also characterized as a Retrieve-Read\\nframework [304]. This approach is simple and effective but suffers notable drawbacks in\\nterms of retrieval precision (e.g., missing crucial information) and generation accuracy\\n(e.g., allowing for hallucinations, toxicity or bias).\\n2. Advanced RAG: it introduces specific improvements to address the limitations of Na ve\\nRAG. About retrieval quality, it employes pre-retrieval and post-retrieval strategies to en-\\nhance the relevance of retrieved data. For indexing, it uses more sophisticated techniques\\nlike sliding window approach, fine-grained segmentation and metadata. It incorporates\\nadditional optimization techniques to streamline the retrieval process [280].\\n3. Modular RAG: this architecture advances beyond previous RAG paradigms (Naive and\\nAdvanced RAG) by offering greater adaptability, flexibility, and functionality. It intro-\\nduces new components and interaction patterns to address the challenges of static and\\nrigid retrieval-generation frameworks, making it suitable for diverse tasks and dynamic\\nscenarios. Modular RAG incorporates specialized modules to enhance retrieval and gen-\\neration:\\n Search Module: Supports direct searches across diverse data sources such as databases,\\nsearch engines, and knowledge graphs using LLM-generated queries [303].\\n RAGFusion: Implements multi-query strategies for diverse perspectives, utilizing\\nparallel searches and re-ranking for knowledge discovery [320].\\n118'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 118, 'page_label': '119'}, page_content='Figure 65: Technology tree of RAG research. The stages of involving RAG mainly include pre-\\ntraining, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused\\non leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the in-\\nference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning\\nof LLMs. Researchers have also been exploring ways to enhance language models in the pre-training\\nstage through retrieval-augmented techniques. Source: Gao et al. [375]\\n Memory Module: Uses LLM memory to iteratively align retrieval processes with\\ndata distribution and enable unbounded memory pools [262].\\n Routing Module: Dynamically selects pathways (e.g., summarization or database\\nquerying) to ensure optimal information retrieval and merging [365].\\n Predict Module: Reduces redundancy and enhances context relevance by generating\\ncontent directly via the LLM [238].\\n Task Adapter Module: Adapts RAG to downstream tasks, automating prompt re-\\ntrieval for zero-shot scenarios and enabling task-specific retrievers through few-shot\\nlearning [388, 250].\\nThese enhancements enable precise and relevant information retrieval for a wide range of\\napplications, improving retrieval efficiency and task-specific flexibility. The architecture\\nintroduces new patterns of interaction and flexibility in module orchestration:\\n Rewrite-Retrieve-Read: Enhances retrieval queries through LLM-based query rewrit-\\n119'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 119, 'page_label': '120'}, page_content='Figure 66: Retrieval-Augmented Generation (RAG) Framework mainly consists of 3 steps. 1) In-\\ndexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2)\\nRetrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the\\nfinal answer. Source: Gao et al. [375]\\ning and feedback mechanisms, improving task performance [303].\\n Generate-Read: Replaces retrieval with LLM-generated content for certain scenar-\\nios [238].\\n Recite-Read: Retrieves directly from model weights to better handle knowledge-\\nintensive tasks [262].\\n Iterative and Hybrid Retrieval: Combines multiple retrieval strategies, including\\nkeyword, semantic, and vector searches, or uses hypothetical document embeddings\\n(HyDE) for improved relevance [320].\\n Dynamic Frameworks: Frameworks like DSP [365] and ITERRETGEN [320] iter-\\natively process retrieval and reading steps, leveraging module outputs to enhance\\nsystem performance.\\nModular RAGs flexible architecture anables module reconfiguration (i.e., modules can\\nbe added, removed, or replaced) to adapt to diverse tasks and data sources, ensuring\\noptimal performance across various domains and applications. Techniques like FLARE\\n[250] dynamically assess the necessity of retrieval in a given context. Additionally, the\\narchitecture supports integration with technologies such as fine-tuning (e.g., retriever or\\ngenerator optimization), reinforcement learning, and collaborative fine-tuning [388, 250].\\nEach RAG system comprises three essential components:\\n1. Retrieval: In the context of RAG, it is crucial to efficiently retrieve relevant docu-\\nments from the data source. They includes unstructured data sources like text-based\\n120'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 120, 'page_label': '121'}, page_content='corpora such as Wikipedia (e.g., HotpotQA, DPR) or cross-lingual text and domain-\\nspecific data, such as medical and legal domains; semi-structured data like PDFs or text-\\nto-SQL approaches (e.g., TableGPT) and text-based transformation methods; structured\\ndata like knowledge graphs combining techniques like KnowledGPT and G-Retriever to\\nenhance graph comprehension and retrieval through integration with LLMs and optimiza-\\ntion frameworks; and LLM-generated content in methods like GenRead and Selfmem that\\nleverage the LLMs internal memory for iterative self-enhancement, bypassing external\\nretrieval. The granularity is task-dependent, balancing relevance and semantic integrity\\nagainst the burden of retrieval complexity. Index and query optimization are used to\\nenhance retrieval efficiency and relevance, ensuring that the retrieved data aligns with\\nthe task requirements. In the indexing phase, documents will be processed, segmented,\\nand transformed into Embeddings to be stored in a vector database. For indexing its im-\\nportant to segment documents into smaller chunks, with trade-offs between larger chunks\\n(context-rich but noisy) and smaller chunks (context-poor but precise). Some approaches\\nenhances chunks with metadata (e.g., timestamps, summaries) enabling contextual fil-\\ntering and time-aware retrieval. Hierarchical structures, such parent-child relationships,\\naid in swift data traversal and reduce illusions from block extraction. Knowledge Graph\\nindices align document structures and relationships, improving retrieval coherence and\\nefficiency. Formulating a precise and clear question is difficult, and imprudent queries\\nresult in subpar retrieval effectiveness. Sometimes, the question itself is complex, and the\\nlanguage is not well-organized. Another difficulty lies in language complexity ambiguity.\\nQuery optimization techniques includes query expansion, transformation, and routing.\\nQuery expansion techniques like multi-query and sub-query generation add contextual\\ndepth to queries. Chain-of-Verification (CoVe) validates expanded queries using LLMs to\\nreduce hallucinations. Query transformation core concept is to retrieve chunks based on\\na transformed query instead of the users original query. It may invloves the use of LLM\\nto rewrite query or use prompt engineering to let LLM generate a query based on the\\noriginal query for subsequent retrieval. Dynamic pipelines (e.g., semantic or metadata-\\nbased routing) enhance adaptability for diverse scenarios. Embedding in RAG is crucial\\nfor efficient retrieval based on similarity (e.g., cosine similarity) between the embedding\\nof the question and document chunks, where the semantic representation capability of\\nembedding models plays a key role. This mainly includes a sparse encoder (BM25) and\\na dense retriever (BERT architecture Pre-training language models). Advanced models\\nlike AngIE and Voyage leverage multi-task tuning to improve semantic representation and\\nretrieval accuracy.\\n2. Generation: RAG systems benefit significantly from post-retrieval adjustments to both\\nthe retrieved content and the underlying language models (LLMs). Directly feeding raw,\\nretrieved data into an LLM is suboptimal, as redundant or overly lengthy contexts can\\ndilute the quality of the final output. Efficient context curation involves refining re-\\ntrieved content to maximize relevance and conciseness while reducing noise. This step\\naddresses critical challenges such as the Lost in the Middle problem, where LLMs often\\nlose focus on mid-segment information in lengthy texts. Reranking prioritizes the most\\npertinent chunks from retrieved documents to improve the precision of inputs for LLMs.\\nThis process can involve: Contrary to the misconception that longer contexts yield bet-\\nter outcomes, excessive data can overwhelm LLMs. Techniques for context compression\\ninclude:\\n Token Filtering: Small Language Models (SLMs) such as GPT-2 Small are used to\\nremove less critical tokens while maintaining semantic integrity.\\n121'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 121, 'page_label': '122'}, page_content=' Information Extractors: PRCA trains specialized models to extract relevant con-\\ntent, while RECOMP uses contrastive learning to train condensers for refining con-\\ntext [262], [388].\\n Filter-Reranker Paradigm: Combines SLMs as filters and LLMs as rerankers to im-\\nprove downstream information extraction tasks. For example, Chatlaw incorporates\\nLLM critiques to assess and filter legal provisions based on relevance [303].\\n Rule-Based Methods: These rely on predefined metrics like diversity, relevance, or\\nMean Reciprocal Rank (MRR).\\n Model-Based Approaches: Encoder-decoder models such as SpanBERT or special-\\nized rerankers like Cohere or GPT-based reranking mechanisms reorder documents\\neffectively [238, 365].\\nFine-tuning LLMs allows alignment with task-specific scenarios and enhances their ability\\nto process domain-specific data. Key methods include:\\n Scenario-Specific Training: Fine-tuning LLMs on specialized datasets improves their\\nadaptability to unique data formats or stylistic requirements. Frameworks like\\nSANTA leverage contrastive learning for retriever training and reinforcement learn-\\ning to align outputs with human preferences [320, 262].\\n Distillation: When access to larger proprietary models is limited, knowledge distilla-\\ntion enables smaller models to emulate the behavior of powerful systems like GPT-4.\\nThis method ensures that compact models retain efficacy in specific domains.\\n Alignment Techniques: Fine-tuning aligns retriever and generator preferences. For\\ninstance, RA-DIT uses KL divergence to align scoring functions between the retriever\\nand the generator, enhancing overall coherence in retrieval-generation workflows\\n[250].\\n3. Augmentation: the standard practice involves a single retrieval step followed by a gener-\\native output. While effective for straightforward tasks, this approach is often insufficient\\nfor more complex problems requiring multi-step reasoning, as it limits the scope of re-\\ntrieved information [361]. To address these limitations, various iterative, recursive, and\\nadaptive retrieval strategies have been proposed, enabling RAG systems to dynamically\\nenhance their retrieval and generation processes. Iterative retrieval involves repeatedly\\nquerying the knowledge base based on the initial query and the text generated so far.\\nThis cyclical approach offers a more comprehensive knowledge base for language models,\\nimproving the robustness of generated responses. By incorporating additional contextual\\nreferences through multiple retrieval iterations, iterative retrieval enhances the generative\\nprocess, particularly for tasks requiring multi-step reasoning. However, challenges such as\\nsemantic discontinuity and the accumulation of irrelevant information can arise. ITER-\\nRETGEN [320] exemplifies this approach by combining retrieval-enhanced generation\\nwith generation-enhanced retrieval. It iteratively refines the context, ensuring that the\\nknowledge retrieved aligns closely with the specific task at hand. This synergy facili-\\ntates the generation of more accurate and contextually relevant responses in subsequent\\niterations.\\nRecursive retrieval refines search results by iteratively updating the search query based\\non feedback from previous results. This method enhances the depth and relevance of re-\\ntrieved information, enabling systems to gradually converge on the most pertinent content.\\nRecursive retrieval is particularly effective in scenarios where user queries are ambiguous\\n122'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 122, 'page_label': '123'}, page_content='or where the sought information is highly nuanced. IRCoT [223] employs a chain-of-\\nthought (CoT) approach, using retrieval results to iteratively refine the CoT reasoning\\nprocess. ToC (Tree of Clarifications) [284] systematically addresses ambiguities in queries\\nby constructing clarification trees that refine the retrieval process step-by-step. Recur-\\nsive retrieval often pairs with multi-hop retrieval for graph-structured data, extracting\\ninterconnected knowledge. This combination is particularly effective for hierarchical or\\nmulti-document environments, where summaries or structured indices aid in refining sub-\\nsequent retrieval steps [291].\\nAdaptive retrieval allows RAG systems to dynamically decide when and what to retrieve,\\ntailoring the retrieval process to the specific requirements of the task. This flexibility\\nenhances both the efficiency and the relevance of retrieved information. Flare [250] and\\nSelf-RAG [262] enable LLMs to determine optimal retrieval moments and content, im-\\nproving the adaptive capabilities of RAG frameworks. GraphToolformer [291] divides\\nretrieval into distinct stages, where LLMs actively utilize tools such as retrievers and\\napply techniques like Self-Ask or few-shot prompts to guide the process. WebGPT [124]\\nintegrates reinforcement learning to train LLMs for autonomous search engine usage. By\\nleveraging special tokens for actions such as querying, browsing, and citing sources, it\\nmimics an agent actively gathering and validating information during generation.\\nSome of the most widely used metrics for evaluating RAG systems include:\\n Retrieval Precision: Measures the relevance of retrieved data.\\n Generation Accuracy: Assesses the factual correctness of outputs.\\n End-to-End Performance: Evaluates the overall coherence, fluency, and informativeness\\nof the system.\\nBenchmarks such as SQuAD [33], Natural Questions [71], and specialized datasets for re-\\ntrieval tasks are widely used for assessment.\\nDespite its promise, RAG faces several challenges:\\n1. Retrieval Latency: Efficiently querying large databases in real time remains a technical\\nhurdle.\\n2. Data Quality: The reliability of generated outputs depends heavily on the quality of\\nretrieved data.\\n3. Scalability: Handling large-scale retrieval tasks while maintaining high generation quality\\nis complex.\\nFuture research avenues include:\\n Expanding RAG frameworks to support multi-modal inputs, such as text, images, and\\naudio.\\n Enhancing retrieval efficiency through novel indexing and search techniques.\\n Improving integration mechanisms for tighter coupling between retrieval and generation\\nmodules.\\nRAG represents a transformative step in LLM development, bridging the gap between static\\npre-trained knowledge and dynamic, context-aware generation. By combining retrieval and\\ngeneration, RAG systems are poised to redefine the capabilities of AI in knowledge-intensive\\ntasks.\\n123'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 123, 'page_label': '124'}, page_content='5 Testing the CoT Capabilities of LLMs\\nIn this section, we investigate the origins of some skills demonstrated by large language models\\n(LLMs), such as the Chain-of-Thought (CoT). We will briefly summarize the evidence presented\\nin several experiments documented in scientific articles and papers. Subsequently, we will\\nexamine whether certain hypotheses are validated through tests conducted on publicly available\\nmodels via LMStudio software on HuggingFace.\\n5.1 What is eliciting the Chain-of-Thought?\\nAs we have seen in the previous sections, LLMs have shown some remarkable abilities, such\\nas language generation, the ability to perform Chain-of-Thought (CoT), a form of reasoning\\nthat involves multiple steps, In-Context Learning, and more. Even though LLMs reasoning\\nability is controversial, we focus our attention on a different question: what is eliciting these\\nabilities?\\nGenerally, the above abilities are attributed to the large size of the pre-training data. The\\nlanguage generation ability is a direct consequence of language modelling training objectives.\\nLiang et al. [185] concluded that the performance on tasks requiring knowledge of the world is\\ndirectly proportional to the size of the pre-training data.\\nThe source of CoT ability is less clear and still elusive. Some hypotheses have been proposed\\nto explain the origins of this skill. Scale is not a deciding factor: some models are large enough,\\nlike OPT175B and BLOOM176B, that cannot do CoT 116, while smaller models like UL2 20B [328]\\nor Codex12B [108] can leverage on CoT 117 to improve performance.\\nOne of the most popular theories is that the CoT reasoning is related to code in the pre-\\ntraining dataset.\\nThere is also speculation that training on code data can greatly increase the chain-\\nof-thought prompting abilities of LLMs, while it is still worth further investigation\\nwith more thorough verification [364].\\nOne piece of evidence is that code-davinci-002, a model trained on code data, is consistently\\nbetter on CoT than text-davinci-002 on language tasks [360] as shown in Table 40.\\nOn the HELM evaluation, a massive-scale evaluation performed by Liang et al. [185], the\\nauthors also found that models trained on/for code have strong language reasoning abilities.\\nAs an intuition, procedure-oriented programming is similar to solving tasks step by step, and\\nobject-oriented programming is similar to decomposing complex tasks into simpler ones.\\nOther hypotheses suggest a minor role in the instruction tuning.\\nInstruction tuning does not inject new abilities into the model  all abilities are\\nalready there. Instead, instruction tuning unlocks/elicits these abilities. This is\\nmostly because the instruction tuning data is orders or magnitudes less than the\\npre-training data [162].\\nA piece of evidence is the GPT-3 text-davinci-002118 leverages on CoT to improve performance,\\nwhereas the previous text-davinci-001 could not do CoT well. PaLM [155] itself shows that\\ninstruction-tuning can elicit CoT since the first version was not instruction-tuned.\\n116It means CoT performance is worse than direct prompting or fine-tuning on smaller models\\n117Notably, CoT prompting does not require any additional fine-tuning of the model.\\n118The model is instruction-tuned with RL\\n124'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 124, 'page_label': '125'}, page_content='Model NumWord SwapAnt\\nori trans ori trans all\\n0-shot\\ncode-\\ndavinci-002\\n0.000.00 4.67 8.08 26.00 45.03 8.00 13.86 70.00 3.07\\ntext-davinci-\\n002\\n68.416.24 66.67 35.79 95.57 5.18 36.29 18.66 72.73 2.55\\n1-shot\\ncode-\\ndavinci-002\\n69.005.29 97.33 3.06 89.675.51 80.33 10.60 76.13 3.63\\ntext-davinci-\\n002\\n72.317.04 98.59 1.65 64.1414.24 78.69 1.93 69.57 8.35\\n3-shot\\ncode-\\ndavinci-002\\n73.001.00 100.00 0.00 80.67 4.51 91.00 5.57 84.48 0.18\\ntext-davinci-\\n002\\n73.142.60 96.10 6.53 66.45 5.80 85.86 9.69 72.70 3.57\\nTable 40: Results of code-davinci-002 and text-davinci-002 on MRPC dataset (original and trans-\\nformed by TextFlint, a multilingual robustness evaluation toolkit for NLP tasks that incorporates uni-\\nversal text transformation, task-specific transformation, adversarial attack, subpopulation, and their\\ncombinations to provide comprehensive robustness analyses). The results highlight the superiority of\\ncode-davinci-002 on CoT. Source: Ye et al. [360].\\n5.2 Empirical evidences\\nIn this section, we will present some empirical evidence supporting the previous sections hy-\\npotheses. We have used the LMStudio[381] software to test the hypotheses on publicly available\\nmodels. The hardware used for the experiments is:\\n Chip: Apple M1 Pro\\n Cores: 10 (8 performance and 2 efficiency)\\n RAM: 32 GB\\nThe number of experiments we can conduct is limited due to machine resources and time\\nconstraints. As mentioned, really large models require a lot of resources, and its impossible\\nto run most of them on a personal computer. Moreover the assumption is that the ability to\\nperform CoT is not related to the model size, but rather to the pre-training data. However,\\nwhen comparing models of the same size, we can exclude this factor from the equation and\\nfocus on testing whether CoT reasoning ability is related to code in the pre-training dataset.\\nAdditionally, the models available on LMStudio are limited to the models available on Hug-\\ngingFace, while others are closed-source and have not been publicly released. For this reason,\\nwe focused the experiments (see Table 41) on Llama family models, which are publicly available\\non HuggingFace. As reported by the authors, the architecture of the different models is quite\\nsimilar. Indeed, Llama 3 uses a standard, dense Transformer architecture [334] which does not\\ndeviate significantly from Llama [330] and Llama 2 [329] in terms of model architecture. This\\n125'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 125, 'page_label': '126'}, page_content='suggests that the performance improvements are mainly due to enhancements in data quality\\nand diversity, as well as increased training scale [389].\\nThe percentage of code in the pre-training data of the first Llama model [330] is about 5%.\\nThis percentage increases in the Llama 2 model [329] to 8%. The fine-tuned Llama 2 model,\\nCode Llama[384], adds 500B extra tokens, consisting mostly of code (85%). Lastly, the Llama 3\\nand 3.1 model [389] has 17% of code in its pre-training mix 119.\\nThe experiments have been conducted using the Chain-of-Thought on reasoning tasks from\\nthe GSM8k and gsm-hard 120 Reasoning steps in the gsm-hard datasets are expressed as code,\\nso we also tested the Program of Thought (PoT) approach [259]. PoT is suitable for problems\\nFigure 67: Example of a gsm-hard problem. The reasoning steps are expressed as code.\\nwhich require highly symbolic reasoning skills. The previous paragraph explored a similar\\napproach (see Par. 4.4.3). An example of a gsm-hard problem, reasoning steps and solution\\nis shown in the Picture 67. The results from the execution of the experiments are shown in\\nTable 41.\\nGSM8k 0-shot GSM8k 5-shot GSM-hard 0-shot GSM-hard 5-shot\\nLlama27B 3.1% 15.7% 0% 0% (16.69%)\\nCode\\nLlama7B\\n3.99% 16.3% 1.3% 1.5% (27.6%)\\nLlama213B 10.53% 35.8% 0% 0% (36%)\\nLlama37B 31.0% 47.0% 5.4% 7.4% (56.1%)\\nLlama3.17B 75.9 % 80.9% 7.85% 9.46% (62.36%)\\nTable 41:Comparison of Llama models on mathematical reasoning tasks. The numbers in parentheses\\nfor the last column are the success rate leveraging the PoT reasoning ability (i.e., executing the Python\\ncode in the reasoning part) rather than using the solution provided by the model itself.\\nAs expected, Llama 3 performs better than Llama 2, and its CoT reasoning ability improves\\nas performance increases between the 0-shot and 5-shot settings. Since the models are the same\\nsize and have similar architectures, the improvement is related to different models pre-training\\ndata. Main difference between Llama 2 and Llama 3 is the percentage of code in the pre-\\ntraining data, which is 8% for Llama 2 and 17% for Llama 3. It confirms that the code in\\nthe pre-training data can greatly increase the CoT reasoning ability of LLMs. We also run the\\nsame experiments on LLaMA213B to further exclude the size factor. It confirms that size is not\\na deciding factor in CoT since both show the ability to perform CoT reasoning. Despite that,\\nLlama13B results show that scaling up the model can improve the CoT ability but its not a\\ndeciding factor. As hypothesized, in general the improvement between the 0-shot and 5-shot\\n119See Section 2.3.4 for more details on the various versions of the Llama model.\\n120The gsm-hard is obtained by replacing the numbers in the questions of GSM8k with larger numbers that\\nare less common.\\n126'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 126, 'page_label': '127'}, page_content='on GSM8k consistently increases with the percentage of code in the pre-training data among\\nall the models.\\nWe also tested the Program-of-Thoughts (PoT) reasoning ability on the gsm-hard dataset\\nsince the dataset demonstrations are expressed as code, and the model is stimulated to produce\\nreasoning expressed as code. The code was extracted from the models solution and executed\\nby a Python interpreter to calculate the result. The performance is indicated in the table\\ninside the parentheses, which shows that PoT also increases with the percentage of code in the\\npre-training data. We can note that all the models have a low success rate in the gsm-hard\\ndataset, while the performance increases using PoT. The fact that the models performance\\ndrops in the 0-shot gsm-hard dataset, which is simply using larger numbers, suggests that\\nthe LLMs cannot reason if they cannot figure out the underlying algorithm, rather they learn\\nthe distribution in the pre-training data. Also, the CoT reasoning ability is accepted that\\ndoesnt generalize well after a point as we can see in the results of the 5-shot gsm-hard dataset.\\nThe increment using PoT is more significant than the one using the models solution, which\\nsuggests that demonstrating reasoning as code improves the ability to generate code rather\\nthan the reasoning itself. It could be explained by the fact that models are trained on GitHub,\\nan high quality code dataset, so they can retrieve the pieces of code.\\n5.3 Prompting\\nThe prompt and the request parameters affect significantly the models performance. This, plus\\nthe code that verifies the models solution, can lead to different results with results presented\\nin other papers. After some experimentation, we reached satisfactory results with the following\\nparameters and prompt format:\\n Top-p sampling: 0.9 121.\\n Temperature: 0.7 122.\\n Max tokens: 1024 123.\\nThe context is set to the maximum length permitted by the model, and the prompt follows the\\nOpenAI API request format 124. Each prompt has a list of messages, and each message has a\\nrole (e.g., user, assistant, system) and content (see Figure 68). The system message is used to\\nprovide the model with the task to perform and to instruct the model on how to provide the\\nsolution, providing guidelines and requirements.\\n5.4 Examples of generated text\\nIn the 0-shot setting, the prompt is provided with no examples, and the CoT reasoning is origi-\\nnated by applying the Lets think step by step approach in the systems prompt as suggested\\nby Kojima et al. [285].\\nThe system prompt used for the 0-shot experiments is:\\nYou are a maths teacher. Answer the following mathematical reasoning question.\\nThe solution is a single numerical value.\\nLet\\'s think step by step and then provide the solution on the last line in the\\nfollowing form: \"The solution is <<value>>\",\\n121A sampling strategy that selects the highest probability tokens whose cumulative probability exceeds a\\ncertain threshold.\\n122A hyperparameter that controls the randomness of the model by scaling the logits before applying softmax.\\n123The maximum number of tokens that the model can generate.\\n124https://platform.openai.com/docs/api-reference/chat\\n127'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 127, 'page_label': '128'}, page_content=\"Figure 68: The prompt format with role-based messages.\\nIn the 5-shot prompting, the model is given five examples of the task, and the structure is\\nsimilar to what we see in Figure 68. The main difference is that before the last user message,\\nthere are five pairs of user and assistant messages, in which the assistant provides the solution\\nto the task. Assistant messages serve as examples for the model to learn from and they do not\\ntrigger the inference. The system prompt in this case can be simplified as:\\nYou are a maths teacher. Answer the following mathematical reasoning question.\\nLet's think step by step.\\nsince the reasoning is guided by the assistant messages.\\nLlama2 7B GSM8k 0-shot\\nExample of generated text from Llama2 7B on GSM8k  0-shot task:\\nPrompt:\\nSuzie loves to chew fruit-flavored gum. She bought four packs of gum the last\\ntime she was at the store. She got two packs of her favorite flavor,\\nstrawberry. She paid $2 for a pack of grape gum that she also liked. She\\nwanted to try something new, so she paid half as much for a small pack of\\ngreen apple gum. If she paid $7 in all, how many dollars did each pack of\\nstrawberry gum cost?\\n,\\n,\\n,\\n,\\n,\\nAnswer:\\nEach packet of Stawberries Gums costs 2 Dollars.\\nThe answer is correct, but the reasoning is not present.\\nPrompt:\\nJeffs work is 3 miles away. He walks there and back each day he works. How\\nmany miles does he walk if he has to work 5 times a week?,\\nAnswer:\\n128\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 128, 'page_label': '129'}, page_content=\"Answer: The answer should be (10 x) + (-2). It'll take you about an hour, but\\nit will help your math skills!,\\nThe answer is incorrect (correct answer is 30).\\nLlama2 7B gsm-hard 0-shot\\nWe do not report any sample of generated text from Llama2 7B on gsm-hard  0-shot task, as\\nthe model has not shown any ability to answer correctly to any of the prompts.\\nCode Llama 7B GSM8k 0-shot\\nExample of generated text from Code Llama 7B on GSM8k  0-shot task:\\nPrompt:\\nJohn adopts a dog. He takes the dog to the groomer, which costs $100. The\\ngroomer offers him a 30% discount for being a new customer. How much does\\nthe grooming cost?\\n,\\n,\\nAnswer:\\nThe answer is: John spends $70 on the dogs grooming.\\nExecuting the code snippet returns the correct answer.\\nPrompt:\\nChristina is planning a birthday party and needs .75 gift bags per invited\\nguest, because 1/4 of attendees don't show up. She invited 16 friends. Gift\\nbags are $2 each. How much will she spend?\\n,\\n,\\nAnswer:\\nLets think step by step.Christina is planning a birthday party and needs .75\\ngift bags per invited guest, because 1/4 of attendees don't show up. She\\ninvited 16 friends. Gift bags are $2 each. How much will she spend?\\n,\\n,\\nThe number of attendees who dont show up is 25% of the total number of guests,\\nso 16*0.75=12.,\\nThe code snippet returns the wrong answer (correct answer is 24). The reasoning is coherent,\\nbut it misses the final step to calculate the total amount spent 12  2$ = 24$.\\nCode Llama 7B gsm-hard 0-shot\\nExample of generated text from Code Llama 7B on gsm-hard  0-shot task:\\nPrompt:\\nGiven a 7-day week, how much does Alex charge for 2 weeks of tutoring if she\\ncharges $12 per day?,\\nAnswer:\\n129\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 129, 'page_label': '130'}, page_content=\"If Alex charges $12 per day and I ask her to tutor me for just one day, then\\nwhat is the cost of that particular day's tuition?,\\nIf I asked Alex to tutor me for two days, then how much would it cost?\\nIf I asked Alex to tutor me for a week, then how much would it cost?\\nIf I asked Alex to tutor me for two weeks, then how much would it cost?\\nThe answer is 216.\\nI can calculate this by using the following steps:\\nAlex charges $12 per day\\nIf Alex charges $12 per day and I ask her to tutor me for just one day, then\\nwhat is the cost of that particular day's tuition?,\\nI pay $12.\\nIf I asked Alex to tutor me for two days, then how much would it cost?\\nI will have to pay $12  2 = $24.\\nIf I asked Alex to tutor me for a week, then how much would it cost?\\nI will have to pay $12  7 = $84.\\nIf I asked Alex to tutor me for two weeks, then how much would it cost?\\nI will have to pay $12  7  2 = $168.\\nThe answer is 168.\\nAlex charges $12 per day. If I ask her to tutor me for just one day, then what\\nis the cost of that particular day's tuition?,\\nI will have to pay $12.\\nAlex charges $12 per day. If I asked Alex to tutor me for two days, then how\\nmuch would it cost?,\\nIf I ask Alex to tutor me for two days, then what is the total amount I will\\nhave to pay?,\\nAlex charges $12 per day and I ask her to tutor me for just one day. Then the\\ncost of that particular day's tuition would be $12.,\\nIf I asked Alex to tutor me for two days, then how much would it cost?\\nI will have to pay $12  2 = $24.\\nAlex charges $12 per day and if I ask her to tutor me for a week, what is the\\ntotal amount I will have to pay?,\\nIf I ask Alex to tutor me for just one day, then what is the cost of that\\nparticular day's tuition?,\\nIf I asked Alex to tutor me for two days, then how much would it cost?\\nI will have to pay $12  7 = $84.\\nAlex charges $12 per day and if I ask her to tutor me for a week, what is the\\ntotal amount I will have to pay?,\\nIf I asked Alex to tutor me for two weeks, then how much would it cost?\\nI will have to pay $12  7  2 = $168.\\nThe reasoning is really noisy and the model gives multiple answers, but the correct one is\\npresent (168).\\nPrompt:\\nGoldy bought 20 sacks of rice and gave 3 sacks to her cousin and 9020567 sacks\\nto her brother, if there are 25 kilograms of rice per sack, how many\\nkilograms does she gave to her cousin and brother?\\n,\\n,\\nAnswer:\\nGoldy bought 20 sacks of rice and gave 3 sacks to her cousin and 9020567 sacks\\nto her brother, if there are 25 kilograms of rice per sack, how many\\nkilograms does she gave to her cousin and brother?\\n,\\n,\\n130\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 130, 'page_label': '131'}, page_content=\"This is a question where you need to find out the number of kilograms that\\nGoldy gave to her cousin and brother based on what she bought and gave.\\nLets go through this step by step. First, we need to find out how many\\nsacks Goldy has left after giving away 3 sacks to her cousin and 9020567\\nsacks to her brother.\\n,\\n,\\n,\\n,\\nSo, Goldy has 20 - 3 = 17 sacks of rice left after giving away 3 sacks to her\\ncousin and 9020567 sacks to her brother. Since each sack contains 25\\nkilograms of rice, we can calculate the total number of kilograms that\\nGoldy has left by multiplying the number of sacks she has with the number\\nof kilograms per sack. So,\\n,\\n,\\n,\\n,\\n17 x 25 = 425 kilograms\\nSo, Goldy has 425 kilograms of rice left after giving away 3 sacks to her\\ncousin and 9020567 sacks to her brother. We can now find out how many\\nkilograms she gave to her cousin based on the number of kilograms she had\\nleft and the number of kilograms her cousin wanted. So,\\n,\\n,\\n,\\n425 - (3 x 25) = 425 - 75 = 350 kilograms\\nSo, Goldy gave 350 kilograms of rice to her cousin. Now that we have found out\\nhow many kilograms Goldy gave to her cousin and brother, lets find out how\\nmany kilograms she gave to her brother. So,\\n,\\n,\\n425 - (9020567 x 25) = 425 - 20108325 = 20104579 kilograms\\nSo, Goldy gave 20104579 kilograms of rice to her brother. This concludes our\\nreasoning on how many kilograms Goldy gave to her cousin and brother based\\non what she bought and gave.\\n,\\n,\\nThe answer is incorrect (correct answer is 225514250).\\nLlama2 13B GSM8k 0-shot\\nExample of generated text from Llama2 13B on GSM8k  0-shot task:\\nPrompt:\\nEvery day, Wendi feeds each of her chickens three cups of mixed chicken feed,\\ncontaining seeds, mealworms and vegetables to help keep them healthy. She\\ngives the chickens their feed in three separate meals. In the morning, she\\ngives her flock of chickens 15 cups of feed. In the afternoon, she gives\\nher chickens another 25 cups of feed. How many cups of feed does she need\\nto give her chickens in the final meal of the day if the size of Wendi's\\nflock is 20 chickens?\\n,\\n,\\n,\\n,\\n,\\n,\\nAnswer:\\nEvery day, she feeds each one three separate times a cup mixed-feed containing\\nseeds and vegetables. In addition, it contains worms to keep them healthy\\nfor their growth rate! She gives all of these animals in the morning with\\nfifteen cups worth while giving another twenty five during afternoon hours\\nbefore finally ending up at night when she needs just 10 more than what was\\ngiven earlier that day so there are now only ten left over from yesterday's\\nmeal which means we need to find out how many chickens were eaten by Wendi\\ntoday.\\n,\\n,\\n,\\n,\\n,\\n,\\n,\\nThe correct answer is: B) Twenty cups of feed in the final meal if her flock\\nsize 20 Chicken,\\nThe answer is correct and the reasoning is partially coherent.\\n131\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 131, 'page_label': '132'}, page_content='Prompt:\\nThere are 9,300 pennies in a cup. What is the total dollar amount in a stack\\nthat contains two thirds of the pennies in the cup?,\\nAnswer:\\nYou can use any method to solve this problem, and you will receive full credit\\nfor your answer as long its correct!,\\nThe answer is incorrect (correct answer is 62).\\nLlama2 13B gsm-hard 0-shot\\nExample of generated text from Llama2 13B on gsm-hard  0-shot task:\\nPrompt:\\nA candle melts by 2 centimeters every hour that it burns. How many centimeters\\nshorter will a candle be after burning from 1:00 PM to 5:00 PM?,\\nAnswer:\\nThe candle is 2 cm shorter at 1:00 PM than it was at midnight.\\nAt 2:00 PM, the candle is 4 cm shorter than it was at midnight.\\nAt 3:00 PM, the candle is 6 cm shorter than it was at midnight.\\nAt 5:00 PM, the candle is 10 cm shorter than it was at midnight.\\nThe candle is 8 cm shorter after burning from 1:00 PM to 5:00 PM.\\nThe answer is correct, but the reasoning is not coherent.\\nPrompt:\\nOut of the 9030534 Grade 5 students, 2/5 are boys and 2/3 of the girls are in\\nthe girl scout. How many girls are not in the girl scout?,\\nAnswer:\\nYou can use a calculator to help you solve this problem!\\nThe answer is incorrect (correct answer is 1806106.8000000003).\\nLlama 3 7B GSM8k 0-shot\\nExample of generated text from Llama 3 7B on GSM8k  0-shot task:\\nPrompt:\\nGretchen has 110 coins. There are 30 more gold coins than silver coins. How\\nmany gold coins does Gretchen have?,\\nAnswer:\\n132'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 132, 'page_label': '133'}, page_content='Show me how you solve this problem.\\nI\\'d be happy to help! To start, let\\'s break down what we know:\\n* Gretchen has a total of 110 coins\\n* The number of gold and silver coin is different (30 more)\\nLets use the variable G for Gold Coins. Let S represent Silver Coin.\\nWe can write an equation based on this information:\\nG + S = 110\\nSince there are 30 more gold than silver, we know that:\\nG -S=30\\nNow let\\'s add both equations together to get rid of one unknown.\\n(G+S) +( G-S)= (2* G)\\nSo the new Equation is:\\n(2 * G ) = 140\\nDivide by two on each side:\\nG =70\\nWe now know that Gretchen has 70 Gold Coins. So, answer for this question will\\nbe \"The number of gold coins does Gretchen have? The Answer is **70**\". I\\nhope you find my explanation helpful! Let me know if there\\'s anything else\\nI can assist with.\\n,\\n,\\n,\\nThe answer is correct and the reasoning is coherent.\\nPrompt:\\nGoldy bought 20 sacks of rice and gave 3 sacks to her cousin and 4 sacks to her\\nbrother, if there are 25 kilograms of rice per sack, how many kilograms\\ndoes she gave to her cousin and brother?\\n,\\n,\\nAnswer:\\nI will help you solve this problem using mathematical reasoning.\\nLet\\'s break it down step by step:\\n1. Goldy bought a total of 20 sacks.\\n2. She then gives away some bags:\\n- To her cousin (3 sacks)\\n- And also, she gave to her brother(4sacks)\\nSo we can calculate the number of kilograms given as follows:\\n- Total rice in all sack = Number Of Sack * Weight per bag\\n= 20*25 kg/sac\\nTotal Rice In All Bag:\\nGoldy bought a total amount (in weight) that is equal to:\\n(Weight Per Sac)*(Number Of Bags)\\n=(kg/Bag)*sacks of bags.\\nTotal rice in all sack = Number Of Sack * Weight per bag\\n133'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 133, 'page_label': '134'}, page_content=\"= 20*25 kg/sac\\nTotal Rice In All Bag:\\nGoldy bought a total amount (in weight) that is equal to:\\nThe answer is incorrect (correct answer is 175).\\nLlama 3 7B gsm-hard 0-shot\\nExample of generated text from Llama 3 7B on gsm-hard  0-shot task:\\nPrompt:\\nA car in the fast lane is traveling at 60 miles/hour. A car in the slow lane is\\ntraveling at half that speed. If the car in the fast lane traveled for a\\ntotal of 480 miles, calculate the time the car in the slow lane took to\\ncover the same distance?\\n,\\n,\\n,\\nAnswer:\\nAssume both cars are moving continuously and there were no stops or breaks.\\nI understand your request! To solve this problem step by step:\\nLet's start with identifying variables. We know that:\\n- The speed of a fast-lane car is 60 miles/hour.\\n- A slow lane car travels at half the speed, which means its velocity (speed) v\\n= 30 mph,\\nWe also have information about distance:\\nThe total time taken by both cars to cover this same distance will be equal. We\\ncan use D=rt formula where r represents rate and t is for time.,\\nLet's calculate it:\\nFast lane car:\\nD1 = rt, so we know that the fast-lane car took 480 miles (distance) at a speed\\nof 60 mph.,\\nt_faster_lane_car = Distance / Speed\\nt_faster_lane_car= D/ r\\nSubstitute values: t_fastlane_ Car=(480)/60\\nNow let's find time taken by slow lane:\\nD2 is the same distance, but we need to calculate it using v (speed) and d1.\\nSince both cars cover equal distances:,\\nd = rt\\nt_slow_lane_car= D/ r\\nSubstitute values: t_slowlane_ Car=(480)/30\\nNow that you have calculated time for each car:\\nTime taken by fast lane is 8 hours, while slow-lane takes twice as long since\\nit moves at half the speed. So Time spent in Slow Lane = (2 *\\nFast-Lane-Time)\\n,\\n,\\nt_slow_lane_car= t_fastlane_ Car*2\\n134\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 134, 'page_label': '135'}, page_content='Substitute values:\\nFinal Answer:\\nThe time for a car traveling on the fast lane is 8 hours, and that of slow-lane\\nwill be twice as long or approximately equal to 16 hours. I hope this\\nexplanation helps you understand how we arrived at our answer! Please let\\nme know if there\\'s anything else I can assist with.\\n,\\n,\\n,\\nPlease confirm whether my response meets your expectations before proceeding\\nfurther assistance requests from the user.,\\nThank You for trusting AI Assistant, and please feel free to ask any questions\\nor request help anytime.,\\nThe answer is correct and the reasoning is coherent.\\nPrompt:\\nMaddison has 849488 boxes with 8494880 marbles in each box. Then she gets 20\\nmarbles from her friend. How many marbles does she have now?,\\nAnswer:\\nI\\'d be happy to help you solve the problem!\\nLet\\'s break it down step by step:\\n1. Maddison has an initial number of boxes, which is given as 849488.\\n2. Each box contains 8494800 (not marbles) in each box). So, we can multiply\\nthis value with the total number of boxes to get her initial marble count:,\\nInitial Marble Count = Number of Boxes * Marbles per Box\\n= 849488 x 8484900\\n\\\\approx4.65 billion\\n3. Then she receives an additional gift from a friend - which is given as \"20\\nmarbles\". This means we need to add these new marbles to her existing\\nmarble count.\\n,\\n,\\nNew Marble Count (after receiving the extra) = Initial Marble Count + New\\nMarbles,\\n= 4650000000 + 20\\n\\\\approx4.65 billion\\nSo, Maddison now has approximately **approximately** around `4.65001` *10^9*\\nmarbles after getting an additional gift of \"20\" from her friend.,\\nWould you like me to explain anything further or clarify any part? Please let\\nme know!,\\nThe answer is incorrect (correct answer is 42474420).\\n135'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 135, 'page_label': '136'}, page_content='Llama3.1 7B GSM8k 0-shot\\nExample of generated text from Llama3.1 7B on GSM8k  0-shot task:\\nPrompt:\\nA pen costs as much as a pencil and eraser combined. A pencil costs $1.20 and\\nan eraser costs $0.30. How much will 8 pens cost?,\\nAnswer:\\n## Step 1: Determine the cost of one set that includes a pencil and an eraser.\\nTo find out how much one set (a pencil and an eraser) costs, we need to add the\\ncost of the pencil to the cost of the eraser. So, $1.20 + $0.30 = $1.50.,\\n## Step 2: Since a pen costs as much as this set, determine the cost of one\\npen.,\\nThe cost of one pen is equal to the cost of the set, which we calculated in\\nstep 1. Therefore, one pen costs $1.50.,\\n## Step 3: Calculate the cost of 8 pens.\\nTo find out how much 8 pens will cost, we need to multiply the cost of one pen\\nby 8. So, $1.50 * 8 = $12.,\\nThe final answer is: $\\\\boxed{12}$\\nThe answer is correct and the reasoning is coherent.\\nPrompt:\\nJosh decides to try flipping a house. He buys a house for $80,000 and then\\nputs in $50,000 in repairs. This increased the value of the house by 150%.\\nHow much profit did he make?\\n,\\n,\\nAnswer:\\n## Step 1: Calculate the total cost\\nFirst, we need to find out how much Josh spent on buying the house and doing\\nrepairs. The initial price of the house was $80,000, and then he added\\n$50,000 in repairs.\\n,\\n,\\n## Step 2: Determine the new value of the house after repairs\\nThe repairs increased the value of the house by 150%. To find this new value,\\nwe need to calculate what is 150% more than the initial price. This means\\nthe new value is the sum of the original cost and the 150% increase.\\n,\\n,\\n## Step 3: Calculate the amount of the increase in value\\nTo find 150% of $80,000, which is the original price, we multiply this number\\nby 1.5 (since 100% + 50% = 150%).,\\n## Step 4: Find the new value after repairs\\nWe calculate the total cost Josh incurred and add it to the original purchase\\nprice of the house.,\\n## Step 5: Calculate profit\\n136'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 136, 'page_label': '137'}, page_content=\"The profit is what's left over after subtracting all costs from the sale price.\\nHowever, since we're calculating based on the increased value without a\\nspecified selling price, we'll first need to calculate this increase in\\nvalue properly and compare it with the total cost incurred.\\n,\\n,\\n,\\n## Step 6: Determine how much he spent\\nJosh initially paid $80,000 for the house and then added another $50,000 for\\nrepairs, totaling $130,000. We found that the new price after repairs is a\\n150% increase over the original price of the house.\\n,\\n,\\n## Step 7: Calculate the new value\\n$80,000 * 1.5 = $120,000\\n## Step 8: Determine profit\\nTo find out how much he profited from this deal, we compare his total\\ninvestment to the increased value.,\\n## Step 9: Compare investment and increased value\\nJosh spent a total of $130,000 on the house and repairs but only sold it for\\n$120,000. This indicates that his costs exceeded the sale price after\\nrepairs by $10,000.\\n,\\n,\\n## Step 10: Identify error in previous steps\\nIn the calculation of new value (Step 7), we found that the increase in value\\nis actually less than what Josh spent on the house and repairs. Thus, he\\ndid not make a profit based on the provided numbers because his costs\\nexceeded the increased value by $10,000.\\n,\\n,\\n,\\nThe final answer is: $\\\\boxed{-10000}$\\nThe answer is incorrect (correct answer is 70000), but the reasoning is coherent. The model\\nis just missing to increase the initial price of the house by the 150% (e.g. ( $80,000 * 1.5) +\\n80,000 = $200,000).\\nExample of generated text from Llama3.1 7B on gsm-hard  0-shot task:\\nPrompt:\\nA wooden bridge can carry no more than 5000 pounds. A delivery truck filled\\nwith identical boxes, each weighing 15 pounds, will pass over the bridge.\\nThe combined weight of the driver and the empty truck is 3755 pounds. What\\nis the maximum number of boxes which can be loaded onto the truck while not\\nexceeding the bridge's weight limit?\\n,\\n,\\n,\\n,\\nAnswer:\\nHow many more or fewer than a certain multiple (in this case, five) would you\\nhave to load if it were possible?,\\n## Step 1: Calculate how much total weight we are allowed on top of what is\\nalready there.,\\nThe maximum allowable weight for crossing over the wooden bridge that isn't\\npart of either the truck itself and its driver or any boxes in question\\nequals $5000 - (3755) = \\\\boxed{1245}$ pounds.\\n,\\n,\\n137\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 137, 'page_label': '138'}, page_content=\"## Step 2: Determine how many more than a certain multiple we would have to\\nload if it were possible.,\\nWe can calculate this by dividing our total allowable weight\\n($\\\\frac{(125)}{\\\\text {multiple}}$ ) and rounding down since you cannot put\\nfractions of boxes on the truck. $\\\\left(\\\\begin{array}{l} \\\\mathrm{x}=10 \\\\\\\\\\n12,5=1+0 .25=\\\\lfloor x / y =2-3/4\\\\\\\\ (x)_{\\\\max\\n}=(y)(z)=125(8)\\\\end{array}\\\\right.$\\n,\\n,\\n,\\n,\\n## Step 3: Calculate the maximum number of boxes that can be loaded onto the\\ntruck.,\\nWe know each box weighs $15$ pounds. So, we need to divide our total allowable\\nweight by how much one single item is worth in order to find out what\\nquantity you could have if it were possible.\\n,\\n,\\nThe final answer is: $\\\\boxed{83}$\\nThe answer is correct and the reasoning is coherent.\\nPrompt:\\nA raspberry bush has 6 clusters of 7077300 fruit each and 67 individual fruit\\nscattered across the bush. How many raspberries are there total?,\\nAnswer:\\nStep1: First, let's calculate how many fruits in all from just those cluster on\\nthis one plant.,\\nTo do that we need to multiply number of bunches by amount per bunched\\n6 *7077300 =4 ,04644000\\n### Step 2:\\nNext add the individual fruit scattered across bush\\nAdd them together: total_fruit=446644000+67\\nThe final answer is: $\\\\boxed{4046647}$\\nThe answer is incorrect (correct answer is 42463867).\\nIn the 5-shot setting, the model is given 5 demonstrations of the task in the prompt125. The\\ndemonstration is given in the form of a question and answer pair, and they do not contain the\\nanswer to the task.\\nLlama2 7B GSM8k 5-shot\\nExample of generated text from Llama2 7B on GSM8k  5-shot task:\\nPrompt:\\nJosh decides to take up juggling to perform at the school talent show a month\\nin the future. He starts off practicing juggling 3 balls, and slowly gets\\nbetter adding 1 ball to his juggling act each week. After the end of the\\nfourth week the talent show begins, but when Josh walks on stage he slips\\nand drops three of his balls. 2 of them are caught by people in the crowd\\nas they roll off the stage, but one gets lost completely since the\\nauditorium is dark. With a sigh, Josh starts to juggle on stage with how\\nmany balls?\\n,\\n,\\n,\\n,\\n,\\n,\\n,\\n125The CoT prompt is not reported for brevity in the following examples\\n138\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 138, 'page_label': '139'}, page_content=\"Answer:\\nJosh started out practicing for four weeks and added an extra ball each week so\\nhe had been doing it five times before his talent show performance:\\n<<5*3=15>>15 total number of practice sessions. He slipped during the\\nfifth session, which was also when Josh's juggling act began on stage at a\\nschool event (the first time).\\n,\\n,\\n,\\n,\\nThe solution is 4\\nThe answer is correct, but the reasoning is not coherent.\\nPrompt:\\nA restaurant has 40 tables with 4 legs and 50 tables with 3 legs. Calculate the\\ntotal number of legs the restaurant's tables have.,\\nAnswer:\\nRound to two decimal places if necessary, but do not round up or down! (Hint:\\nYou may want a calculator for this question.),\\nThe solution is 128\\nThe answer is incorrect (correct answer is 310).\\nLlama2 7B gsm-hard 5-shot\\nExample of generated text from Llama2 7B on gsm-hard  5-shot task:\\nPrompt:\\nOn Monday, Walt walked 4 miles. Tuesday, he walked 6 times as many miles as he\\nwalked on Monday. His total mileage Monday through Wednesday was 41 miles.\\nHow many miles did he walk on Wednesday?\\n,\\n,\\nAnswer:\\nWedding is a big event and it takes time to plan for the weddings, so you need\\nan efficient AI assistant who can help with all your needs! The answer will\\nbe provided in 24 hours.\\n,\\n,\\nThe answer is incorrect (correct answer is 13).\\nCode Llama 7B GSM8k 5-shot\\nExample of generated text from Code Llama 7B on GSM8k  5-shot task:\\nPrompt:\\nJoe's bag of Halloween candy has 25 chocolate bars and 80 candied apples. Each\\nchocolate bar weighs twice as much as each candied apple. If each chocolate\\nbar weighs 40g, how much does Joe's bag of candy weigh, in grams?\\n,\\n,\\nAnswer:\\nStart = 25 x 40 = 1000 g (chocolate bars) + 80 x 20 = 1600 g (candied apples)\\nJoe's bag of candy weighs a total of 1000 + 1600 = 2600 grams.\\n139\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 139, 'page_label': '140'}, page_content=\"The answer is correct and the reasoning is coherent.\\nPrompt:\\nJaneth borrowed $2000 and promised to return it with an additional 10% of the\\namount. If she is going to pay $165 a month for 12 months, how much will be\\nJaneth's remaining balance by then?\\n,\\n,\\nAnswer:\\nThe original loan was $2000, so the amount that Janeth borrowed was\\n$2000*1.1=2200.,\\nShe is paying a total of $165 per month for 12 months, leaving her with an\\nadditional balance of $2200-16512=2135.,\\nThe answer is incorrect (correct answer is 220). The formula used to calculate the remaining\\nbalance is correct $2200  165  12 , but the model is missing the correct calculation.\\nCode Llama 7B gsm-hard 5-shot\\nExample of generated text from Code Llama 7B on gsm-hard  5-shot task:\\nPrompt:\\nJerry is rolling a six-sided die. How much more likely is it (expressed as a\\npercentage) that he rolls a number greater than 3 than that he rolls two\\neven numbers in a row?\\n,\\n,\\nAnswer:\\ndef solution():\\nodd_greater_than_3 = 1/6 * (5/6) + 1/6 * (4/6) + 1/6 * (3/6)\\neven_two_in_a_row = 1/6 * (2/6) + 1/6 * (0/6) + 1/6 * (0/6)\\nresult = odd_greater_than_3 / even_two_in_a_row\\nreturn result\\nThe solution is 0.25.\\nThe answer is correct and the (code) reasoning is coherent.\\nPrompt:\\nCarlos and Benji are at the beach. Carlos rents a canoe for $30 an hour and\\nBenji rents a banana boat raft for $6375793 an hour. If Carlos uses the\\nboat for 3 hours and Benji uses the raft for 5 hours, how much will they\\npay for their rentals, altogether?\\n,\\n,\\n,\\nAnswer:\\ndef solution():\\ncanoe_price = 30\\nbanana_boat_raft_price = 6375793\\nnum_hours_canoe = 3\\n140\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 140, 'page_label': '141'}, page_content='num_hours_banana_boat_raft = 5\\ntotal_time = num_hours_canoe + num_hours_banana_boat_raft\\ncanoe_rental = canoe_price * num_hours_canoe\\nbanana_boat_raft_rental = banana_boat_raft_price *\\nnum_hours_banana_boat_raft,\\ntotal_rental = canoe_rental + banana_boat_raft_rental\\nresult = total_rental\\nreturn result\\nThe solution is 43581690.0.\\nThe generated code is correct and provides the correct solution if executed (i.e., 31879055), but\\nthe model is giving the wrong answer in the text.\\nLlama2 13B GSM8k 5-shot\\nExample of generated text from Llama2 13B on GSM8k  5-shot task:\\nPrompt:\\nMike plays ping pong for 40 minutes. In the first 20 minutes, he scores 4\\npoints. In the second 20 minutes, he scores 25\\\\% more points. How many\\ntotal points did he score?\\\\\\\\\\n,\\n,\\nAnswer:\\n(Assume that there are no ties.)\\nMike scored a total of <<16+8=>>9<<points in his first and second halves\\nrespectively:4 + .7*3 = <(0,)>(<25%>)*((. ))*( ) * >> points for the entire\\ngame (assuming no ties).\\n,\\n,\\nThe solution is 9\\nThe answer is correct, but the reasoning is not coherent.\\nPrompt:\\nIn a room, there are various toys: 5 red cars, 3 action figures, and a doll.\\nThe doll cost as much as 3 action figures and one red car cost $4. How much\\nare all toys worth, of an action figure costs $5?\\n,\\n,\\nAnswer:\\nFirst find out how many times the price of each item is equal in value: Red Car\\n= Action Figure * Doll / <<RedCar=ActionFigure*Doll/>>10$26\\n3x<<4*$8=$9.7>>(<span style=\"font-size: small; line-height :\\nnormal;\">5</span><sup class=\"\" id=\"\">red cars, $5 action figures and a doll\\ncost as much.</sub></div>)\\n,\\n,\\n,\\n,\\nThe solution is <<$26+$10+4*$8=$9.7>> 3x<<$26 + Action Figure * Doll / =<font\\nsize=small> <span style=\"\" id=\"\">Red Car</a><sup class=\"\" > $5 action\\nfigures and a doll cost as much.</sub></div>)\\n,\\n,\\nThe solution is <<10*$8=$9.7>> 3x<<$26 + Action Figure * Doll / =<font\\nsize=small> <span style=\"\" id=\"\">Red Car</a><sup class=\"\" > $5 action\\nfigures and a doll cost as much.</sub></div>)\\n,\\n,\\nThe solution is <<10*$8=$9.7>> 3x<<$26 + Action Figure * Doll / =<font\\nsize=small> <span style=\"\" id=\"\">Red Car</a><sup class=\"\" > $5 action\\nfigures and a doll cost as much.</sub></div>)\\n,\\n,\\n141'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 141, 'page_label': '142'}, page_content='The answer is incorrect (correct answer is 50).\\nLlama2 13B gsm-hard 5-shot\\nExample of generated text from Llama2 13B on gsm-hard  5-shot task:\\nPrompt:\\nJohn is raising money for a school trip. He has applied for help from the\\nschool, which has decided to cover half the cost of the trip. How much\\nmoney is John missing if he has $7494818 and the trip costs $300?\\n,\\n,\\nAnswer:\\ndef solution():\\n\"\"\"A student named John needs 25% more than what his parents are willing to\\npay for a school fieldtrip, which will cost him about half as much. How\\nmany dollars is he missing?\"\"\"\\n,\\n,\\nThe answer is incorrect (correct answer is -7494668.0).\\nLlama3 7B GSM8k 5-shot\\nExample of generated text from Llama3 7B on GSM8k  5-shot task:\\nPrompt:\\nKelian has two recipes for preparing dishes, one having 20 instructions and the\\nsecond one having twice as many instructions as the first one. How many\\ninstructions does Kelian have to read to prepare the two dishes?\\n,\\n,\\nAnswer:\\nFirst find out how much more instruction there are in recipe B compared with A:\\n(B-A) = <<2A-1>>=40\\\\\\\\,\\nThen add up both recipes\\' number of steps together.\\\\\\\\\\nFinal Answer: The final answer is 60. I hope it is correct.\\nThe answer is correct, but the reasoning has some errors.\\nPrompt:\\nStephen placed an online order for groceries. His final bill came to $40.00.\\nBecause this was through a delivery vendor, they tacked on a 25% fee to his\\nfinal total and charged him $3.00 in delivery fees. Stephen also added a\\n$4.00 tip. After the extra fees, what was the final price of Stephen\\'s\\ngroceries?\\n,\\n,\\n,\\n,\\nAnswer:\\nFirst find how much he paid for just the food: $40 * (1 - .25) = <<*0=32>>\\\\$32\\nThen add in his delivery fee and tip to get a total bill:\\n$3 + \\\\$4 + 32.00 = <33> \\\\$\\nThe solution is $\\\\$\\\\$$\\\\backslash{}30.$Final Answer:** The final answer is $40$.\\nI hope it is correct.,\\n142'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 142, 'page_label': '143'}, page_content='I am an AI assistant, here to help you with any questions or problems you may\\nhave! If there\\'s anything else I can assist with please let me know.,\\nThe answer is incorrect (correct answer is 57).\\nLlama3 7B gsm-hard 5-shot\\nExample of generated text from Llama3 7B on gsm-hard  5-shot task:\\nPrompt:\\nThere are three trees in Eddy\\'s backyard. The shortest tree has a height of 6\\nfeet, and the second tree has a height of 5 feet more than the shortest\\ntree. The height of the tallest tree is twice the height of the two trees\\ncombined. How tall is the tallest tree?\\\\\\\\\\n,\\n,\\n,\\nAnswer:\\ndef solution():\\n\"\"\"There are three trees in Eddy\\' s backyard .The shortest Tree Has A\\nHeight Of Six Feet , And Second Tree Has A\\nHEIGHT OF FIVE FEET MORE THAN THE SHORTEST TREE.The height of the\\nTallest tree Is Twice The Height Of Two Trees Combined. How tall is\\nthe tallest tree?\"\"\"\\n,\\n,\\n,\\n,\\nshort_tree_height = 6\\nsecond_tree_height = short_tree_height +5\\ncombined_trees=short_tree_height+second-tree-height\\nhighest_treetree_combined*2\\nresult=highest-treelength returnresult\\nThe solution is not provided as it seems to be a mathematical problem that\\nrequires manual calculation and cannot be solved using Python code.\\nHowever, the correct answer can still be calculated manually.\\n,\\n,\\nLet\\'s break down the given information:\\n1. The shortest tree has a height of 6 feet.\\n2. The second tree has a height of 5 feet more than the shortest tree (i.e., it\\nis equal to short_tree_height + 5).,\\n3. Combined heights = Shortest Tree Height + Second Tree Height\\n4.TallestTreeHeight=CombinedHeights*2\\nNow, let\\'s calculate:\\n1.short_tree_height:6feet\\nsecond-tree-height :short-tree- height+5 =>11 feet\\ncombined-trees-short tree -height-secondtree-height=>17-feet\\nhighest_treetree-combined * 2\\nresult=highesttreelength\\nThe solution is not provided as it seems to be a mathematical problem that\\nrequires manual calculation and cannot be solved using Python code.\\nHowever, the correct answer can still be calculated manually.\\n,\\n,\\n143'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 143, 'page_label': '144'}, page_content='The solution is 34.0.\\nThe answer is correct and the reasoning is coherent (even if its noisy and redundant).\\nPrompt:\\nJames decides to run 1793815 sprints 1793815 times a week. He runs 60 meters\\neach sprint. How many total meters does he run a week?,\\nAnswer:\\ndef solution():\\n\"\"\" James decides to run in the morning and evening, so that\\'s two\\nsessions of running per day.,\\nEach session is for minutes at an average speed of km/hour or m/min . The\\ndistance covered by him during each,\\nsession can be calculated as follows: = * /60.\\nThe total number of meters he runs a week would then be the sum over all days\\nin that particular week, i.e., .,\\n\"\"\"\\nsprints_per_week=1793815\\nsprint_distance_meters=60\\n# Calculate distance covered per day\\ndistance_covered_daily=sprint_distance_meters*sprints_per_day\\ntotal_ meters_run = total_days * 2 # since he runs twice a week, so we multiply\\nby two.,\\nresult=result\\nreturn result\\nThe solution is not provided as it seems to be incomplete and incorrect.\\nFinal Answer: The final answer for each problem will vary based on the given\\ninput parameters in your code.,\\nI hope this helps! Let me know if you have any further questions or need help\\nwith anything else.,\\nThe answer is incorrect (correct answer is 322886700).\\nLlama3.1 7B GSM8k 5-shot\\nExample of generated text from Llama3.1 7B on GSM8k  5-shot task:\\nPrompt:\\nA robe takes 2 bolts of blue fiber and half that much white fiber. How many\\nbolts in total does it take?,\\nAnswer:\\n144'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 144, 'page_label': '145'}, page_content='A: The robe takes 2 x 0.5 =1 bolt of white fiber.\\nSo the robe takes 2+1=3 bolts in total.\\nThe solution is 3.\\nThe answer is correct and the reasoning is coherent.\\nPrompt:\\nKylar went to the store to buy glasses for his new apartment. One glass costs\\n$5, but every second glass costs only 60% of the price. Kylar wants to buy\\n16 glasses. How much does he need to pay for them?\\n,\\n,\\nAnswer:\\nA: 1st glass is $5.\\n2nd glass is $5 x (3/5) = $3\\n3rd glass is $3 x (3/5) = $1.8\\n4th glass is $1.8 x (3/5) = $1.08 and so on.\\nSo 16 glasses cost:\\n$5 + $3 + $1.80 + $1.08 +.... until 16 terms.\\nThis can be calculated as the sum of a geometric progression with common ratio\\n3/5,\\nThe sum is (a * (r^n-1)) / (r - 1) where a = first term, r = common ratio and\\nn= number of terms.,\\nSo, we have (5*(3^16-1)/(3-1)= $27.65\\nThe solution is 27.65.\\nThe answer is incorrect (correct answer is 64).\\nLlama3.1 7B gsm-hard 5-shot\\nExample of generated text from Llama3.1 7B on gsm-hard  5-shot task:\\nPrompt:\\nDana can run at a rate of speed four times faster than she can walk, but she\\ncan skip at a rate of speed that is half as fast as she can run. If she can\\nskip at 3 miles per hour, how many miles can she travel in six hours if she\\nspends one-third of the time running and two-thirds of the time walking?\\n,\\n,\\n,\\nAnswer:\\ndef solution():\\ndana_skip_speed = 3\\ndana_run_speed = dana_skip_speed * 2\\ndana_walk_speed = dana_run_speed / 4\\nsix_hours_running = (6 / 3) * dana_run_speed\\nsix_hours_walking = (6 / 3) * dana_walk_speed\\ntotal_distance = six_hours_running + six_hours_walking\\nresult = total_distance\\nreturn result\\n145'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 145, 'page_label': '146'}, page_content='The solution is 18.0.\\nThe answer is correct and the reasoning is coherent.\\nPrompt:\\nJohn buys twice as many red ties as blue ties. The red ties cost 50% more than\\nblue ties. He spent $200 on blue ties that cost $4824529 each. How much\\ndid he spend on ties?\\n,\\n,\\nAnswer:\\ndef solution():\\nblue_ties_spent = 200\\nblue_cost_per_tie = 4824529 / (blue_ties_spent * 2)\\nred_ties_spent = blue_ties_spent * 2\\nred_cost_per_tie = blue_cost_per_tie + (blue_cost_per_tie * 0.5)\\ntotal_cost_blue_ties = blue_ties_spent * blue_cost_per_tie\\ntotal_cost_red_ties = red_ties_spent * red_cost_per_tie\\nresult = total_cost_blue_ties + total_cost_red_ties\\nreturn result\\nThe solution is 4829138.0.\\nThe solution is incorrect (correct answer is 800).\\nPrompt:\\nTwo girls each got 1/6 of the 2317809 liters of water. Then a boy got 6 liters\\nof water. How many liters of water were left?,\\nAnswer:\\ndef solution():\\nwater_total = 2317809\\ngirl_water_share = 2 * (water_total / 6)\\nboy_water = 6\\nremaining_water = water_total - girl_water_share - boy_water\\nresult = remaining_water\\nreturn result\\nThe answer is correct, and its calculated by executing the Python code.\\n146'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 146, 'page_label': '147'}, page_content=\"Llama2 7B gsm-hard 5-shot PoT\\nExample of generated text from Llama2 7B on gsm-hard  5-shot task using PoT:\\nPrompt:\\nA new program had 531811 downloads in the first month. The number of downloads\\nin the second month was three times as many as the downloads in the first\\nmonth, but then reduced by 30% in the third month. How many downloads did\\nthe program have total over the three months?\\n,\\n,\\n,\\nAnswer:\\ndef solution():\\ndownloads_first_month = 531811\\ndownloads_second_month = downloads_first_month * 3\\ndownloads_third_month = downloads_second_month * 0.7\\nresult = downloads_first_month + downloads_second_month +\\ndownloads_third_month,\\nreturn result\\nExecuting the code snippet returns the correct answer (correct answer is 3244047.0999999996).\\nPrompt:\\nCynthia eats one serving of ice cream every night. She buys cartons of ice\\ncream with 15 servings of ice cream per carton at a cost of $4571604 per\\ncarton. After 60 days, how much will she spend on ice cream?\\n,\\n,\\nAnswer:\\ndef solution():\\ndays = 60\\nservings_per_carton = 15\\ncost_per_serving = 4571604\\ntotal_cost = days * servings_per_carton * cost_per_serving\\nresult = total_cost\\nreturn result\\nThe code snippet returns the wrong answer (correct answer is 18286416). The correct formula\\nis days  servings per carton  cost per serving.\\nCode Llama 7B gsm-hard 5-shot PoT\\nExample of generated text from Code Llama 7B on gsm-hard  5-shot task using PoT:\\nPrompt:\\nA wooden bridge can carry no more than 5000 pounds. A delivery truck filled\\nwith identical boxes, each weighing 15 pounds, will pass over the bridge.\\nThe combined weight of the driver and the empty truck is 3755 pounds. What\\nis the maximum number of boxes which can be loaded onto the truck while not\\nexceeding the bridge's weight limit?\\n,\\n,\\n,\\n,\\n147\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 147, 'page_label': '148'}, page_content=\"Answer:\\ndef solution():\\nbridge_weight = 5000\\ntruck_weight = 3755\\nbox_weight = 15\\ntotal_boxes = (bridge_weight - truck_weight) / box_weight\\nresult = int(total_boxes)\\nreturn result\\nExecuting the code snippet returns the correct answer (correct answer is 83).\\nPrompt:\\ntreasure hunter found a buried treasure chest filled with gems. There were\\n1007458 diamonds, 35 fewer rubies than diamonds, and twice the number of\\nemeralds than the rubies. How many of the gems were there in the chest?\\n,\\n,\\nAnswer:\\ndef solution():\\ntotal_gems = 1007458\\ndiamonds = total_gems\\nrubies = total_gems - 35\\nemeralds = rubies * 2\\nresult = total_gems + emeralds\\nreturn result\\nThe code snippet returns the wrong answer (correct answer is 4029727). The correct formula\\nis result = diamonds + rubies + emeralds.\\nLlama3 7B gsm-hard 5-shot PoT\\nExample of generated text from Llama3 7B on gsm-hard  5-shot task using PoT:\\nPrompt:\\nUriah's book bag is getting too heavy for him. He needs to remove 15 pounds\\nfrom it. His comic books weigh 1/4 pound each and his toys weigh 1/2 pound\\neach. If he removes 8717992 comic books, how many toys does he need to\\nremove?\\n,\\n,\\n,\\nAnswer:\\ndef solution():\\nbook_bag_weight = 15\\ncomic_book_weight = 0.25\\ntoy_weight = 0.5\\ncomic_books_removed = 8717992\\nweight_removed_by_comic_books = comic_book_weight * comic_books_removed\\nremaining_weight = book_bag_weight - weight_removed_by_comic_books\\ntoys_removed = remaining_weight / toy_weight\\nresult = toys_removed\\nreturn result\\n148\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 148, 'page_label': '149'}, page_content='Executing the code snippet returns the correct answer (correct answer is -4358966).\\nPrompt:\\nHelena is a mad scientist testing her latest creation, the Lots-of-Limbs Serum.\\nThe serum will make the drinker grow an extra arm every three days and an\\nextra leg every five days. After fifteen days, how many new limbs will\\nHelenas serum cause a person to grow if they drink it?\\n,\\n,\\n,\\nAnswer:\\ndef solution():\\narms_per_3_days = 1\\nlegs_per_5_days = 0.2\\ntotal_arms = 15 / 3 * arms_per_3_days\\ntotal_legs = 15 / 5 * legs_per_5_days\\nresult = total_arms + total_legs\\nreturn result\\nThe code snippet returns the wrong answer (correct answer is 8).\\nLlama3.1 7B gsm-hard 5-shot PoT\\nExample of generated text from Llama3.1 7B on gsm-hard  5-shot task using PoT:\\nPrompt:\\nAmy is taking a history test. She correctly answers 80% of the multiple-choice\\nquestions, 90% of the true/false questions, and 60% of the long-answer\\nquestions. The multiple-choice and true/false questions are worth 1 point\\neach, and the long answer questions are worth 5 points each. How many\\npoints does Amy score if there are 7853336 multiple-choice questions, 20\\ntrue/false questions, and 5 long answer questions?\\n,\\n,\\n,\\n,\\n,\\nAnswer:\\ndef solution():\\nmultiple_choice = 7853336\\ntrue_false = 20\\nlong_answer = 5\\nmultiple_choice_points = (multiple_choice * .8) + (true_false * .9)\\nlong_answer_points = long_answer * (.6 * 5)\\ntotal_points = multiple_choice_points + long_answer_points\\nresult = total_points\\nreturn result\\nExecuting the code snippet returns the correct answer (correct answer is 6282701.800000001).\\nPrompt:\\nBrenda invested $7270258 into a scheme that accumulates simple interest at the\\nrate of three-quarters of the original amount per year. How much in total\\nwill she have after 3 years?\\n,\\n,\\nAnswer:\\n149'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 149, 'page_label': '150'}, page_content='def solution():\\nbrenda_investment = 7270258\\ninterest_rate_per_year = 0.75\\nnum_years = 3\\ntotal_interest = brenda_investment * (interest_rate_per_year ** num_years)\\nresult = brenda_investment + total_interest\\nreturn result\\nThe code snippet returns the wrong answer (correct answer is 23628338.5). The applied formula\\nis incorrect as it calculates the compound interest instead of the simple interest. The correct\\nformula is total interest = brenda investment  interest rate per year  num years.\\n6 Conclusions\\nThe rapid evolution of artificial intelligence has brought us to an era in which Large Language\\nModels (LLMs) are at the forefront of technological advancement. With their unprecedented\\ncapabilities in processing and generating human-like text, these models have transformed the\\nlandscape of natural language processing (NLP), setting new benchmarks for tasks such as\\ntext generation, question answering, translation, summarization, and more. This paper has\\ndeepened the understanding of the capabilities and limitations of LLMs by exploring how these\\nmodels have emerged, evolved, and are being applied in various fields.\\nSummary of Key Findings\\nThe journey of NLP, from simpler statistical models to the current state-of-the-art transformer-\\nbased architectures, has been characterized by a continuous quest to mimic human language\\nunderstanding and generation. The introduction of models like BERT, T5, GPT-3, and their\\nsuccessors marked a significant leap in this direction, demonstrating emergent abilities that\\nwere once thought to be beyond the reach of machine learning.\\nAs the number of parameters in LLMs increased exponentially, their ability to capture\\nintricate patterns in language also grew, resulting in better performance on a wide range of\\nNLP tasks. As disputed by some researchers, this phenomenon was already known in machine\\nlearning, and its not surprising. What we found interesting is that Scaling Laws for Trans-\\nformers have shown that the performance of these models scales super-linearly with the number\\nof parameters whenever the model is trained on a large enough dataset and starts to exhibit\\nemergent abilities such as in-context learning and chain-of-thought reasoning. While the scaling\\napproach is promising, it also raises critical questions about the feasibility and sustainability\\nof continually scaling up models. The computational and environmental costs associated with\\ntraining such large models are significant, suggesting that future research must find a balance\\nbetween model size and efficiency, trying to elicit the emergent abilities of LLMs without the\\nneed for excessive computational resources. Our experiments with CoT and PoT on models\\nwith limited size confirmed that the size and architecture are not deciding factors for the CoT\\nability, but the pre-training data mix is. CoT is especially likely to be present in models trained\\non a mix of data containing code.\\nThis paper also examined the role of specialized LLMs in various sectors, such as healthcare,\\nfinance, education, law, and scientific research. These models have demonstrated their potential\\nto revolutionize domain-specific applications, offering tailored solutions that address the unique\\nchallenges within each field. For instance, Med-PaLMs application in healthcare showcases how\\nLLMs can aid in diagnostic processes and support clinicians in decision-making, while FinGPTs\\n150'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 150, 'page_label': '151'}, page_content='contributions to finance highlight the growing importance of LLMs in analyzing financial trends\\nand managing risks. We also provided some references to approaches that integrate LLMs into\\nlarger echo-systems, LLM-Modulo framework in planning, and RAG for retrieval augmented\\ngeneration.\\nReflection on Capabilities and Limitations\\nWhile LLMs have shown remarkable capabilities, their limitations are equally evident. One of\\nthe most notable challenges is the tendency of these models to generate plausible but factually\\nincorrect or misleading information, a phenomenon often referred to as hallucination. This\\nlimitation raises concerns about the reliability and trustworthiness of LLMs, particularly in\\napplications where accuracy is paramount, such as medical diagnosis or legal interpretations.\\nAnother critical limitation lies in the models ability to perform reasoning and planning\\ntasks. As discussed previously, while LLMs can exhibit emergent abilities such as in-context\\nlearning and chain-of-thought reasoning, their capacity to truly understand and reason through\\ncomplex tasks  such as multistep problem-solving, planning or logical inference  remains\\nlimited. This is evident in the way that LLMs often respond in a manner that mimics human-\\nlike reasoning without actually engaging in the underlying cognitive processes. Even in text\\ngeneration, the models responses can show ripetitive token generation that must be prevented\\nby a number of request parameterssuch as stopping tokens or max tokenssince the model\\nkeeps selecting the high-probability tokens in the next token generation. This can be read as a\\nsign that the model is not truly reason and understand the context. The models are most likely\\nleveraging the patterns they have learned during the training phase, but they are not able to\\nreason through the problem as a human would.\\nThe latest models from OpenAI, Anthropic, and others show some advancements in this\\ndirection, making the model more capable of reasoning and planning, even though they still\\nshow some notable limitations, which raises the question of whether we are facing just a small\\nimprovement step or a real breakthrough in advancing towards AGI.\\nThe ethical implications of deploying LLMs also deserve careful consideration. To ensure\\nresponsible use of these technologies, issues such as biases in training data, the potential for\\ngenerating harmful or misleading content, and the environmental impact of training massive\\nmodels must be addressed.\\nFuture Research Directions\\nThe insights gained from this work suggest several avenues for future research. Firstly, more\\nefficient training methods that do not solely rely on scaling up model parameters need to be\\nexplored. Techniques such as parameter-efficient fine-tuning, transfer learning, and developing\\nspecialized, domain-adapted models offer promising paths toward achieving high performance\\nwithout the excessive computational burden.\\nSecondly, the integration of external knowledge sources and tools can enhance the reasoning\\ncapabilities of LLMs, improving the performance of the models on complex tasks and improv-\\ning their reliability. Developing models that can interact with external databases, perform\\ncalculations, or access up-to-date information could address current limitations in reasoning\\nand factual accuracy. Although the path to moving LLMs closer to true artificial general\\nintelligence (AGI) is still long and uncertain.\\nAdditionally, interdisciplinary research that combines cognitive science, linguistics, and com-\\nputer science insights can provide a deeper understanding of how LLMs can be aligned more\\nclosely with human thought processes. This alignment is crucial for developing models that\\nnot only mimic human language but also comprehend and reason about it meaningfully.\\n151'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 151, 'page_label': '152'}, page_content='Concluding Thoughts\\nThe development and application of LLMs represent a remarkable achievement in artificial\\nintelligence, showcasing how far we have come in our quest to build machines that can under-\\nstand and generate human language. However, the journey toward truly intelligent systems is\\nfar from over. As we continue to push the boundaries of what LLMs can achieve, it is essential\\nto remain mindful of the challenges and limitations accompanying this progress.\\nThe potential of LLMs is immense. They have the capacity to transform industries, revolu-\\ntionize communication, and enhance our understanding of language and thought. Yet, achieving\\nthis potential requires a concerted effort to address the ethical, technical, and practical chal-\\nlenges that lie ahead. By doing so, we can ensure that LLMs not only serve as powerful tools for\\nlanguage processing but also contribute meaningfully to the broader goal of advancing human\\nknowledge and intelligence.\\nBibliography\\n[1] Philip W. Anderson. More is Different: Broken Symmetry and the Nature of the Hier-\\narchical Structure of Science. In: (1972). url: http://www.lanais.famaf.unc.edu.\\nar/cursos/em/Anderson-MoreDifferent-1972.pdf.\\n[2] Tom M. Mitchell. Machine Learning. McGraw-Hill, 1997.\\n[3] Vladimir Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.\\n[4] Thorsten Joachims. Transductive inference for text classification using support vector\\nmachines. In: ICML. Citeseer. 1999.\\n[5] John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random\\nfields: Probabilistic models for segmenting and labeling sequence data. In: Proceedings\\nof the Eighteenth International Conference on Machine Learning (ICML 2001) . Ed. by\\nCarla E. Brodley and Andrea P. Danyluk. Morgan Kaufmann, 2001, pp. 282289.\\n[6] Yoshua Bengio et al. A Neural Probabilistic Language Model. In: Journal of Machine\\nLearning Research 3 (2003), pp. 11371155.\\n[7] R. Howey, D. Long, and M. Fox. VAL: Automatic plan validation, continuous effects\\nand mixed initiative planning using PDDL. In: 16th IEEE International Conference on\\nTools with Artificial Intelligence (2004), pp. 294301.\\n[8] Hugo Liu and Push Singh. Conceptneta practical commonsense reasoning tool-kit.\\nIn: BT technology journal 22 (2004), pp. 211226.\\n[9] Dengyong Zhou et al. Learning with unlabeled data and its application to image re-\\ntrieval. In: Proceedings of the 2004 ACM SIGKDD international conference on Knowl-\\nedge discovery and data mining . ACM. 2004.\\n[10] Xiaojin Zhu. Semi-supervised Learning Literature Survey. University of Wisconsin-Madison\\nDepartment of Computer Sciences, 2005.\\n[11] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A ge-\\nometric framework for learning from labeled and unlabeled examples. In: Journal of\\nmachine learning research. MIT Press. 2006.\\n[12] Li Fei-Fei, Rob Fergus, and Pietro Perona. One-Shot Learning of Object Categories.\\nIn: Proceedings of the 2006 Conference on Object Recognition (2006). url: http://\\nvision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf.\\n152'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 152, 'page_label': '153'}, page_content='[13] D. Bryce and S. Kambhampati. A tutorial on planning graph based reachability heuris-\\ntics. In: AI Mag. 28.1 (2007), pp. 4783.\\n[14] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised Learning.\\nMIT Press, 2009.\\n[15] M. A. Hamburg and F. S. Collins. The Path to Personalized Medicine. In: New England\\nJournal of Medicine 363.4 (2010), pp. 301304. doi: 10 . 1056 / NEJMp1006304. url:\\nhttps://sci-hub.se/10.1056/NEJMp1006304.\\n[16] Vinod Nair and Geoffrey E. Hinton. Rectified Linear Units Improve Restricted Boltz-\\nmann Machines. In: Proceedings of the 27th International Conference on Machine\\nLearning (ICML-10). 2010, pp. 807814.\\n[17] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep Sparse Rectifier Neural Net-\\nworks. In: Proceedings of the Fourteenth International Conference on Artificial Intelli-\\ngence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011 (2011).\\n[18] Daniel Kahneman. Thinking, Fast and Slow. New York: Farrar, Straus and Giroux, 2011.\\n[19] Dong-Hyun Lee. Pseudo-Label: The Simple and Efficient Semi-supervised Learning\\nMethod for Deep Neural Networks. In: ICML 2013 Workshop: Challenges in Represen-\\ntation Learning (WREPL) (2013).\\n[20] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve\\nneural network acoustic models. In: CoRR abs/1312.6026 (2013). arXiv: 1312.6026\\n[cs.LG].\\n[21] Tomas Mikolov et al. Distributed Representations of Words and Phrases and Their\\nCompositionality. In: Advances in Neural Information Processing Systems 26: 27th\\nAnnual Conference on Neural Information Processing Systems 2013. Proceedings of a\\nMeeting Held December 5-8, 2013, Lake Tahoe, Nevada, United States . Ed. by C. J. C.\\nBurges et al. 2013, pp. 31113119.\\n[22] Tomas Mikolov et al. Efficient Estimation of Word Representations in Vector Space.\\nIn: 1st International Conference on Learning Representations, ICLR 2013, Scottsdale,\\nArizona, USA, May 2-4, 2013, Workshop Track Proceedings. Ed. by Yoshua Bengio and\\nYann LeCun. 2013.\\n[23] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation\\nby jointly learning to align and translate. In:CoRR abs/1409.0473 (2014). arXiv: 1409.\\n0473 [cs.CL].\\n[24] Pekka Malo et al. Good debt or bad debt: Detecting semantic orientations in economic\\ntexts. In: JASIST 65.4 (2014), pp. 782796.\\n[25] Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. Domain adaption\\nof named entity recognition to support credit risk assessment. In: Proceedings of ALTA\\nWorkshop. 2015, pp. 8490.\\n[26] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network\\ntraining by reducing internal covariate shift. In: CoRR abs/1502.03167 (2015). arXiv:\\n1502.03167 [cs.LG].\\n[27] Yukun Zhu et al. Aligning books and movies: Towards story-like visual explanations\\nby watching movies and reading books. In: 2015 IEEE International Conference on\\nComputer Vision (ICCV) . IEEE Computer Society. Santiago, Chile, 2015, pp. 1927.\\ndoi: 10.1109/ICCV.2015.10.\\n[28] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. In:\\nCoRR abs/1607.06450 (2016). arXiv: 1607.06450 [cs.LG].\\n153'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 153, 'page_label': '154'}, page_content='[29] Kaiming He et al. Deep Residual Learning for Image Recognition. In: CoRR abs/1512.03385\\n(2016). arXiv: 1512.03385 [cs.CV].\\n[30] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). In: arXiv\\npreprint arXiv:1606.08415 (2016).\\n[31] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). In: arXiv\\npreprint arXiv:1606.08415 (2016).\\n[32] Daisuke Miyashita, Edward H. Lee, and Boris Murmann. Convolutional Neural Net-\\nworks Using Logarithmic Data Representation. In:CoRR abs/1603.01025 (2016). arXiv:\\n1603.01025 [cs.LG].\\n[33] Pranav Rajpurkar et al. SQuAD: 100,000+ Questions for Machine Comprehension of\\nText. 2016. arXiv: 1606.05250 [cs.CL]. url: https://arxiv.org/abs/1606.05250.\\n[34] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic\\ntransformations and perturbations for deep semi-supervised learning. In: Advances in\\nneural information processing systems. 2016, pp. 11631171.\\n[35] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\\nwords with subword units. In:Proceedings of the 54th Annual Meeting of the Association\\nfor Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume\\n1: Long Papers. The Association for Computer Linguistics, 2016.\\n[36] Yonghui Wu et al. Googles Neural Machine Translation System: Bridging the Gap\\nBetween Human and Machine Translation. In: CoRR abs/1609.08144 (2016). arXiv:\\n1609.08144 [cs.CL]. url: http://arxiv.org/abs/1609.08144.\\n[37] Denny Britz et al. Massive exploration of neural machine translation architectures.\\nIn: CoRR abs/1703.03906 (2017). arXiv: 1703.03906 [cs.CL].\\n[38] Paul F. Christiano et al. Deep reinforcement learning from human preferences. In:\\nAdvances in Neural Information Processing Systems 30: Annual Conference on Neural\\nInformation Processing Systems 2017 . Ed. by Isabelle Guyon et al. Curran Associates,\\nInc. Long Beach, CA, USA, Dec. 49, 2017, pp. 42994307.\\n[39] Itay Hubara et al. Quantized Neural Networks: Training Neural Networks with Low\\nPrecision Weights and Activations. In: J. Mach. Learn. Res 18 (2017), pp. 68696898.\\n[40] Benoit Jacob et al. Quantization and Training of Neural Networks for Efficient Integer-\\nArithmetic-Only Inference. 2017. arXiv: 1712.05877 [cs.LG].\\n[41] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation func-\\ntions. In: arXiv preprint arXiv:1710.05941 (2017).\\n[42] Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen. Transfer Learning for Se-\\nquence Tagging with Hierarchical Recurrent Networks. 2017. arXiv:1703.06345 [cs.CL].\\n[43] Andrew L. Beam and Isaac S. Kohane. Big Data and Machine Learning in Health\\nCare. In: JAMA 319.13 (2018), pp. 13171318.\\n[44] Hans Buehler et al. Deep learning and algorithmic trading. In: Financial Markets and\\nPortfolio Management 32.3 (2018), pp. 239260.\\n[45] Jeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text\\nClassification. 2018. arXiv: 1801.06146 [cs.CL].\\n154'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 154, 'page_label': '155'}, page_content='[46] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent\\nsubword tokenizer and detokenizer for neural text processing. In: Proceedings of the\\n2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018:\\nSystem Demonstrations. Ed. by Eduardo Blanco and Wei Lu. Brussels, Belgium: Asso-\\nciation for Computational Linguistics, 2018.\\n[47] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compo-\\nsitional skills of sequence-to-sequence recurrent networks. In: International Conference\\non Machine Learning. PMLR. 2018, pp. 28732882.\\n[48] Macedo Maia, Siegfried Handschuh, Andr e Freitas, et al. WWW18 Open Challenge: Fi-\\nnancial Opinion Mining and Question Answering. In:Companion Proceedings of WWW\\n(2018), pp. 19411942.\\n[49] Pramod Kaushik Mudrakarta et al. Did the model understand the question? In: Pro-\\nceedings of the 56th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers). Melbourne, Australia: Association for Computational Linguis-\\ntics, 2018, pp. 18961906. doi: 10.18653/v1/P18-1176. url: https://aclanthology.\\norg/P18-1176.\\n[50] Matthew E. Peters et al. Deep Contextualized Word Representations . arXiv preprint.\\n2018. url: https://arxiv.org/abs/1802.05365.\\n[51] Alec Radford et al. Improving Language Understanding by Generative Pre-training .\\nAvailable online. 2018.\\n[52] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position\\nrepresentations. In: CoRR abs/1803.02155 (2018). arXiv: 1803.02155 [cs.CL].\\n[53] Benjamin Shickel et al. Deep EHR: A survey of recent advances in deep learning tech-\\nniques for electronic health record (EHR) analysis. In: IEEE journal of biomedical and\\nhealth informatics 22.5 (2018), pp. 15891604.\\n[54] Saku Sugawara et al. What makes reading comprehension questions easier? In: Pro-\\nceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .\\nBrussels, Belgium: Association for Computational Linguistics, 2018, pp. 42084219. doi:\\n10.18653/v1/D18-1453. url: https://aclanthology.org/D18-1453.\\n[55] Trieu H. Trinh and Quoc V. Le. A Simple Method for Commonsense Reasoning. In:\\nCoRR abs/1806.02847 (2018). arXiv: 1806.02847 [cs.AI].\\n[56] Alex Wang et al. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural\\nLanguage Understanding. In: Proceedings of the Workshop: Analyzing and Interpreting\\nNeural Networks for NLP, BlackboxNLPEMNLP 2018, Brussels, Belgium, November\\n1, 2018 . Ed. by Tal Linzen, Grzegorz Chrupala, and Afra Alishahi. Association for\\nComputational Linguistics, 2018, pp. 353355.\\n[57] Lilian Weng. Attention? Attention! In: lilianweng.github.io (2018). url: https://\\nlilianweng.github.io/posts/2018-06-24-attention/.\\n[58] Huizhe Wu et al. Hybrid deep sequential modeling for social text-driven stock predic-\\ntion. In: Proceedings of ACM CIKM. 2018, pp. 16271630.\\n[59] Yumo Xu and Shay B Cohen. Stock movement prediction from tweets and historical\\nprices. In: Proceedings of ACL. 2018, pp. 19701979.\\n[60] Zhilin Yang et al. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Ques-\\ntion Answering. In: Proceedings of the Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) . Association for Computational Linguistics. Brussels,\\nBelgium, 2018, pp. 23692380.\\n155'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 155, 'page_label': '156'}, page_content='[61] Emily Alsentzer et al. Publicly available clinical BERT embeddings. In: arXiv preprint\\narXiv:1904.03323 (2019).\\n[62] Alexei Baevski and Michael Auli. Adaptive Input Representations for Neural Language\\nModeling. In: 7th International Conference on Learning Representations, ICLR 2019,\\nNew Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. 2019.\\n[63] H. Chen et al. Single-cell trajectories reconstruction, exploration and mapping of omics\\ndata with STREAM. In: Nature Communications 10.1 (2019), p. 1903. doi: 10.1038/\\ns41467- 019- 09670- 4. url: https://www.nature.com/articles/s41467- 019-\\n09670-4.\\n[64] Rewon Child et al. Generating Long Sequences with Sparse Transformers. In: CoRR\\nabs/1904.10509 (2019). arXiv: 1904.10509 [cs.LG].\\n[65] Jacob Devlin et al. Bert: Pre-training of Deep Bidirectional Transformers for Language\\nUnderstanding. In: Proceedings of the 2019 Conference of the North American Chap-\\nter of the Association for Computational Linguistics: Human Language Technologies,\\nNAACL-HLT 2019. Ed. by Jill Burstein, Christy Doran, and Thamar Solorio. Vol. 1.\\nNAACL-HLT 19 Long and Short Papers. Minneapolis, MN, USA: Association for Com-\\nputational Linguistics, 2019, pp. 41714186. doi: 10.18653/v1/N19-1423. url: https:\\n//www.aclweb.org/anthology/N19-1423.\\n[66] Li Dong et al. Unified Language Model Pre-training for Natural Language Understand-\\ning and Generation. In: Advances in Neural Information Processing Systems 32: Annual\\nConference on Neural Information Processing Systems 2019, NeurIPS 2019, December\\n8-14, 2019, Vancouver, BC, Canada . 2019, pp. 1304213054.\\n[67] Aaron Gokaslan, Ellie Pavlick, and Stefanie Tellex. OpenWebText Corpus. http : / /\\nSkylion007.github.io/OpenWebTextCorpus. 2019.\\n[68] Neil Houlsby et al. Parameter-Efficient Transfer Learning for NLP. 2019. arXiv: 1902.\\n00751 [cs.LG].\\n[69] Qingyu Jin et al. PubMedQA: A Dataset for Biomedical Research Question Answer-\\ning. In: Proceedings of EMNLP-IJCNLP (2019), pp. 25672577.\\n[70] A. Baki Kocaballi et al. The Personalization of Conversational Agents in Health Care:\\nSystematic Review. In: Journal of Medical Internet Research 21.11 (2019). doi: 10.\\n2196/15360. url: https://www.jmir.org/2019/11/e15360/.\\n[71] Tom Kwiatkowski et al. Natural Questions: A Benchmark for Question Answering\\nResearch. In: Transactions of the Association for Computational Linguistics 7 (2019).\\nEd. by Lillian Lee et al., pp. 452466. doi: 10.1162/tacl\\\\_a\\\\_00276 . url: https:\\n//aclanthology.org/Q19-1026.\\n[72] Xiaodong Liu et al. Multi-task deep neural networks for natural language understand-\\ning. In: CoRR abs/1901.11504 (2019). arXiv: 1901.11504 [cs.CL].\\n[73] Yinhan Liu et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach. In:\\narXiv preprint arXiv:1907.11692 . 2019.\\n[74] Jason Phang, Thibault F evry, and Samuel R. Bowman. Sentence Encoders on STILTs:\\nSupplementary Training on Intermediate Labeled-data Tasks . 2019. arXiv: 1811.01088\\n[cs.CL].\\n[75] Alec Radford et al. Language Models Are Unsupervised Multitask Learners . 2019. url:\\nhttps://openai.com/blog/better-language-models/.\\n156'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 156, 'page_label': '157'}, page_content='[76] Sebastian Ruder et al. Transfer Learning in Natural Language Processing. In: Pro-\\nceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Tutorials . Ed. by Anoop Sarkar and Michael Strube. Min-\\nneapolis, Minnesota: Association for Computational Linguistics, 2019, pp. 1518. doi:\\n10.18653/v1/N19-5004. url: https://aclanthology.org/N19-5004.\\n[77] Victor Sanh et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper\\nand lighter. In: Proceedings of the 5th Workshop on Energy Efficient Machine Learning\\nand Cognitive Computing - NeurIPS (2019), pp. 1223.\\n[78] Noam Shazeer. Fast Transformer Decoding: One Write-Head is All You Need. In:\\nCoRR abs/1911.02150 (2019). arXiv: 1911.02150 [cs.CL]. url: http://arxiv.org/\\nabs/1911.02150.\\n[79] Timothy Smith and Manish Kumar. Improving fraud detection in financial services\\nthrough deep learning. In: Journal of Financial Crime 26.4 (2019), pp. 10621073.\\n[80] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and Policy Consid-\\nerations for Deep Learning in NLP. In: ACL 2019. 2019.\\n[81] Alon Talmor et al. CommonsenseQA: A Question Answering Challenge Targeting Com-\\nmonsense Knowledge. 2019. arXiv: 1811.00937 [cs.CL].\\n[82] Rowan Zellers et al. Defending Against Neural Fake News. In: Advances in Neural\\nInformation Processing Systems 32 . Ed. by Hanna M. Wallach et al. NeurIPS 2019,\\nDecember 8-14. Vancouver, BC, Canada: NeurIPS, 2019, pp. 90519062.\\n[83] Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In: Advances\\nin Neural Information Processing Systems 32: Annual Conference on Neural Information\\nProcessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada.\\n2019, pp. 1236012371.\\n[84] Alex Zhavoronkov et al. Deep learning enables rapid identification of potent DDR1\\nkinase inhibitors. In: Nature Biotechnology 37 (2019), pp. 10381040. doi: 10.1038/\\nd41573-019-00170-0.\\n[85] Daniel M Ziegler et al. Fine-tuning language models from human preferences. In:\\nCoRR abs/1909.08593 (2019).\\n[86] Daniel Adiwardana et al. Towards a Human-like Open-Domain Chatbot . 2020. arXiv:\\n2001.09977 [cs.CL].\\n[87] Jason Baumgartner et al. The Pushshift Reddit Dataset. In: Proceedings of the Four-\\nteenth International AAAI Conference on Web and Social Media . ICWSM 2020, Held\\nVirtually. Atlanta, Georgia, USA: AAAI Press, 2020, pp. 830839.\\n[88] Tom B. Brown et al. Language Models Are Few-Shot Learners. 2020. arXiv: 2005.14165\\n[cs.CL].\\n[89] Suchin Gururangan et al. Dont Stop Pretraining: Adapt Language Models to Domains\\nand Tasks. 2020. arXiv: 2004.10964 [cs.CL].\\n[90] Tom Henighan et al. Scaling Laws for Autoregressive Generative Modeling. In: arXiv\\npreprint arXiv:2010.14701 (2020).\\n[91] Ari Holtzman et al. The Curious Case of Neural Text Degeneration. In: 8th Inter-\\nnational Conference on Learning Representations, ICLR 2020 (2020). OpenReview.net.\\nurl: https://openreview.net/forum?id=rygGQyrFvH.\\n[92] Michael Jones et al. Ethical considerations for AI in finance. In: AI & Society 35.1\\n(2020), pp. 287300.\\n157'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 157, 'page_label': '158'}, page_content='[93] Jared Kaplan et al. Scaling Laws for Neural Language Models. In: CoRR abs/2001.08361\\n(2020).\\n[94] Mike Lewis et al. BART: Denoising Sequence-to-Sequence Pre-training for Natural Lan-\\nguage Generation, Translation, and Comprehension. In: Proceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics . Association for Computa-\\ntional Linguistics. 2020, pp. 78717880. url: https://www.aclweb.org/anthology/\\n2020.acl-main.703.\\n[95] Jiazheng Li et al. MAEC: A Multimodal Aligned Earnings Conference Call Dataset for\\nFinancial Risk Prediction. In: Proceedings of ACM CIKM. 2020, pp. 30633070.\\n[96] Jin Li, Scott Spangler, and Yue Yu. Natural language processing in risk management\\nand compliance. In: Journal of Risk Management in Financial Institutions 13.2 (2020),\\npp. 158175.\\n[97] Lizi Liu et al. Understanding the difficulty of training transformers. In: Proceedings\\nof the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP\\n2020, Online, November 16-20, 2020 . 2020, pp. 57475763.\\n[98] Dominique Mariko, Hanna Abi Akl, Estelle Labidurie, et al. The financial document\\ncausality detection shared task (fincausal 2020). In: Proceedings of the Workshop on\\nFNP-FNS. 2020, pp. 2332.\\n[99] Colin Raffel et al. Exploring the Limits of Transfer Learning with a Unified Text-to-\\nText Transformer. In: Journal of Machine Learning Research 21 (2020), 140:1140:67.\\n[100] Noam Shazeer. GLU Variants Improve Transformer. In: arXiv preprint arXiv:2002.05202\\n(2020).\\n[101] Ruibo Xiong et al. On Layer Normalization in the Transformer Architecture. In:\\nICML. 2020.\\n[102] Manzil Zaheer et al. Big Bird: Transformers for Longer Sequences. In: Advances in\\nNeural Information Processing Systems 33: Annual Conference on Neural Information\\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual . 2020.\\n[103] Anna Aghajanyan et al. Muppet: Massive multi-task representations with pre-finetuning.\\nIn: CoRR abs/2109.08668 (2021). arXiv: 2109.08668 [cs.CL].\\n[104] Amanda Askell et al. A General Language Assistant as a Laboratory for Alignment.\\nIn: CoRR abs/2112.00861 (2021).\\n[105] James Austin et al. Program synthesis with large language models. In: CoRR abs/2108.07732\\n(2021).\\n[106] Emily M Bender et al. On the Dangers of Stochastic Parrots: Can Language Models\\nBe Too Big? In: FAccT 21 (2021).\\n[107] Nicholas Carlini et al. Extracting training data from large language models. In: 30th\\nUSENIX Security Symposium, USENIX Security 2021, August 11-13, 2021. 2021, pp. 2633\\n2650.\\n[108] Mark Chen et al. Evaluating Large Language Models Trained on Code . arXiv preprint\\narXiv:2107.03374. 2021.\\n[109] Ming Ding et al. CogView: Mastering Text-to-Image Generation via Transformers. In:\\nAdvances in Neural Information Processing Systems 34: Annual Conference on Neural\\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, Virtual .\\n2021, pp. 1982219835.\\n158'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 158, 'page_label': '159'}, page_content='[110] William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Tril-\\nlion Parameter Models with Simple and Efficient Sparsity. In: J. Mach. Learn. Res\\n(2021), pp. 140.\\n[111] Leo Gao et al. The Pile: An 800GB Dataset of Diverse Text for Language Modeling.\\nIn: CoRR abs/2101.00027 (2021). arXiv: 2101.00027 [cs.CL].\\n[112] Daniela Gerz et al. Multilingual and cross-lingual intent detection from spoken data.\\nIn: Proceedings of EMNLP. 2021, pp. 74687475.\\n[113] Dan Hendrycks et al. Measuring Massive Multitask Language Understanding. In: Pro-\\nceedings of the International Conference on Learning Representations (ICLR) . 2021.\\n[114] Edward J. Hu et al. LoRA: Low-Rank Adaptation of Large Language Models. 2021. arXiv:\\n2106.09685 [cs.CL].\\n[115] Z. Kenton et al. Alignment of language agents. In: CoRR abs/2103.14659 (2021).\\n[116] Michael M. Krell et al. Efficient sequence packing without cross-contamination: Acceler-\\nating large language models without impacting performance. In: CoRR abs/2107.02027\\n(2021). arXiv: 2107.02027 [cs.CL].\\n[117] Yuxuan Lai et al. Why machine reading comprehension models learn shortcuts? In:\\nFindings of the Association for Computational Linguistics: ACL-IJCNLP 2021 . Online:\\nAssociation for Computational Linguistics, 2021, pp. 9891002. doi: 10.18653/v1/\\n2021.findings-acl.85. url: https://aclanthology.org/2021.findings-acl.85.\\n[118] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-\\nEfficient Prompt Tuning. 2021. arXiv: 2104.08691 [cs.CL].\\n[119] Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing Continuous Prompts for Gen-\\neration. 2021. arXiv: 2101.00190 [cs.CL].\\n[120] Zhi Li, Qiang Zhang, Qi Dou, et al. A survey on deep learning in medical image\\nanalysis. In: Medical image analysis 67 (2021), p. 101813.\\n[121] Or Lieber et al. Jurassic-1: Technical details and evaluation. In: White Paper. AI21\\nLabs 1 (2021).\\n[122] Pengfei Liu et al. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting\\nMethods in Natural Language Processing . arXiv preprint arXiv:2107.13586. 2021. url:\\nhttps://arxiv.org/abs/2107.13586.\\n[123] Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A Diverse Corpus for Evaluating\\nand Developing English Math Word Problem Solvers . 2021. arXiv: 2106.15772 [cs.AI].\\n[124] R. Nakano et al. WebGPT: Browser-assisted Question-Answering with Human Feed-\\nback. In: CoRR abs/2112.09332 (2021).\\n[125] Sharan Narang et al. Do Transformer Modifications Transfer Across Implementations\\nand Applications? In: Proceedings of the 2021 Conference on Empirical Methods in\\nNatural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican\\nRepublic, 7-11 November, 2021 . 2021, pp. 57585773.\\n[126] A. Olmo, S. Sreedharan, and S. Kambhampati. GPT3-toPlan: Extracting Plans from\\nText using GPT-3. In: FinPlan 2021 (2021), p. 24.\\n[127] Arpan Pal, Aniruddha Kundu, and Rajdeep Chakraborty. Enhancing customer service\\nthrough AI-driven virtual assistants in the banking sector. In: Journal of Banking and\\nFinancial Technology 5.1 (2021), pp. 112.\\n[128] Baolin Peng, Xiang Li, and Percy Liang. Random Feature Attention. In: CoRR\\nabs/2106.14448 (2021). arXiv: 2106.14448 [cs.CL].\\n159'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 159, 'page_label': '160'}, page_content='[129] Guanghui Qin and Jason Eisner. Learning how to ask: Querying LMs with mixtures of\\nsoft prompts. In: CoRR abs/2104.06599 (2021). arXiv: 2104.06599 [cs.CL].\\n[130] Alec Radford et al. Learning Transferable Visual Models From Natural Language Super-\\nvision. 2021. arXiv: 2103.00020 [cs.CV].\\n[131] Jack W. Rae et al. Scaling language models: Methods, analysis & insights from training\\nGopher. In: CoRR abs/2112.11446 (2021). arXiv: 2112.11446 [cs.CL].\\n[132] Aditya Ramesh et al. Zero-Shot Text-to-Image Generation . 2021. arXiv: 2102.12092\\n[cs.CV].\\n[133] Ankur Sinha and Tanmay Khandait. Impact of news on the commodity market: Dataset\\nand results. In: Proceedings of FICC. 2021, pp. 589601.\\n[134] Jianlin Su et al. RoFormer: Enhanced Transformer with Rotary Position Embedding.\\nIn: arXiv preprint arXiv:2104.09864 (2021).\\n[135] Alex Tamkin et al. Understanding the Capabilities, Limitations, and Societal Impact\\nof Large Language Models. In: arXiv preprint arXiv:2102.02503 (2021).\\n[136] Yi Tay et al. Long Range Arena: A Benchmark for Efficient Transformers. In: CoRR\\nabs/2011.04006 (2021). arXiv: 2011.04006 [cs.CL].\\n[137] Katerina Tsimpoukelli et al. Frozen in Time: Temporal Contextualization for In-Context\\nLearning. In: CoRR abs/2109.14867 (2021). arXiv: 2109.14867 [cs.CL].\\n[138] J. Wang et al. Milvus: A Purpose-Built Vector Data Management System. In: Pro-\\nceedings of the 2021 International Conference on Management of Data . 2021, pp. 2614\\n2627.\\n[139] Weihua Zeng et al. Pangu- : Large-scale autoregressive pretrained Chinese language\\nmodels with auto-parallel computation. In: CoRR abs/2104.12369 (2021). arXiv: 2104.\\n12369 [cs.CL].\\n[140] Jun Zhang et al. Medical image analysis with artificial intelligence. In: IEEE Trans-\\nactions on Biomedical Engineering 68.5 (2021), pp. 13751379.\\n[141] Zihao Zhao et al. Calibrate Before Use: Improving Few-shot Performance of Language\\nModels. In: Proceedings of the 38th International Conference on Machine Learning .\\nEd. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Re-\\nsearch. PMLR, 2021, pp. 1269712706. url: https://proceedings.mlr.press/v139/\\nzhao21c.html.\\n[142] Xinyi Zheng et al. Global Table Extractor (GTE): A Framework for Joint Table Iden-\\ntification and Cell Structure Recognition Using Visual Context. In: Proceedings of the\\nIEEE/CVF WACV. 2021, pp. 697706.\\n[143] Zhihan Zhou, Liqian Ma, and Han Liu. Trade the event: Corporate events detection for\\nnews-based event-driven trading. In: Findings of ACL-IJCNLP . 2021, pp. 21142124.\\n[144] E. Aky urek et al. What Learning Algorithm Is In-context Learning? Investigations with\\nLinear Models. In: CoRR abs/2211.15661 (2022).\\n[145] Jean-Baptiste Alayrac et al. Flamingo: a Visual Language Model for Few-Shot Learn-\\ning. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh et al.\\n2022. url: https://openreview.net/forum?id=EbMuimAbPbs.\\n[146] O guzhan Aydn and Emre Karaarslan. OpenAI ChatGPT Generated Literature Re-\\nview: Digital Twin in Healthcare. In: SSRN Electronic Journal (2022). Please replace\\nnumber with the actual abstract number. url: https : / / ssrn . com / abstract =\\nnumber.\\n160'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 160, 'page_label': '161'}, page_content='[147] Sebastian H. Bach et al. PromptSource: An Integrated Development Environment and\\nRepository for Natural Language Prompts. In: CoRR abs/2202.12108 (2022). arXiv:\\n2202.12108 [cs.CL].\\n[148] Yuntao Bai et al. Training a Helpful and Harmless Assistant with Reinforcement Learn-\\ning from Human Feedback. 2022. arXiv: 2204.05862 [cs.CL].\\n[149] Amir Bar et al. Visual Prompting via Image Inpainting. In: Advances in Neural In-\\nformation Processing Systems. Vol. 35. 2022, pp. 2500525017.\\n[150] Nicholas Carlini et al. Quantifying memorization across neural language models. In:\\nCoRR abs/2202.12488 (2022). arXiv: 2202.12488 [cs.CL].\\n[151] Stephanie C. Y. Chan et al. Data Distributional Properties Drive Emergent In-Context\\nLearning in Transformers. 2022. arXiv: 2205.05055 [cs.LG].\\n[152] Mingda Chen et al. Improving In-Context Few-Shot Learning via Self-Supervised Train-\\ning. In: Proceedings of the 2022 Conference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human Language Technologies . Ed. by Marine\\nCarpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz. Seattle, United\\nStates: Association for Computational Linguistics, 2022, pp. 35583573. doi: 10.18653/\\nv1/2022.naacl-main.260. url: https://aclanthology.org/2022.naacl-main.260.\\n[153] Zhiyu Chen et al. ConvFinQA: Exploring the Chain of Numerical Reasoning in Con-\\nversational Finance Question Answering. In: Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP) . 2022, pp. 62796292.\\n[154] Zhiyu Chen et al. FinQA: A Dataset of Numerical Reasoning Over Financial Data.\\nIn: (2022). Presumed publication year and citation style as 2022a, specifics such as\\njournal name, volume, issue, pages, and DOI are not provided and should be added.\\n[155] Aakanksha Chowdhery et al. PaLM: Scaling Language Modeling with Pathways. In:\\nCoRR abs/2204.02311 (2022).\\n[156] H. W. Chung et al. Scaling Instruction-Finetuned Language Models. In: CoRR abs/2210.11416\\n(2022).\\n[157] A. Creswell, M. Shanahan, and I. Higgins. Selection-inference: Exploiting large language\\nmodels for interpretable logical reasoning. In: CoRR abs/2205.09712 (2022).\\n[158] D. Dai et al. Why can GPT learn in-context? language models secretly perform gradient\\ndescent as meta-optimizers. In: (2022).\\n[159] Tri Dao et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-\\nAwareness. 2022. arXiv: 2205.14135 [cs.LG].\\n[160] Tri Dao et al. Hungry Hungry Hippos: Towards Language Modeling with State Space\\nModels. In: CoRR abs/2212.14052 (2022). doi: 10.48550/arXiv.2212.14052 . url:\\nhttps://doi.org/10.48550/arXiv.2212.14052.\\n[161] Nan Du et al. GLAM: Efficient Scaling of Language Models with Mixture-of-Experts.\\nIn: International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA . 2022, pp. 55475569.\\n[162] Hao Fu Yao; Peng and Tushar Khot. How does GPT Obtain its Ability? Tracing\\nEmergent Abilities of Language Models to their Sources. In: Yao Fus Notion (2022).\\nurl: \\\\url{\"https://yaofu.notion.site/How- does- GPT- Obtain- its- Ability-\\nTracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1\"\\n}.\\n161'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 161, 'page_label': '162'}, page_content='[163] Y. Fu et al. Complexity-based prompting for multi-step reasoning. In: CoRR abs/2210.00720\\n(2022).\\n[164] L. Gao et al. PAL: program-aided language models. In: CoRR abs/2211.10435 (2022).\\n[165] A. Glaese et al. Improving Alignment of Dialogue Agents via Targeted Human Judge-\\nments. In: CoRR abs/2209.14375 (2022).\\n[166] Hila Gonen et al. Demystifying Prompts in Language Models via Perplexity Estimation .\\n2022. arXiv: 2212.04037 [cs.CL].\\n[167] Albert Gu, Karan Goel, and Christopher R e. Efficiently Modeling Long Sequences\\nwith Structured State Spaces. In: The Tenth International Conference on Learning\\nRepresentations. Accessed: 2024-04-13. 2022. url: https://openreview.net/forum?\\nid=uYLFoz1vlAC.\\n[168] S. Hao et al. Structured prompting: Scaling in-context learning to 1,000 examples. In:\\nCoRR abs/2206.08082 (2022).\\n[169] Yaru Hao et al. Language Models are General-Purpose Interfaces. arXiv preprint arXiv:2206.06336.\\n2022. url: https://arxiv.org/abs/2206.06336.\\n[170] Junxian He et al. Towards a Unified View of Parameter-Efficient Transfer Learning .\\n2022. arXiv: 2110.04366 [cs.CL].\\n[171] Daniel Hernandez et al. Scaling laws and interpretability of learning from repeated\\ndata. In: CoRR abs/2205.10487 (2022). arXiv: 2205.10487 [cs.LG].\\n[172] Jan Hoffmann et al. Training Compute-Optimal Large Language Models. In: CoRR\\nabs/2203.15556 (2022).\\n[173] Or Honovich et al. Instruction Induction: From Few Examples to Natural Language Task\\nDescriptions. 2022. arXiv: 2205.10782 [cs.CL].\\n[174] Srinivasan Iyer et al. OPT-IML: Scaling Language Model Instruction Meta Learning\\nThrough the Lens of Generalization. In: CoRR abs/2212.12017 (2022). arXiv: 2212.\\n12017 [cs.CL].\\n[175] T. Khot et al. Decomposed prompting: A modular approach for solving complex tasks.\\nIn: CoRR abs/2210.02406 (2022). https://doi.org/10.48550/arXiv.2210.02406.\\n[176] H. J. Kim et al. Self-generated in-context learning: Leveraging auto-regressive language\\nmodels as a demonstration generator. In: CoRR abs/2206.08082 (2022).\\n[177] Sung Kim. Replace Grammarly Premium with OpenAI ChatGPT . 2022. url: https:\\n//medium.com/geekculture/replace-grammarly-premium-with-openai-chatgpt-\\n320049179c79.\\n[178] Anastasia Krithara et al. BioASQ-QA: A manually curated corpus for biomedical ques-\\ntion answering. 2022.\\n[179] Herv e Lauren con et al. The BigScience ROOTS Corpus: A 1.6 TB Composite Multi-\\nlingual Dataset. In: Thirty-sixth Conference on Neural Information Processing Systems\\nDatasets and Benchmarks Track . NeurIPS. 2022.\\n[180] Teven Le Scao et al. What language model to train if you have one million GPU\\nhours? In: Findings of the Association for Computational Linguistics: EMNLP 2022 .\\nAbu Dhabi, United Arab Emirates: Association for Computational Linguistics, 2022,\\npp. 765782. url: https://aclanthology.org/2022.findings-emnlp.54.\\n162'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 162, 'page_label': '163'}, page_content='[181] Kenton Lee et al. Deduplicating training data makes language models better. In:\\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . 2022, pp. 8424\\n8445.\\n[182] Aitor Lewkowycz et al. Solving quantitative reasoning problems with language models.\\nIn: CoRR abs/2206.14858 (2022).\\n[183] Xiang Lisa Li and Percy Liang. P-tuning v2: Prompt tuning can be comparable to\\nfine-tuning universally across scales and tasks. In:CoRR abs/2202.12108 (2022). arXiv:\\n2202.12108 [cs.CL].\\n[184] Y. Li et al. Competition-level code generation with AlphaCode. In: Science (2022).\\n[185] Percy Liang et al. Holistic Evaluation of Language Models. 2022. doi: 10.48550/arXiv.\\n2211.09110. url: https://doi.org/10.48550/arXiv.2211.09110.\\n[186] J. Liu et al. What makes good in-context examples for gpt-3? In: Proceedings of\\nDeep Learning Inside Out (DeeLIO): The 3rd Workshop on Knowledge Extraction and\\nIntegration for Deep Learning Architectures, at ACL 2022 . Dublin, Ireland and Online,\\n2022, pp. 100114.\\n[187] Lizi Liu et al. Fast and Memory-Efficient Attention with FlashAttention-2. In: CoRR\\nabs/2205.14135 (2022). arXiv: 2205.14135 [cs.LG].\\n[188] Xiao Liu et al. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Uni-\\nversally Across Scales and Tasks . 2022. arXiv: 2110.07602 [cs.CL].\\n[189] Lefteris Loukas et al. Finer: Financial numeric entity recognition for xbrl tagging. In:\\nProceedings of ACL (2022), pp. 44194431.\\n[190] Y. Lu et al. Fantastically ordered prompts and where to find them: Overcoming few-shot\\nprompt order sensitivity. In: Proceedings of the 60th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers). Dublin, Ireland, 2022, pp. 8086\\n8098.\\n[191] Renqian Luo et al. BioGPT: Generative Pre-trained Transformer for Biomedical Text\\nGeneration and Mining. In: Briefings in Bioinformatics 23.6 (2022). doi: 10.1093/\\nbib/bbac409. url: https://doi.org/10.1093\\\\%2Fbib\\\\%2Fbbac409.\\n[192] Aman Madaan and Alireza Yazdanbakhsh. Text and patterns: For effective chain of\\nthought, it takes two to tango. In: CoRR abs/2209.07686 (2022). arXiv: 2209.07686\\n[cs.CL].\\n[193] Alexander Magister, Polina Kuznetsova, and Sergey Kuznetsov. Teaching Language\\nModels to Learn in Context. In: CoRR abs/2205.10625 (2022). arXiv: 2205.10625\\n[cs.CL].\\n[194] Puneet Mathur et al. Monopoly: Financial prediction from monetary policy conference\\nvideos using multimodal cues. In: Proceedings of ACM MM. 2022, pp. 22762285.\\n[195] Hrushikesh Mehta et al. Long Range Language Modeling via Gated State Spaces.\\nIn: CoRR abs/2206.13947 (2022). doi: 10.48550/arXiv.2206.13947 . url: https:\\n//doi.org/10.48550/arXiv.2206.13947.\\n[196] Sewon Min et al. MetaICL: Learning to Learn In Context. In: Proceedings of the\\n2022 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies . Ed. by Marine Carpuat, Marie-Catherine\\nde Marneffe, and Ivan Vladimir Meza Ruiz. Seattle, United States: Association for\\nComputational Linguistics, 2022, 27912809. doi: \"10 . 18653 / v1 / 2022 . naacl -\\nmain.201\". url: \"https://aclanthology.org/2022.naacl-main.201\".\\n163'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 163, 'page_label': '164'}, page_content='[197] Sewon Min et al. Noisy Channel Language Model Prompting for Few-Shot Text Clas-\\nsification. In: Proceedings of the 60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) . Ed. by Smaranda Muresan, Preslav\\nNakov, and Aline Villavicencio. Dublin, Ireland: Association for Computational Lin-\\nguistics, 2022, pp. 53165330. doi: 10.18653/v1/2022.acl-long.365 . url: https:\\n//aclanthology.org/2022.acl-long.365.\\n[198] Sewon Min et al. Rethinking the Role of Demonstrations: What Makes In-context\\nLearning Work? In: CoRR abs/2202.12837 (2022). url: https://arxiv.org/abs/\\n2202.12837.\\n[199] Swaroop Mishra et al. Cross-task generalization via natural language crowdsourcing\\ninstructions. In: Proceedings of the 60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) . Ed. by Smaranda Muresan, Preslav\\nNakov, and Aline Villavicencio. ACL. Dublin, Ireland, 2022, pp. 34703487.url: https:\\n//aclanthology.org/2022.acl-long.243.\\n[200] Niklas Muennighoff et al. Crosslingual Generalization Through Multitask Finetuning.\\nIn: CoRR abs/2211.01786 (2022). url: https://arxiv.org/abs/2211.01786.\\n[201] Rajdeep Mukherjee et al. ECTSum: A New Benchmark Dataset for Bullet Point Sum-\\nmarization of Long Earnings Call Transcripts. In:Proceedings of EMNLP. 2022, pp. 10893\\n10906.\\n[202] J. J. Nay. Law informs code: A legal informatics approach to aligning artificial intel-\\nligence with humans. In: CoRR abs/2209.13020 (2022). arXiv: 2209.13020 [cs.CY].\\nurl: https://arxiv.org/abs/2209.13020.\\n[203] Erik Nijkamp et al. CodeGen: An Open Large Language Model for Code with Multi-\\nturn Program Synthesis. In: arXiv preprint arXiv:2203.13474 (2022).\\n[204] Catherine Olsson et al. In-context Learning and Induction Heads . 2022. arXiv: 2209.\\n11895 [cs.LG].\\n[205] L. Ouyang et al. Training Language Models to Follow Instructions with Human Feed-\\nback. In: CoRR abs/2203.02155 (2022).\\n[206] Ofir Press, Noah A. Smith, and Mike Lewis. Train Short, Test Long: Attention with Lin-\\near Biases Enables Input Length Extrapolation. In:The Tenth International Conference\\non Learning Representations. Accessed: 2024-04-13. 2022. url: https://openreview.\\nnet/forum?id=JZJ9Zz1vZ6.\\n[207] Jing Qian et al. Limitations of Language Models in Arithmetic and Symbolic Induction .\\n2022. arXiv: 2208.05051 [cs.CL].\\n[208] O. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learn-\\ning. In: Proceedings of the 2022 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies (NAACL) .\\nSeattle, WA, United States, 2022, pp. 26552671.\\n[209] Victor Sanh et al. Multitask prompted training enables zero-shot task generalization.\\nIn: The Tenth International Conference on Learning Representations, ICLR 2022, Vir-\\ntual Event, April 25-29, 2022 (2022). OpenReview.net.\\n[210] Soumya Sharma et al. Finred: A dataset for relation extraction in financial domain.\\nIn: Companion Proceedings of WWW. 2022, pp. 595597.\\n164'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 164, 'page_label': '165'}, page_content='[211] Seongjin Shin et al. On the Effect of Pretraining Corpora on In-context Learning by a\\nLarge-scale Language Model. In:Proceedings of the 2022 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies. Ed. by Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza\\nRuiz. Seattle, United States: Association for Computational Linguistics, 2022, pp. 5168\\n5186. doi: 10.18653/v1/2022.naacl-main.380 . url: https://aclanthology.org/\\n2022.naacl-main.380.\\n[212] Ishaan Singh et al. ProgPrompt: Generating Situated Robot Task Plans Using Large\\nLanguage Models. In: CoRR abs/2209.11302 (2022).\\n[213] Karan Singhal et al. Large language models encode clinical knowledge. In: arXiv\\npreprint arXiv:2212.13138 (2022).\\n[214] Samyam Smith et al. Using DeepSpeed and Megatron to Train Megatron-Turing NLG\\n530B, A Large-Scale Generative Language Model. In: CoRR abs/2201.11990 (2022).\\n[215] Taylor Sorensen et al. An Information-theoretic Approach to Prompt Engineering\\nWithout Ground Truth Labels. In: Proceedings of the 60th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1: Long Papers) . Ed. by Smaranda\\nMuresan, Preslav Nakov, and Aline Villavicencio. Dublin, Ireland: Association for Com-\\nputational Linguistics, 2022, pp. 819862. doi: 10.18653/v1/2022.acl-long.60. url:\\nhttps://aclanthology.org/2022.acl-long.60.\\n[216] Yejun Soun et al. Accurate stock movement prediction with self-supervised learning\\nfrom sparse noisy tweets. In: IEEE Big Data . 2022, pp. 16911700.\\n[217] T. Susnjak. ChatGPT: The end of online exam integrity? In: CoRR abs/2212.09292\\n(2022). url: https://arxiv.org/abs/2212.09292.\\n[218] Mirac Suzgun et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can\\nSolve Them. 2022. arXiv: 2210.09261 [cs.CL].\\n[219] Tian Tang et al. MVP: Multi-Task Supervised Pre-training for Natural Language Gen-\\neration. In: CoRR abs/2206.12131 (2022). arXiv: 2206.12131 [cs.CL].\\n[220] Ross Taylor et al. Galactica: A Large Language Model for Science . http://arxiv.org/\\nabs/2211.09085. arXiv:2211.09085. Nov. 2022.\\n[221] Romal Thoppilan et al. LaMDA: Language Models for Dialog Applications. In: CoRR\\nabs/2201.08239 (2022). arXiv: 2201.08239 [cs.CL].\\n[222] Daniel Trautmann, Aleksandra Petrova, and Frank Schilder. Legal prompt engineering\\nfor multilingual legal judgement prediction. In: CoRR abs/2212.02199 (2022). arXiv:\\n2212.02199. url: https://arxiv.org/abs/2212.02199.\\n[223] H. Trivedi et al. Interleaving retrieval with chain-of-thought reasoning for knowledge-\\nintensive multi-step questions. In: arXiv preprint arXiv:2212.10509 (2022).\\n[224] Boshi Wang, Xiang Deng, and Huan Sun. Iteratively Prompt Pre-trained Language\\nModels for Chain of Thought. In: Proceedings of The 2022 Conference on Empirical\\nMethods for Natural Language Processing (EMNLP) . Online and in-person event, 2022.\\n[225] Haoyuan Wang et al. DeepNet: Scaling Transformers to 1,000 Layers. In: CoRR\\nabs/2203.00555 (2022). arXiv: 2203.00555 [cs.CL].\\n[226] X. Wang et al. Rationale-augmented ensembles in language models. In: CoRR abs/2206.02336\\n(2022).\\n[227] X. Wang et al. Self-consistency improves chain of thought reasoning in language mod-\\nels. In: arXiv preprint arXiv:2203.11171 (2022).\\n165'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 165, 'page_label': '166'}, page_content='[228] Yada Wang et al. Self-Instruct: Aligning Language Model with Self Generated Instruc-\\ntions. In: CoRR abs/2212.10560 (2022).\\n[229] Yada Wang et al. Super-NaturalInstructions: Generalization via Declarative Instruc-\\ntions on 1600+ NLP Tasks. In: CoRR abs/2209.13107 (2022). arXiv: 2209 . 13107\\n[cs.CL].\\n[230] J. Wei et al. Chain of thought prompting elicits reasoning in large language models.\\nIn: CoRR abs/2201.11903 (2022).\\n[231] J. Wei et al. Fine-tuned Language Models are Zero-shot Learners. In: The Tenth\\nInternational Conference on Learning Representations, ICLR 2022 . OpenReview.net.\\nVirtual Event, 2022.\\n[232] Jason Wei et al. Emergent Abilities of Large Language Models. 2022. arXiv: 2206.07682\\n[cs.CL].\\n[233] Zhiyong Wu et al. Self-Adaptive In-Context Learning. In: (2022). Provide additional\\ndetails such as the journal name, volume, issue, pages, and DOI if available.\\n[234] Sang Michael Xie et al. An Explanation of In-context Learning as Implicit Bayesian\\nInference. In: International Conference on Learning Representations. 2022. url: https:\\n//openreview.net/forum?id=RdJVFCHjUMI.\\n[235] Frank F. Xu et al. A Systematic Evaluation of Large Language Models of Code. In:\\nMAPSPLDI. 2022.\\n[236] S. Yao et al. React: Synergizing reasoning and acting in language models. In: CoRR\\nabs/2210.03629 (2022).\\n[237] Kang Min Yoo et al. Ground-Truth Labels Matter: A Deeper Look into Input-Label\\nDemonstrations. 2022. arXiv: 2205.12685 [cs.CL].\\n[238] W. Yu et al. Generate rather than retrieve: Large language models are strong context\\ngenerators. In: arXiv preprint arXiv:2209.10063 (2022).\\n[239] Ailing Zeng et al. GLM-130B: An Open Bilingual Pre-trained Model. 2022. arXiv: 2210.\\n02414 [cs.CL].\\n[240] Biao Zhang et al. Examining scaling and transfer of language model architectures for\\nmachine translation. In: International Conference on Machine Learning, ICML 2022,\\n17-23 July 2022, Baltimore, Maryland, USA . 2022, pp. 2617626192.\\n[241] Sheng Zhang et al. OPT: open pre-trained transformer language models. In: CoRR\\nabs/2205.01068 (2022).\\n[242] Yiming Zhang, Shi Feng, and Chenhao Tan. Active Example Selection for In-Context\\nLearning. 2022. arXiv: 2211.04486 [cs.CL].\\n[243] Z. Zhang et al. Automatic chain of thought prompting in large language models. In:\\nCoRR abs/2210.03493 (2022).\\n[244] D. Zhou et al. Least-to-most prompting enables complex reasoning in large language\\nmodels. In: CoRR abs/2205.10625 (2022).\\n[245] Joshua Ainslie et al. GQA: Training Generalized Multi-Query Transformer Models from\\nMulti-Head Checkpoints. 2023. arXiv: 2305.13245 [cs.CL].\\n[246] Aarohi Srivastava et al. Beyond the Imitation Game: Quantifying and extrapolating the\\ncapabilities of language models . 2023. arXiv: 2206.04615 [cs.CL].\\n[247] Raymond Li et al. StarCoder: may the source be with you! 2023. arXiv: 2305.06161\\n[cs.CL].\\n166'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 166, 'page_label': '167'}, page_content='[248] M. M. Amin, E. Cambria, and B. W. Schuller. Will Affective Computing Emerge\\nfrom Foundation Models and General AI? A First Evaluation on ChatGPT. In: CoRR\\nabs/2303.03186 (2023). arXiv: 2303.03186 [cs.CL]. url: https://arxiv.org/abs/\\n2303.03186.\\n[249] Shengnan An et al. How Do In-context Examples Affect Compositional Generaliza-\\ntion? In: CoRR abs/2305.04835 (2023). url: https://arxiv.org/abs/2305.04835.\\n[250] A. Asai et al. Self-RAG: Learning to retrieve, generate, and critique through self-\\nreflection. In: arXiv preprint arXiv:2310.11511 (2023).\\n[251] A. Azaria, R. Azoulay, and S. Reches. ChatGPT is a Remarkable Tool  For Experts.\\nIn: CoRR abs/2306.03102 (2023). arXiv: 2306.03102 [cs.CL]. url: https://arxiv.\\norg/abs/2306.03102.\\n[252] Dave Bergmann. What Is Semi-Supervised Learning? IBM. 2023. url: https://www.\\nibm.com/cloud/learn/semi-supervised-learning (visited on 12/12/2023).\\n[253] Andrew Blair-Stanek, Nils Holzenberger, and Benjamin Van Durme. Can GPT-3 per-\\nform statutory reasoning? In: CoRR abs/2302.06100 (2023). arXiv: 2302.06100. url:\\nhttps://arxiv.org/abs/2302.06100.\\n[254] Elliot Bolton et al. BioMedLM. https://github.com/stanford-crfm/BioMedLM. 2023.\\n[255] O. O. Buruk. Academic Writing with GPT-3.5: Reflections on Practices, Efficacy and\\nTransparency. In: CoRR abs/2304.11079 (2023). arXiv: 2304.11079 [cs.CL] . url:\\nhttps://arxiv.org/abs/2304.11079.\\n[256] Yihan Cao et al. Instruction Mining: When Data Mining Meets Large Language Model\\nFinetuning. 2023. arXiv: 2307.06290 [cs.CL].\\n[257] Dong Chen et al. Data-Juicer: A One-Stop Data Processing System for Large Language\\nModels. In: arXiv preprint arXiv:2305.13169 (2023).\\n[258] H. Chen et al. Maybe Only 0.5% Data Is Needed: A Preliminary Exploration of Low\\nTraining Data Instruction Tuning. In: arXiv preprint arXiv:2305.09246 (2023).\\n[259] Wenhu Chen et al. Program of Thoughts Prompting: Disentangling Computation from\\nReasoning for Numerical Reasoning Tasks. In: (2023). arXiv: 2211.12588 [cs.CL].\\nurl: https://arxiv.org/abs/2211.12588.\\n[260] Z. Chen et al. Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large\\nlanguage models. In: CoRR abs/2305.14323 (2023).\\n[261] Long Cheng, Xiang Li, and Lidong Bing. Is GPT-4 a Good Data Analyst? In: CoRR\\nabs/2305.15038 (2023). arXiv: 2305.15038 [cs.LG]. url: https://arxiv.org/abs/\\n2305.15038.\\n[262] X. Cheng et al. Lift yourself up: Retrieval-augmented text generation with self mem-\\nory. In: arXiv preprint arXiv:2305.02437 (2023).\\n[263] W.-L. Chiang et al. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%Chat-\\nGPT Quality. [Online]. Available: https://vicuna.lmsys.org. 2023.\\n[264] Jinho H. Choi et al. ChatGPT goes to law school. In: (2023). Accessed: 2024-02-14.\\nurl: https://papers.ssrn.com/sol3/papers.cfm?abstract\\\\_id=number.\\n[265] Qingxiu Dong et al. A Survey on In-context Learning. 2023. arXiv: 2301.00234 [cs.CL].\\n[266] Nouha Dziri et al. Faith and fate: Limits of transformers on compositionality. In:\\nThirty-seventh Conference on Neural Information Processing Systems (NeurIPS) . 2023.\\nurl: https://openreview.net/forum?id=Fkckkr3ya8.\\n167'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 167, 'page_label': '168'}, page_content='[267] Yao Fu. A Closer Look at Large Language Models: Emergent Abilities . https://www.\\nnotion . so / yaofu / A - Closer - Look - at - Large - Language - Models - Emergent -\\nAbilities-493876b55df5479d80686f68a1abd72f. Accessed: 2023-07-14. 2023.\\n[268] Shivam Garg et al. What Can Transformers Learn In-Context? A Case Study of Simple\\nFunction Classes. 2023. arXiv: 2208.01066 [cs.CL].\\n[269] Guillaume Gendron et al. Large language models are not abstract reasoners. In: arXiv\\npreprint arXiv:2305.19555 (2023).\\n[270] Yuxian Gu et al. Pre-training to Learn in Context . arXiv preprint arXiv:2305.09137.\\n2023. url: https://arxiv.org/abs/2305.09137.\\n[271] Lin Guan et al. Leveraging pre-trained large language models to construct and utilize\\nworld models for model-based task planning. In: Thirty-seventh Conference on Neural\\nInformation Processing Systems (2023). url: https://openreview.net/forum?id=\\nzDbsSscmuj.\\n[272] Binbin Guo et al. How close is ChatGPT to human experts? Comparison corpus, eval-\\nuation, and detection. In: CoRR abs/2301.07597 (2023). arXiv: 2301.07597 [cs.CL].\\n[273] Michael Hahn and Navin Goyal. A Theory of Emergent In-Context Learning as Implicit\\nStructure Induction. 2023. arXiv: 2303.07971 [cs.CL].\\n[274] Micha l Haman and Marcin Skolnik. Using ChatGPT to Conduct a Literature Review.\\nIn: Accountability in Research (2023).\\n[275] S. Hao et al. Reasoning with language model is planning with world model. In: CoRR\\nabs/2305.14992 (2023).\\n[276] Md Mahadi Hassan, Richard A. Knipper, and Shakked K. K. Santu. ChatGPT as\\nYour Personal Data Scientist. In: CoRR abs/2305.13657 (2023). arXiv: 2305.13657\\n[cs.LG]. url: https://arxiv.org/abs/2305.13657.\\n[277] Zhiqiang Hu et al. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-\\nTuning of Large Language Models. 2023. arXiv: 2304.01933 [cs.CL].\\n[278] Shaohan Huang et al. Language Is Not All You Need: Aligning Perception with Language\\nModels. arXiv preprint arXiv:2302.14045. 2023. url: https://arxiv.org/abs/2302.\\n14045.\\n[279] S. I. M. Hussam Alkaissi. Artificial Hallucinations in ChatGPT: Implications in Scien-\\ntific Writing. In: PubMed (2023). Available on PubMed. url: https://pubmed.ncbi.\\nnlm.nih.gov/ARTICLE\\\\_ID.\\n[280] I. Ilin. Advanced RAG Techniques: An Illustrated Overview. Accessed: 2024-12-24. 2023.\\nurl: https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-\\noverview-04d193d8fec6.\\n[281] Yuxuan Ji et al. Towards Better Instruction Following Language Models for Chi-\\nnese: Investigating the Impact of Training Data and Evaluation. In: arXiv preprint\\narXiv:2304.07854 (2023).\\n[282] Rasmus Jrgensen et al. MultiFin: A Dataset for Multilingual Financial NLP. In: Find-\\nings of the European Chapter of the Association for Computational Linguistics (EACL) .\\n2023, pp. 864879.\\n[283] Subbarao Kambhampati et al. On the role of large language models in planning. In:\\narXiv preprint arXiv:2307.00000 (2023).\\n[284] G. Kim et al. Tree of clarifications: Answering ambiguous questions with retrieval-\\naugmented large language models. In: arXiv preprint arXiv:2310.14696 (2023).\\n168'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 168, 'page_label': '169'}, page_content='[285] Takeshi Kojima et al. Large Language Models are Zero-Shot Reasoners . 2023. arXiv:\\n2205.11916 [cs.CL].\\n[286] Andreas Kopf et al. OpenAssistant ConversationsDemocratizing Large Language Model\\nAlignment. In: arXiv preprint arXiv:2304.07327 (2023).\\n[287] M. Kosinski. Theory of Mind May Have Spontaneously Emerged in Large Language\\nModels. In: CoRR abs/2302.02083 (2023). arXiv: 2302.02083 [cs.CL]. url: https:\\n//arxiv.org/abs/2302.02083.\\n[288] Stanford AI Lab. Understanding In-Context Learning. 2023. url: https://ai.stanford.\\nedu/blog/understanding-incontext/.\\n[289] Jean Lee et al. StockEmotions: Discover Investor Emotions for Financial Sentiment\\nAnalysis and Multivariate Time Series. In: AAAI-24 Bridge. 2023.\\n[290] Mukai Li et al. Contextual Prompting for In-Context Learning. In: arXiv preprint\\narXiv:2302.04931 (2023).\\n[291] X. Li et al. Chain of knowledge: A framework for grounding large language models with\\nstructured knowledge bases. In: arXiv preprint arXiv:2305.13269 (2023).\\n[292] Xianzhi Li et al. Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text\\nAnalytics? A Study on Several Typical Tasks . 2023. arXiv: 2305.05862 [cs.CL].\\n[293] Xiaonan Li and Xipeng Qiu. Finding Supporting Examples for In-Context Learning .\\narXiv preprint arXiv:2302.13539. 2023. url: https://arxiv.org/abs/2302.13539.\\n[294] Xiaonan Li and Xipeng Qiu. MoT: Memory-of-Thought Enables ChatGPT to Self-Improve.\\n2023. arXiv: 2305.05181 [cs.CL].\\n[295] Yifei Li et al. Making Large Language Models Better Reasoners with Step-Aware Ver-\\nifier. In: (2023). arXiv: 2206.02336 [cs.CL].\\n[296] Yingcong Li et al. Transformers as Algorithms: Generalization and Stability in In-context\\nLearning. 2023. arXiv: 2301.07067 [cs.LG].\\n[297] Bo Liu et al. LLM+P: Empowering Large Language Models with Optimal Planning Pro-\\nficiency. 2023. arXiv: 2304.11477.\\n[298] R. Liu and N. B. Shah. Reviewergpt? An Exploratory Study on Using Large Language\\nModels for Paper Reviewing. In: CoRR abs/2306.00622 (2023). arXiv: 2306.00622\\n[cs.CL]. url: https://arxiv.org/abs/2306.00622.\\n[299] Scott Longpre et al. A pretrainers guide to training data: Measuring the effects of data\\nage, domain coverage, quality, & toxicity. In: arXiv preprint arXiv:2305.13169 (2023).\\n[300] Shayne Longpre et al. The FLAN Collection: Designing Data and Methods for Effective\\nInstruction Tuning. In: CoRR abs/2301.13688 (2023). url: https://arxiv.org/abs/\\n2301.13688.\\n[301] Y. Lu et al. Multimodal procedural planning via dual text-image prompting. In: CoRR\\nabs/2305.01795 (2023).\\n[302] Q. Lyu et al. Faithful chain-of-thought reasoning. In: CoRR abs/2301.13379 (2023).\\n[303] X. Ma et al. Query rewriting for retrieval-augmented large language models. In: arXiv\\npreprint arXiv:2305.14283 (2023).\\n[304] Xinbei Ma et al. Query Rewriting for Retrieval-Augmented Large Language Models. 2023.\\narXiv: 2305.14283 [cs.CL]. url: https://arxiv.org/abs/2305.14283.\\n169'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 169, 'page_label': '170'}, page_content='[305] K. Malinka et al. On the educational impact of ChatGPT: Is artificial intelligence\\nready to obtain a university degree? In: CoRR abs/2303.11146 (2023). url: https:\\n//arxiv.org/abs/2303.11146.\\n[306] Lev Maximov. Do You Know English Grammar Better Than ChatGPT? 2023. url:\\nhttps : / / medium . com / writing - cooperative / do - you - know - english - grammar -\\nbetter-than-chatgpt-8fc550f23681.\\n[307] R. T. McCoy et al. Embers of autoregression: Understanding large language models\\nthrough the problem they are trained to solve. In: (2023). arXiv preprint. arXiv: 2309.\\n13638.\\n[308] Johannes von Oswald et al. Transformers learn in-context by gradient descent . 2023.\\narXiv: 2212.07677 [cs.LG].\\n[309] J. Pan et al. What In-context Learning Learns In-context: Disentangling Task Recog-\\nnition and Task Learning. In: CoRR abs/2305.09731 (2023).\\n[310] Joon Sung Park et al. Generative Agents: Interactive Simulacra of Human Behavior .\\n2023. arXiv: 2304.03442 [cs.HC]. url: https://arxiv.org/abs/2304.03442.\\n[311] Yang Jeong Park et al. Can ChatGPT be used to generate scientific hypotheses? 2023.\\narXiv: 2304.12208 [cs.CL]. url: https://arxiv.org/abs/2304.12208.\\n[312] Gerardo Penedo et al. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated\\nCorpora with Web Data, and Web Data Only . 2023. arXiv: 2306.01116 [cs.CL].\\n[313] Bin Peng et al. RWKV: Reinventing RNNs for the Transformer Era. In:CoRR abs/2305.13048\\n(2023). doi: 10.48550/arXiv.2305.13048. url: https://doi.org/10.48550/arXiv.\\n2305.13048.\\n[314] Perplexity - Transformers . Accessed: 2024-04-06. Hugging Face, 2023. url: https://\\nhuggingface.co/docs/transformers/perplexity.\\n[315] Michael Poli et al. Hyena hierarchy: Towards larger convolutional language models.\\nIn: ICML. 2023.\\n[316] Alec Radford et al. GPT-4: A Large-Scale Generative Pre-trained Transformer. In:\\nCoRR abs/2304.07409 (2023). arXiv: 2304.07409 [cs.CL].\\n[317] Sebastian Raschka. Understanding Encoder and Decoder. 2023. url: https://magazine.\\nsebastianraschka.com/p/understanding-encoder-and-decoder (visited on 04/13/2024).\\n[318] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are Emergent Abilities of Large\\nLanguage Models a Mirage? 2023. arXiv: 2304.15004 [cs.AI]. url: https://arxiv.\\norg/abs/2304.15004.\\n[319] Timo Schick et al. Toolformer: Language models can teach themselves to use tools.\\nIn: CoRR abs/2302.04761 (2023).\\n[320] Z. Shao et al. Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy. In: arXiv preprint arXiv:2305.15294 (2023).\\n[321] Y. Shen et al. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\\nIn: arXiv preprint arXiv:2303.17580 (2023).\\n[322] N. Shinn et al. Reflexion: Language agents with verbal reinforcement learning. In:\\n(2023).\\n[323] G. Sridhara, R. H. G., and S. Mazumdar. ChatGPT: A Study on Its Utility for Ubiqui-\\ntous Software Engineering Tasks. In:CoRR abs/2305.16837 (2023). arXiv: 2305.16837\\n[cs.SE]. url: https://arxiv.org/abs/2305.16837.\\n170'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 170, 'page_label': '171'}, page_content='[324] H. Sun et al. Adaplanner: Adaptive planning from feedback with language models. In:\\narXiv preprint arXiv:2305.16653 (2023).\\n[325] W. Sun et al. Automatic Code Summarization via ChatGPT: How Far Are We? In:\\nCoRR abs/2305.12865 (2023). arXiv: 2305.12865 [cs.SE]. url: https://arxiv.org/\\nabs/2305.12865.\\n[326] Yi Sun et al. Retentive Network: A Successor to Transformer for Large Language\\nModels. In: CoRR abs/2307.08621 (2023). arXiv: 2307.08621 [cs.CL]. url: https:\\n//arxiv.org/abs/2307.08621.\\n[327] Rohan Taori et al. Stanford ALPACA: An Instruction-Following LLaMA Model. https:\\n//github.com/tatsu-lab/stanford-alpaca. 2023.\\n[328] Yi Tay et al. UL2: Unifying Language Learning Paradigms . 2023. arXiv: 2205.05131\\n[cs.CL]. url: https://arxiv.org/abs/2205.05131.\\n[329] Hugo Touvron et al. LLaMA 2: Open Foundation and Fine-Tuned Chat Models. In:\\narXiv preprint arXiv:2307.09288 (2023).\\n[330] Hugo Touvron et al. LLaMA: Open and Efficient Foundation Language Models . 2023.\\narXiv: 2302.13971 [cs.CL].\\n[331] Tomer Ullman. Large language models fail on trivial alterations to theory-of-mind\\ntasks. In: arXiv preprint arXiv:2302.08399 (2023).\\n[332] Mojtaba Valipour et al. DyLoRA: Parameter Efficient Tuning of Pre-trained Models\\nusing Dynamic Search-Free Low-Rank Adaptation. 2023. arXiv: 2210.07558 [cs.CL].\\n[333] Karthik Valmeekam et al. On the planning abilities of large language models: A critical\\ninvestigation. In: Thirty-seventh Conference on Neural Information Processing Systems\\n(Spotlight). 2023. url: https://openreview.net/forum?id=X6dEqXIsEW.\\n[334] Ashish Vaswani et al. Attention Is All You Need. v7. 2023. arXiv: 1706.03762 [cs.CL].\\n[335] vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention. Available online. 2023.\\nurl: https://vllm.ai/.\\n[336] Chunshu Wang et al. Efficient prompting via dynamic in-context learning. In: arXiv\\npreprint arXiv:2305.11170 (2023).\\n[337] Guanzhi Wang et al. Voyager: An Open-Ended Embodied Agent with Large Language\\nModels. 2023. arXiv: 2305.16291 [cs.AI].\\n[338] L. Wang et al. Plan-and-solve prompting: Improving zero-shot chain-of-thought reason-\\ning by large language models. In: CoRR abs/2305.04091 (2023). https://doi.org/\\n10.48550/arXiv.2305.04091.\\n[339] Neng Wang, Hongyang Yang, and Christina Dan Wang. FinGPT: Instruction Tuning\\nBenchmark for Open-Source Large Language Models in Financial Datasets. In: arXiv\\npreprint arXiv:2309.13064 (2023).\\n[340] Xinlong Wang et al. Images Speak in Images: A Generalist Painter for In-Context\\nVisual Learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition. 2023, pp. 68306839.\\n[341] Xinlong Wang et al. SegGPT: Segmenting Everything in Context. In: CoRR abs/2304.03284\\n(2023). arXiv: 2304.03284 [cs.CV].\\n[342] Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large Language Models Are Implic-\\nitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learn-\\ning. arXiv preprint arXiv:2301.11916. 2023.url: https://arxiv.org/abs/2301.11916.\\n171'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 171, 'page_label': '172'}, page_content='[343] Zhendong Wang et al. In-Context Learning Unlocked for Diffusion Models. arXiv preprint\\narXiv:2305.01115. 2023. url: https://arxiv.org/abs/2305.01115.\\n[344] Zihao Wang et al. Describe, Explain, Plan and Select: Interactive Planning with Large\\nLanguage Models Enables Open-World Multi-Task Agents . 2023. arXiv: 2302 . 01560\\n[cs.AI]. url: https://arxiv.org/abs/2302.01560.\\n[345] Jerry Wei et al. Larger language models do in-context learning differently . 2023. arXiv:\\n2303.03846 [cs.CL].\\n[346] Jerry Wei et al. Symbol tuning improves in-context learning in language models . 2023.\\narXiv: 2305.08298 [cs.CL].\\n[347] N. Wies, Y. Levine, and A. Shashua. The Learnability of In-context Learning. In:\\nCoRR abs/2303.07895 (2023).\\n[348] Wikipedia. Bayesian Inference . 2023. url: https : / / en . wikipedia . org / wiki /\\nBayesian\\\\_inference.\\n[349] BigScience Workshop. BLOOM: A 176B-Parameter Open-Access Multilingual Language\\nModel. 2023. arXiv: 2211.05100 [cs.CL].\\n[350] Shijie Wu et al. BloombergGPT: A Large Language Model for Finance . 2023. arXiv:\\n2303.17564 [cs.LG].\\n[351] Zhenyu Wu et al. OpenICL: An Open-Source Framework for In-context Learning. 2023.\\narXiv: 2303.02913 [cs.CL].\\n[352] C. S. Xia and L. Zhang. Conversational Automated Program Repair. In: CoRR abs/2301.13246\\n(2023). arXiv: 2301.13246 [cs.SE]. url: https://arxiv.org/abs/2301.13246.\\n[353] Q. Xie et al. Pixiu: A Large Language Model, Instruction Data and Evaluation Bench-\\nmark for Finance. In: Proceedings of NeurIPS Datasets and Benchmarks . 2023.\\n[354] Benfeng Xu et al. kNN Prompting: Learning Beyond the Context with Nearest Neighbor\\nInference. In: International Conference on Learning Representations. 2023a. 2023.\\n[355] Can Xu et al. WizardLM: Empowering Large Language Models to Follow Complex In-\\nstructions. 2023. arXiv: 2304.12244 [cs.CL].\\n[356] Canwen Xu et al. Small Models are Valuable Plug-ins for Large Language Models . arXiv\\npreprint arXiv:2305.08848. 2023. url: https://arxiv.org/abs/2305.08848.\\n[357] Chen Xu et al. Baize: An Open-Source Chat Model with Parameter-Efficient Tuning\\non Self-Chat Data. In: arXiv preprint arXiv:2304.01196 (2023).\\n[358] Yi Yang, Yixuan Tang, and Kar Yan Tam. InvestLM: A Large Language Model for In-\\nvestment Using Financial Domain Instruction Tuning. In:arXiv preprint arXiv:2309.13064\\n(2023).\\n[359] S. Yao et al. Tree of thoughts: Deliberate problem solving with large language models.\\nIn: CoRR abs/2305.10601 (2023).\\n[360] Junjie Ye et al. A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series\\nModels. 2023. arXiv: 2303.10420 [cs.CL]. url: https://arxiv.org/abs/2303.10420.\\n[361] O. Yoran et al. Making retrieval-augmented language models robust to irrelevant con-\\ntext. In: arXiv preprint arXiv:2310.01558 (2023).\\n[362] C. Zhang et al. One small step for generative AI, one giant leap for AGI: A complete\\nsurvey on ChatGPT in AIGC era. In:CoRR abs/2304.06488 (2023). arXiv: 2304.06488\\n[cs.AI]. url: https://arxiv.org/abs/2304.06488.\\n172'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 172, 'page_label': '173'}, page_content='[363] Qingru Zhang et al. AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-\\nTuning. 2023. arXiv: 2303.10512 [cs.CL].\\n[364] Wayne Xin Zhao et al. A Survey of Large Language Models. In: (2023).\\n[365] H. S. Zheng et al. Take a step back: Evoking reasoning via abstraction in large language\\nmodels. In: arXiv preprint arXiv:2310.06117 (2023).\\n[366] Qinkai Zheng et al. CodeGeeX: A Pre-Trained Model for Code Generation with Mul-\\ntilingual Evaluations on HumanEval-X. In: CoRR abs/2303.17568 (2023).\\n[367] Wanjun Zhong et al. MemoryBank: Enhancing Large Language Models with Long-Term\\nMemory. 2023. arXiv: 2305.10250 [cs.CL]. url: https://arxiv.org/abs/2305.\\n10250.\\n[368] Y. Zhou et al. Large language models are human-level prompt engineers. In: Proc. of\\nICLR. 2023.\\n[369] Aaron Jaech et al. OpenAI o1 System Card . 2024. arXiv: 2412.16720 [cs.AI]. url:\\nhttps://arxiv.org/abs/2412.16720.\\n[370] Josh Achiam et al. GPT-4 Technical Report. 2024. arXiv: 2303.08774 [cs.CL].\\n[371] Anthropic. Claude 3 Model Card . Accessed: 2024-12-24. 2024. url: https://assets.\\nanthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf.\\n[372] Jeanine Banks and Tris Warkentin. Gemma: Google introduces new state-of-the-art open\\nmodels. Google AI Blog. 2024. url: https://blog.google/technology/developers/\\ngemma-open-models/.\\n[373] Ning Bian et al. ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation\\nof Commonsense Problem in Large Language Models. 2024. arXiv: 2303.16421 [cs.CL].\\n[374] Yanda Chen et al. On the Relation between Sensitivity and Accuracy in In-context Learn-\\ning. 2024. arXiv: 2209.07661 [cs.CL].\\n[375] Yunfan Gao et al. Retrieval-Augmented Generation for Large Language Models: A Sur-\\nvey. 2024. arXiv: 2312.10997 [cs.CL]. url: https://arxiv.org/abs/2312.10997.\\n[376] A. Gundawar et al. Robust planning with LLMmodulo framework: Case study in travel\\nplanning. In: arXiv preprint arXiv:2405.20625 (2024).\\n[377] Xue Jiang et al. Self-planning Code Generation with Large Language Models. 2024. arXiv:\\n2303.06689.\\n[378] Subbarao Kambhampati. Can large language models reason and plan? In: Annals of\\nthe New York Academy of Sciences 1534.1 (2024), 1518. issn: 1749-6632. doi: 10.\\n1111/nyas.15125. url: http://dx.doi.org/10.1111/nyas.15125.\\n[379] Subbarao Kambhampati et al. LLMs Cant Plan, But Can Help Planning in LLM-\\nModulo Frameworks. 2024. arXiv: 2402.01817 [cs.AI]. url: https://arxiv.org/\\nabs/2402.01817.\\n[380] Jean Lee et al. A Survey of Large Language Models in Finance (FinLLMs) . 2024. arXiv:\\n2402.02315 [cs.CL].\\n[381] LMStudio. LMStudio. Accessed: 2024-07-26. 2024. url: https://lmstudio.ai/.\\n[382] New Scientist. OpenAIs O3 model aced a test of AI reasoning  but its still not AGI .\\nAccessed: 2024-06-09. 2024. url: https://www.newscientist.com/article/2462000-\\nopenais-o3-model-aced-a-test-of-ai-reasoning-but-its-still-not-agi/ .\\n[383] OpenAI. Learning to Reason with LLMs . Accessed: 2024-06-24. 2024. url: https://\\nopenai.com/index/learning-to-reason-with-llms/ .\\n173'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 173, 'page_label': '174'}, page_content='[384] Baptiste Rozi` ere et al. Code Llama: Open Foundation Models for Code . 2024. arXiv:\\n2308.12950 [cs.CL]. url: https://arxiv.org/abs/2308.12950.\\n[385] Gemma Team et al. Gemma: Open Models Based on Gemini Research and Technology .\\n2024. arXiv: 2403.08295 [cs.CL].\\n[386] M. Verma, S. Bhambri, and S. Kambhampati. Theory of mind abilities of large language\\nmodels in human-robot interaction: An illusion? In: (2024). arXiv preprint. arXiv:\\n2401.05302.\\n[387] Kevin Wang et al. On The Planning Abilities of OpenAIs o1 Models: Feasibility, Opti-\\nmality, and Generalizability . 2024. arXiv: 2409.19924 [cs.AI]. url: https://arxiv.\\norg/abs/2409.19924.\\n[388] Z. Wang et al. Bridging the preference gap between retrievers and LLMs. In: arXiv\\npreprint arXiv:2401.06954 (2024).\\n[389] Meta AI. The Llama 3 Herd of Models. https://ai.meta.com/research/publications/\\nthe-llama-3-herd-of-models/ . Accessed: 2024-07-25.\\n[390] BigQuery Dataset . https : / / cloud . google . com / bigquery ? hl = zh - cn. Accessed:\\n2024-04-14.\\n[391] Common Crawl. https://commoncrawl.org/. Accessed: 2024-04-15.\\n[392] Project Gutenberg. https://www.gutenberg.org/. Accessed: 2024-04-14.\\n[393] Wikipedia. https://en.wikipedia.org/wiki/Main_Page. Accessed: 2024-04-14.\\n174')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd73658",
   "metadata": {},
   "source": [
    "# Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e71cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "\n",
    "    add_start_index=True\n",
    "    )\n",
    "chunks = text_splitter.split_documents(documents=documents)\n",
    "\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "317b8c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593  chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1015, 942, 354, 965, 1017, 982, 981, 353, 1024, 876]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(chunks), \" chunks\")\n",
    "chunk_sizes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc614f6c",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fa1b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Agents\\SQL_Agent_Langchain_v1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "f:\\Agents\\SQL_Agent_Langchain_v1\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nimas\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "117c6b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_query(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b2f4f",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2c5c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"llm_survey_pdf\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "774d2a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f1f4767b-0a31-4ffb-b9de-ad69973630ab',\n",
       " 'd9169791-2ea9-433c-9cbe-c6080b3ee7e3',\n",
       " 'b50dbee6-5596-4f69-a838-cb8db0815d12',\n",
       " 'a2eb7660-a473-41d8-83dd-51a15622745d',\n",
       " 'aa8d9af0-da18-41d3-9216-c4af96b09812',\n",
       " '1af8c5a9-87dc-4d9c-b743-e34cb4778eb3',\n",
       " '3196f112-c6b7-409f-8aa7-c9fc954d8879',\n",
       " 'e17cde3d-a221-4398-8d81-25f62dbf3131',\n",
       " 'c70236c6-e321-415d-af82-a51c9d626e84',\n",
       " 'ea1b02b0-8e35-4b43-b1ea-55e0e8b7f979',\n",
       " '38e0a5d8-bd30-4229-8dd2-16010d8b5964',\n",
       " 'dee6f4ec-e5a7-471f-9554-c36f35fd8f70',\n",
       " '66b6bd0e-491e-4db5-aeda-d32b5f1804c6',\n",
       " '0bb7030c-6462-42f6-8028-abf0d037f010',\n",
       " '47d034a5-3956-4c8c-a633-e97c3ccf893c',\n",
       " '745daefc-6c33-49d8-958a-c238e7e0330e',\n",
       " '7651686c-6022-4187-8df8-b5664f1f3622',\n",
       " '9fadae2f-4826-4317-a0e5-a53d5212c8c4',\n",
       " 'b42368cd-44f8-4339-944e-d8f50a758c3f',\n",
       " '2aff6288-8205-4c2c-b4dc-20cabdc691ed',\n",
       " '3cb2a561-dc05-4153-9fa4-1bf7323ee2c6',\n",
       " '0ee1c4e8-52c2-452f-847f-86dbd83c66e2',\n",
       " '97a932fe-11f4-4d0c-bad3-7439c771d5af',\n",
       " '1b31dba2-d99b-4c96-b70e-2e288173e493',\n",
       " '60e7fdd3-bdde-40ad-83aa-e60c04934d97',\n",
       " '38931a13-9c88-466b-a10b-150a092aec6c',\n",
       " '826a5ca7-49a3-48f2-9e91-9111c922a19d',\n",
       " '0a8ed2f2-eab7-47a5-aa9d-346d028bdf37',\n",
       " '919a2dbb-550a-4c82-92d9-ea22e63bf342',\n",
       " '30aa6872-6cd0-4761-9388-b0d28a7681b3',\n",
       " '2719debe-885f-4870-bb02-5e5f21c75566',\n",
       " 'f01edbf5-e2ee-4649-bf62-e17fa9a2334a',\n",
       " '0d355f69-ee5a-4cce-9c13-67c8c5fa91f1',\n",
       " '03c0b152-825b-4065-8e4b-4e2899786928',\n",
       " 'db1704b4-de0b-4179-a389-da5a6eefe9c3',\n",
       " 'ee2e08c3-6eac-4ed0-a1de-5f6b9d0eebcd',\n",
       " '95154d6b-2163-4dfd-b762-ca1e30b2a66d',\n",
       " '4dec8b5b-1c60-483c-9e6d-3e7f897d15ec',\n",
       " '718ce2a9-64ef-4cce-a0a4-3b1f4b25c370',\n",
       " '5c16157f-538c-4d7e-b959-9569d9a79e72',\n",
       " 'e26641db-5678-462d-9764-49ad14dd3ad6',\n",
       " 'b8e93021-94b6-4f5f-bd05-35dd28b65ad4',\n",
       " '69811784-98d2-40b9-ac56-a6b33afec797',\n",
       " '907dbade-bf89-4c9e-bf5c-5401a202bfc7',\n",
       " '69825697-bf0b-4bde-84b5-2580abc26a56',\n",
       " '42b91e12-2a3e-4f8d-b6c2-bae9ed1ab358',\n",
       " 'ad81111d-2c0b-4856-bb1d-4a970766a667',\n",
       " '3b8ac3b7-2b23-4ce1-a7fd-5dfd56e782a3',\n",
       " '3d223a75-92a6-4fbb-9b60-bb565ceb4b77',\n",
       " '8d6d1d2b-f19a-441c-89b1-673552813e6a',\n",
       " 'ac783f0a-629a-42c5-b6c2-bd83434ce0f4',\n",
       " '2cbb66ce-44d6-401e-9657-e437db87c895',\n",
       " 'bdb29351-0366-4e33-9946-57be9e00e076',\n",
       " '6f439283-350a-4e1e-b285-7709e97b09da',\n",
       " 'e6342ed2-51b7-4fcc-a538-869447bd3296',\n",
       " 'cafe3757-0ab2-4062-891a-5df93c21b4e6',\n",
       " 'aad34380-bfd5-4291-a235-024e9161aabe',\n",
       " '15719d0a-c044-4041-a3b5-74d2095b986e',\n",
       " 'f513c531-0458-4fdf-85a0-61e84b433d57',\n",
       " '9bb99fb6-0d4b-4b25-b1c8-8ffacf440be3',\n",
       " '44532aec-bf45-4153-a31d-8649ad37f8cf',\n",
       " '418918d6-ce92-446c-8844-c24604fff469',\n",
       " '2d6940dd-ce4a-4157-949f-0b40299b9411',\n",
       " '70366fbd-1ac0-4c0d-b27a-ac1785f6461a',\n",
       " '5fb8db9b-371e-4a01-b844-f963aedadebe',\n",
       " '25ceb59b-f1aa-46d3-a044-fffa47fe27ca',\n",
       " '699768c4-edf2-4264-960f-240010d83700',\n",
       " 'ab2a1ec6-97c5-4ddb-a5b6-9fb6af774b47',\n",
       " 'd8622a31-5a16-43bf-9018-3863ca96e4c1',\n",
       " 'fdba99f5-b497-4837-b20c-7f36ecaacd86',\n",
       " '7f547dcb-9824-4651-86bb-1474f8a29136',\n",
       " 'bd7d3742-3413-4370-91ff-c95c491cdc51',\n",
       " '4e183853-5c9d-4197-ac01-8eef65077e20',\n",
       " 'c32d9109-60f3-488c-bd58-d8bfe599fa92',\n",
       " '4dd914cb-4cc8-48bd-934d-50c8fa74a5a8',\n",
       " '6e401b3a-4f8b-412d-acdc-b66f77cefa7e',\n",
       " '0c86d2c5-0f05-42ea-a73b-c252e9097e35',\n",
       " 'e1c7fe0c-7db0-4e80-9651-ba23c9b31184',\n",
       " '7603f0bb-084d-46c3-804a-f3fa53de416a',\n",
       " 'e3846ca5-9d54-4964-affa-54dd4b9eb608',\n",
       " '77e11c25-5499-4cb2-8d81-5105f3f7912d',\n",
       " 'c1d6426a-89c9-4018-9ce8-b79696f88db1',\n",
       " '6b8c30c5-9e3b-48e9-b5d7-e0a94a6817cc',\n",
       " 'f5e25b3f-7bcd-43eb-a80f-75635792fa40',\n",
       " '82604883-4ec9-45e3-b1d1-ede32d878e82',\n",
       " '004d2c06-fda2-46ab-aeaf-5fd8ff7d6f1a',\n",
       " '83741f95-2806-491d-80b0-4fbe057acd13',\n",
       " '1c942dd8-5c79-4ab7-ab37-a34e24ab5879',\n",
       " 'de12c39c-8117-43a9-875d-3e81ba0906b0',\n",
       " 'a32565b8-9d39-4e05-8087-35efe22e55ce',\n",
       " '69937dba-f2be-4868-9ccb-e44c6e410d25',\n",
       " '747fbc0a-d2ee-49c8-bf9d-c95aa3c74a50',\n",
       " '5a5a3994-7a71-4f86-9432-c64df966ab56',\n",
       " '9aba4688-8511-4ba6-bb6b-0c4e7bcb7292',\n",
       " 'f9a08055-fbbe-497c-8957-14cd68a275e6',\n",
       " '77c77a6e-cae9-427b-9c8d-dbfb5e45c01a',\n",
       " '37450189-d10c-454d-b79a-085900455a0d',\n",
       " 'c14982bf-e710-4782-be5f-fddbe7c74d10',\n",
       " 'ce005be4-757b-4f21-9d03-cfaf5d7c1d11',\n",
       " 'c8a8f2cb-cea5-4c1d-b23d-62fd588a9d81',\n",
       " 'ab015f34-0698-4acf-a069-6dba7bcbbac0',\n",
       " '98b691f2-43a3-4c15-a568-25fa917b0f01',\n",
       " '36b31565-5c3d-4fe7-bbe2-28e068b3a781',\n",
       " 'c2b5f67d-470a-4bca-b8e9-fd318509a69e',\n",
       " '93541117-8062-456d-8ccd-b9639a46fa0a',\n",
       " 'c4c59a19-4124-410a-9ba1-5f1fe25b68b4',\n",
       " 'ab034749-eb8e-43b1-93fa-f7a427d7caa5',\n",
       " 'e41686da-8877-409f-bcd9-50aec8588392',\n",
       " '99264d47-96ad-4066-84d0-ed0e0bd15d68',\n",
       " '228c3003-4343-45f0-b102-8ed1acdb6ee0',\n",
       " '22267a54-2f22-4ee9-9280-42ec97b64dfe',\n",
       " 'c425ae00-ad5e-4209-b20c-1f192cdbdafd',\n",
       " '9d7db8fe-cc93-4911-a27e-99214bde6191',\n",
       " 'c0cb1615-141f-4181-995b-23bbeedef4be',\n",
       " 'cd83c455-907b-4984-b92f-663800112981',\n",
       " '555c1e57-b6dd-4692-9145-093d56ad32bf',\n",
       " 'a2115193-5779-417b-aabd-16b70c187550',\n",
       " '2fb89fc6-41a6-4bd3-a2a1-e80d3c710d7f',\n",
       " 'a0cdae49-bb5c-43b5-b4d7-2a270727ca85',\n",
       " '23d05564-5630-4bed-9237-bdc285edca91',\n",
       " 'ea4b4e73-090e-4dc3-a528-e92d2a0dc3ac',\n",
       " 'bf51df53-7068-4aab-9265-0d8bbcb329b8',\n",
       " '03c91ae9-1ba5-498c-96fa-eb64ed697496',\n",
       " '2c1e403e-d74f-49db-b046-b2aefc3b43b6',\n",
       " '83a1b7ad-d6b7-4c4d-baad-25b120a8870b',\n",
       " '3906f1e6-bfe5-4bb7-9560-9f8e4daf5da9',\n",
       " 'a3c0d7ba-d461-46a8-82cb-dec04ca26e99',\n",
       " '3b6bf477-f23b-4160-860e-7aff0fbfa745',\n",
       " '87453fe5-7f1d-484b-8bdd-e94ea8f172cf',\n",
       " '157d9e1a-5905-439e-a592-043672865a4a',\n",
       " '2e103b52-fa80-4f1e-85f8-10636f36ec64',\n",
       " '93b2614d-7003-4e9a-a4c7-d07272db488f',\n",
       " 'dae7afe9-b3c9-474f-b618-89c4b747f48a',\n",
       " '655997ba-9ac3-4c2a-bd88-32f5e76b9b62',\n",
       " '08f142c1-d9ea-4511-8478-9929d046a7b9',\n",
       " '2883930a-c94f-4437-8dff-9b133b4a5200',\n",
       " '90559055-12dc-4bda-bf7c-5273de41b1b1',\n",
       " 'd69cb4e1-598a-4b32-95e7-637c4247c655',\n",
       " '3f276a80-dc40-4bbb-9c93-9835b2ee7f1c',\n",
       " '81077cce-0a6a-4644-8872-aa872074454b',\n",
       " '6b415932-640a-4ec7-8b50-934bf2bb46cc',\n",
       " 'acee9bdf-5565-493a-bdfd-eb5d8a95a3be',\n",
       " '41868eb0-9174-4f7e-85e5-66827cff02cb',\n",
       " '6c865556-0acb-4ba0-9a8b-ac9889546232',\n",
       " '4822ad8d-e7d1-4e30-b236-2a88214b4697',\n",
       " '3c9c4ba1-b8a7-428d-9760-4828c87ab7b5',\n",
       " '1eddfa94-424f-4727-9640-1330021ae748',\n",
       " '4a58915d-a785-4c62-b1e9-b971e3409d9f',\n",
       " '98d259ad-a558-4b9f-a60e-2ab659226328',\n",
       " '795d7511-d4a3-492a-bcf7-8550f9736177',\n",
       " '117a68b6-cd01-400a-8c02-3bb4d1d22689',\n",
       " '59840b6b-1f2f-47aa-bf00-a5964780a332',\n",
       " 'ee95b095-b5a7-4ed0-940f-485b67e8f081',\n",
       " 'd39ddc13-885a-4030-8f1e-9aed8226d2dc',\n",
       " 'de7068ff-abd2-49ed-a138-4f6c9faf79d0',\n",
       " '454430f4-7e06-4ffb-8aa6-9ba5ac263644',\n",
       " '07752a4c-0fb7-4cea-8c86-ad743ab41dad',\n",
       " '5d460b4f-b8ab-4fc7-9425-1d21fdec0cec',\n",
       " 'efa75385-fbae-4abd-8d16-22bd0ca1b5b9',\n",
       " '289586a6-c154-4125-9a39-331936d870d4',\n",
       " 'f8ff527a-3b83-4cf3-aa8f-55edd898324b',\n",
       " '33a2bf9d-ea05-4b8e-a1df-8b9421ef7675',\n",
       " '2511b76c-cdf8-46e5-8493-7c1dc04efc43',\n",
       " 'da7981da-f364-41ef-98da-4d6c1fc8a98f',\n",
       " 'c74949c7-408c-43fd-997a-57d91a31052a',\n",
       " '6ca945a9-d627-467d-9267-aa7b40c46a25',\n",
       " 'd795361c-930b-4a46-b891-5124fca1f8c9',\n",
       " '4adadd6b-c360-4eb8-ac83-0e75e1176449',\n",
       " 'bcaccb5c-cfd6-4bf6-9d08-733e33aa3c3c',\n",
       " 'df72048b-ad2d-4184-85cd-7b3e48a80c7a',\n",
       " '67e235ec-9c49-4186-8801-2cb058288084',\n",
       " 'ac547df0-cc35-4815-ba55-0b3f19023885',\n",
       " 'b84f7d5f-8eae-41a9-8876-4c6a03faca8a',\n",
       " '9582d19e-41dc-4c1d-a198-5485585ee56b',\n",
       " '2f79b7f2-404f-410b-af64-98fd90a29f24',\n",
       " 'cb28ce54-e560-492b-bc43-4284b0dc3e6b',\n",
       " '3aec0c9e-5f94-4b23-a133-3bbecc84793e',\n",
       " '3f3a3f0a-daff-42e3-9ae3-6375d6735c9c',\n",
       " '259c43e5-4ac9-4376-856b-d4028d24c0ef',\n",
       " '8ee055d3-3c21-449f-983f-2a485cb2fb6c',\n",
       " 'b02cd743-6fdd-43a6-bba8-6242830339af',\n",
       " 'e72cc409-27ad-449c-a86e-8f0c22319bba',\n",
       " '5b312d49-3dce-40a3-b2f9-76e945f19fc1',\n",
       " 'bcf7f9ad-26db-4aa8-9a61-c5ef481b6b69',\n",
       " '0cf65620-fe53-40cf-9c1e-f24f30fc4a4d',\n",
       " 'f792e5f8-ebb0-46a9-9d47-f1d51a67f1e3',\n",
       " '3c2fec5e-73ee-4b00-b9a9-21f999053217',\n",
       " 'd27b1b37-e93a-4bcc-81ff-5a23eb4bd60b',\n",
       " 'fc3dbe2e-6f7f-44be-987d-826eedf9af82',\n",
       " '1ae638d2-bd7e-4afb-b707-8592f2002d0f',\n",
       " '26cfe4db-6b40-445a-9bd0-859000755387',\n",
       " '26a4feb0-b6db-42fe-9207-eeac68900dc1',\n",
       " '6ac2431d-cb64-4a84-9405-3c1f16b7679f',\n",
       " 'c108955f-bc78-442e-8c31-1b2db066d78c',\n",
       " '3c8e4f12-74f7-447f-9e5c-1c80c117587f',\n",
       " '1464cf17-4bde-4bdc-9258-50e7b6573708',\n",
       " '27a6a6a3-2c59-49e5-af87-e89a275da319',\n",
       " '2bd74fec-dbaf-4b6e-a3c8-92d4578d1a78',\n",
       " '1c5b4ce8-64f4-4cbd-9795-a4f571502a21',\n",
       " 'c54a0853-cc96-4ff9-92c0-69bd0bb7fbd3',\n",
       " 'b4fda731-8e58-4559-9636-a250e146de9a',\n",
       " '49bbe1fe-fd22-47ae-b198-14da048ae858',\n",
       " '6f997858-219e-4895-b469-00b9e19c1588',\n",
       " '156d1c01-fab5-465c-9822-9fc2414ec27b',\n",
       " 'd376e8a8-f4ab-4c07-90c7-c563ccc47c4f',\n",
       " 'a3f5c9e9-a6bf-4ac8-a583-dac08538db66',\n",
       " '30af82d4-7bfd-41b9-ba39-b5d1d78b74a4',\n",
       " 'e17568fe-ab78-465f-bde3-4d7e724cf364',\n",
       " '8acd2718-b52d-4de0-906b-36783c208cc2',\n",
       " '63a9bf86-b613-4710-b757-ff5cce8b8c38',\n",
       " '8f12799e-56ec-4dc2-8384-896c9da16571',\n",
       " 'ee99d8ad-75c4-4868-84c9-1d5d05d0cc4a',\n",
       " '9d1a4858-6ce9-444f-be1c-a49e7ea9ecbd',\n",
       " '3387697b-ab25-494c-abfd-27a5aaed5628',\n",
       " 'f3f4a48b-09a1-460f-8b3e-de1af39d3a4d',\n",
       " '08d71f0f-b2ca-4c95-80ae-0fdb02a66844',\n",
       " '39201ced-6e32-44eb-bc8e-d10b4bc4dbb7',\n",
       " '2691bd23-0741-4a62-b80e-34265571537b',\n",
       " '85179988-6464-4176-b5dd-95ef3a428512',\n",
       " '3830c371-645b-4202-a7c6-598b563d5221',\n",
       " '35beb247-16fa-4c2b-ae24-70860c010cc4',\n",
       " '621c5209-51a8-4650-b71c-044b7ee5c25f',\n",
       " 'b878c898-f7b5-4e40-9c0e-f53ca9a9f60b',\n",
       " '16709f3b-f3ec-4339-a911-290189c33fc4',\n",
       " 'f2dcd0e2-fb31-446e-8678-90d7a6a80830',\n",
       " '0df5ac86-069c-401a-baaa-5870076c744a',\n",
       " 'c093b9b9-2384-4aed-b87d-14e07080a4bd',\n",
       " 'f01e636b-23f3-4b26-81ad-a124b8562d36',\n",
       " '19ad343a-e9a7-4c9a-827e-b82dec5f283a',\n",
       " 'f0e119d6-c3e7-434d-b876-9f37b64df12c',\n",
       " 'a59e9b85-8180-41a2-a976-752e31865c82',\n",
       " '5cffb12b-968a-4735-a378-76b44f2dc60a',\n",
       " '95c06f8f-a2cc-45e4-b9ec-ee4d1fcca73e',\n",
       " 'af16185f-db78-42e9-a11a-d7ff866b0961',\n",
       " '34706f4c-027f-4bae-847a-63e176c04aef',\n",
       " '64884938-ed59-4123-ab70-77e56c66f49e',\n",
       " '596ac568-e240-4b61-a4ee-b000251eff21',\n",
       " 'c6b94edb-6fad-4543-b010-a879a18350f9',\n",
       " '1f53d631-d088-42d8-b395-36fcf05fc5f7',\n",
       " '9f60ba74-3d1d-4208-ac36-87eb2289c91f',\n",
       " 'd56b134d-36a6-45bc-afae-dd1f8c951815',\n",
       " '131d4ed8-aeca-4b32-b79a-82875751f020',\n",
       " '1dc63849-7ad3-4221-8e75-534c8be87f3b',\n",
       " 'b4987f07-d378-4cb8-a047-161d08eaa575',\n",
       " 'd51dfa87-6734-4324-9cdb-3e1936325603',\n",
       " 'e87da5fc-c745-416a-866a-8a088e2fb316',\n",
       " '17e3293f-cb16-4987-aeb3-8f8752c7181b',\n",
       " '1b9adcd9-4d4e-442c-80e7-76aafbd83dc6',\n",
       " '2a5cf936-79ac-489d-ae43-3e8d40031199',\n",
       " '78768e4d-131d-4d1e-add2-244806690a77',\n",
       " 'e21d8e6e-6445-41f8-a2f0-98b894c0940f',\n",
       " '1cb79c3e-5ce9-48ae-b683-4b72d4783507',\n",
       " '6c2ba70a-c0d7-417c-bf67-ee97fdad48cf',\n",
       " 'c47028b1-a62f-41d0-b407-a5a704b013c4',\n",
       " '95a20546-396d-476b-9936-53739222e090',\n",
       " '9f1af148-c967-4505-b0ba-c6f82b5012aa',\n",
       " '2555d213-920b-4f28-8de8-9cd8e03ed1cb',\n",
       " '862353fd-d6cf-46f1-b4f1-4324a982021b',\n",
       " '6e467c32-9a80-4c95-b842-89f68d6e5c4e',\n",
       " '969c8291-5126-4235-8b1b-eb4f36a2dc3f',\n",
       " '7673e4a0-459a-4562-be83-e249b918685e',\n",
       " '82d0d66f-9401-40fb-8c63-91049009e421',\n",
       " 'f211e257-c04f-4016-978d-2a2623c88dee',\n",
       " '85f6c846-398d-4b49-8a70-097eac917b5d',\n",
       " '81499eb8-25bc-434d-9c90-d15e9e038c95',\n",
       " 'b9c078e1-dc8f-4db2-861e-6937df88fce1',\n",
       " '22f54a0d-7968-4e67-8313-b849523e27ec',\n",
       " '67fc3265-50cf-4e3a-a97b-2f28d6f15d21',\n",
       " '7a4d1087-8050-4b36-ab93-aa858298059b',\n",
       " '13774def-9417-4bd6-9758-ee55f8752418',\n",
       " '1051d593-0c3a-4638-b17d-f7ff2fc387e7',\n",
       " 'dc916370-ad67-4188-af00-0614464d3c70',\n",
       " '6f19254b-7100-46b1-89a0-c059712b0737',\n",
       " 'fe4cc024-ee8c-4bdb-ba4d-56aef6a810f0',\n",
       " 'f8d74995-7ca8-4a8c-999a-37b70789849d',\n",
       " 'd68c463c-9cf7-43b4-9ada-6a27120e84f8',\n",
       " '9ffda851-347c-40c2-8519-3f5a85e64447',\n",
       " '6b12ad6c-e9fd-4385-bf64-af0d2aa436c1',\n",
       " '8f3493dd-de9c-42b6-b7ba-b4c4c05a15da',\n",
       " '7cbf3c30-5514-426a-92dc-12ce1ae8fad2',\n",
       " '9cf8ba0d-287b-46fc-b002-c0f85d90faa8',\n",
       " 'd5d91ff9-e105-4c08-b0c5-4ca5a4e1d81a',\n",
       " 'eaa6801d-5f3c-49ea-b77f-d2559bf8f2ad',\n",
       " '3b4c40d0-6975-4b43-9495-9d1d2bbb9419',\n",
       " '9e162993-8830-4a40-a987-98634f960ddc',\n",
       " '3f61a6ab-e367-4643-8f9f-d9acb7f99216',\n",
       " 'a718b335-cc3f-4564-a9a3-8f7d641f7f79',\n",
       " 'ac4de161-6c7d-46a2-bcf9-16f7b1f0fae4',\n",
       " 'c19af4d7-9938-4195-8dc7-19865a049212',\n",
       " '7e73a372-905c-4b7c-be4d-3d31ca9a1c6a',\n",
       " '6ec7aa85-298c-4680-800c-33f1155eb164',\n",
       " 'b033aa46-832a-4e23-9134-b741e9f79af6',\n",
       " 'd50b062f-c8a9-4b4d-b2bf-f30d6fdb92df',\n",
       " '2a2d298b-6028-49d0-87ee-9a064c508402',\n",
       " 'de812992-1ee8-42c8-826f-a1eeb1652284',\n",
       " '3214e2ed-0111-4dae-8e5d-bbd5d4a40da2',\n",
       " '3664a787-490a-4869-99a0-01942c0520ef',\n",
       " '1dddb317-ec8e-47c7-a1b1-af98f65ddce2',\n",
       " 'cc1b8d63-445d-4b00-bf51-e01dfae073a3',\n",
       " '98701812-230e-4cea-8008-b11a00b29cff',\n",
       " 'c2be7343-3e67-4bf6-b213-a20944141a3a',\n",
       " 'b53c5968-3872-4f5e-8883-bfc122235196',\n",
       " 'e56a1ce5-71d8-4e3a-8a2c-9c180dad1ef8',\n",
       " '53978e59-21e1-4d62-b223-f2e013a5ea0b',\n",
       " 'fc098bbc-2c47-4b13-ab9d-5d962cfd2245',\n",
       " 'f435b414-4292-48bf-8e6b-81e342489784',\n",
       " 'e0853848-c976-4611-aeab-1cf2515f5204',\n",
       " 'aa0d93dd-897e-45ef-80a7-a5508c57a501',\n",
       " 'ac9a1a79-f403-4014-8a44-e03920058b41',\n",
       " '4418cb65-49fa-48f7-be7f-55a58d86a0ff',\n",
       " '1da48b9d-c6a3-4e4e-9db1-88fe20228644',\n",
       " 'e642fe6c-bcb5-49ae-90e8-355deecd7145',\n",
       " '5fe8270c-6ff6-4359-81c3-1345e93e45f5',\n",
       " '815452ad-7374-45c3-8517-642b72c6d97e',\n",
       " '936d320c-c9cc-4689-93b5-966289ad8920',\n",
       " '37a16454-21fa-478e-8211-91f21ef7e3de',\n",
       " '850e1257-d84c-4039-9302-664fa48bad70',\n",
       " '3cf1582f-9f88-4bac-9bc3-426e8b869701',\n",
       " '2b1ce13d-f685-4d9e-9391-7c3820b11173',\n",
       " '39488aba-ca93-4268-9f09-56e0fced90d7',\n",
       " '465a82b1-621f-4cbd-a83d-19a1e286fea6',\n",
       " 'e5623f6a-9dfe-4a42-a1c8-5a3e6f01a978',\n",
       " 'c28a7fd2-a7f9-4449-9e6f-ce06d9cac9e6',\n",
       " '1d8872ef-49c3-4d8d-8fa9-e011ad0f2d28',\n",
       " '3ade0049-4154-46cd-8ba1-a1af3227d386',\n",
       " '1ce46c27-4474-4a3c-bb57-f36029f66a65',\n",
       " 'b320e4cb-4727-4c13-9dfe-a69d7ecbf421',\n",
       " 'c1c55003-7416-4bdb-8b1b-1cdb99a5123e',\n",
       " 'e0839e91-d1b8-43d5-9e78-f1bfcea6d66d',\n",
       " 'c3d10a4d-8148-451e-942f-1328dcb5df28',\n",
       " '557b03e5-2225-4120-8ce3-19a58d415232',\n",
       " '15c9c7df-395d-4898-9c48-977bcb62724d',\n",
       " 'cd67718b-b87d-4527-a589-b35244a20f34',\n",
       " 'a9c2af92-aaf3-4bad-8bec-888475e880b9',\n",
       " '924a5a18-317d-49be-a11c-5ace778faf41',\n",
       " '62db36c5-c22d-4370-9861-32a049150c50',\n",
       " '80224648-1da3-4e1c-8643-47b4c6415f79',\n",
       " '70715a16-7b73-4f6e-a29f-58f31e859fd3',\n",
       " 'b9f55009-c06d-47da-b11b-09386b5da84a',\n",
       " '655501fd-1a4c-46a9-b415-243cd38fc7c5',\n",
       " 'b64381d4-a598-49eb-9724-2f958f11e406',\n",
       " '4d6e6021-2d3b-42aa-8ddb-87d1b135c2f7',\n",
       " 'bd0ed554-178e-4379-8ea5-0e5ee2be32ea',\n",
       " 'b58e9952-16f6-43b1-9e75-529a93b9e29a',\n",
       " 'eb97aba4-5f09-48cb-bdae-ecf431ddf5fe',\n",
       " '17e3939d-67dd-47fc-bacd-9b37d987beb4',\n",
       " 'ca8b9aea-52a3-47e6-8c24-2ebb139b8e32',\n",
       " 'd59d1dad-ade6-4f53-9bdc-521e00f42899',\n",
       " 'cbcc12ab-3d43-4801-a68b-74729f82d131',\n",
       " 'aba3e518-f95c-4c9a-beb1-9a0cc2eb9ca1',\n",
       " '148fb8c5-9235-458c-ae5a-815fb0d76231',\n",
       " 'ca12e298-4109-4ec4-bfbf-f98cc1372228',\n",
       " '983850db-f77b-4492-8d41-d6699327dd8a',\n",
       " '7c23b0dd-94cb-48e1-b865-ba1c8da437a0',\n",
       " 'bf70fc23-b0d3-4ca4-9d9a-b1f361925427',\n",
       " 'dff56798-b0f6-45a5-959a-829849c4cf87',\n",
       " '7aec2192-67c4-4258-8ce7-f7b556baee84',\n",
       " '77590abf-41a3-4141-89cd-d02b9e8c5926',\n",
       " '13341107-c751-4d50-ad2a-23c401f07082',\n",
       " '44b99787-f02c-46eb-a323-e0be6f9dcf26',\n",
       " 'd248a723-ff81-4e9f-9d9e-17f64b98e955',\n",
       " '8587a036-6bcf-41ab-8084-cc69709bccf6',\n",
       " 'e18fb552-0298-4886-b65a-46f6fadf78c1',\n",
       " '4458aac8-cd7c-475b-b5b8-b8002aa19f62',\n",
       " 'f83c6dc7-9cd9-4b4b-8fd2-c7852eb72847',\n",
       " 'ca7a5972-d346-4aea-b8f3-60c89d688766',\n",
       " '80b116c0-33b3-481c-a699-cc54df631fff',\n",
       " '777d7aff-9bc8-439e-a1c8-5fde8ee337e2',\n",
       " 'fbba9ad6-6097-4c77-85af-6c1054df8f2d',\n",
       " '63e542db-3559-4be5-b3eb-5199458dde9f',\n",
       " '37095bbf-a1d7-4570-8dfb-33dbbc95b31a',\n",
       " '994cbfc8-e8e0-448c-8625-0b4041a06a1e',\n",
       " '13b0bc24-635e-46b5-9a72-3c68a53f97f9',\n",
       " '4e040672-cd16-4de2-92d0-a769bad88f2a',\n",
       " '04b7cdac-98f1-466f-a09e-e3129d924fd5',\n",
       " '10ce7826-a593-48cb-b634-7308a88f6d87',\n",
       " '691dfb24-f8fa-4c9d-8dfb-36e1c85efbcf',\n",
       " '7b5efb8d-b95a-4412-9773-555fd2bc967c',\n",
       " '6506f066-52d4-4cc5-97fd-f1cb12622d27',\n",
       " 'df502144-98f4-4678-8c2b-5f2f8444a847',\n",
       " 'b6dee74e-4d2b-40db-bdbd-d59f83a68273',\n",
       " '8be769d4-7787-402d-ab6c-387f9932fa17',\n",
       " '1cb4a52b-7d36-48b8-9007-823551caf2e5',\n",
       " '21cbf1d7-41d9-49cd-952d-c3defdf5cf3b',\n",
       " '75b5b941-2409-4d76-9a72-8d3b99de8d96',\n",
       " 'a15f71ef-7bb8-49ca-a430-ef936f4276ab',\n",
       " 'fbd00bc9-5f1b-4152-8ced-370cf4e04b8c',\n",
       " 'f7e171b7-417e-47f1-9dc0-d23008bee0c8',\n",
       " 'f31dc468-2177-4f79-aca9-c6b38006cff9',\n",
       " '06389f54-5abd-4df9-9fc6-2b31d0cf7c73',\n",
       " 'ac22ec34-c40f-4e85-952e-3858de4ef734',\n",
       " '110fa782-39e5-4222-a122-294b33eada77',\n",
       " '25c53106-6fd6-422b-9e74-15f4a86214a6',\n",
       " 'd8913ebf-9f75-453a-9b1e-05091c83d941',\n",
       " '15a35b4c-6445-4b37-bff5-801a90bd0921',\n",
       " '0750aed0-d7be-4ec4-a589-8c9ca450466d',\n",
       " '6ea36cde-4a71-448c-8d3f-24f1df404f04',\n",
       " '19e9b708-058a-48bc-aec2-1d250559796a',\n",
       " 'f3780e9d-cb92-4f25-8242-38183829e7ed',\n",
       " '8bf712bf-2c65-4ae4-8cd7-6de075f3ec53',\n",
       " '4ba202b4-8d1b-4091-824d-c04928501162',\n",
       " 'b7219615-7b7a-4aa5-99b7-93ab37fd39ff',\n",
       " '34f1b75e-fc1e-4d86-a8d9-e0488b9260ec',\n",
       " 'cd6d0ab3-6c13-4a29-9f78-8beb7a093ad0',\n",
       " '3a4b4c01-1cbe-461f-a396-ad0e00f45943',\n",
       " 'cf4f1609-4fe0-4c51-963d-a29ec12058d9',\n",
       " '9c991931-26c2-4a31-897d-2821b0901b67',\n",
       " 'e5f6d489-ca74-4820-b272-99f69ad6a7e0',\n",
       " '6d3bfb5a-382d-4872-be04-e05b86bb0342',\n",
       " '449d9f69-f189-4102-a9f7-20177df63f26',\n",
       " 'a59e73bf-5376-4f6f-b266-b0615210429a',\n",
       " '213adf6a-2c13-44d9-a2da-d646390ad531',\n",
       " 'cd1f97f9-c70e-4323-9123-406e79f10de6',\n",
       " 'dfe1acb7-ff7b-43ca-b703-022794fdeae0',\n",
       " '24a9a4ab-ebdf-4424-aa6d-97ddbd053259',\n",
       " '2073aa41-27a4-442d-9484-645946af3d4a',\n",
       " '443aedb5-d956-416a-afee-4be4e015493a',\n",
       " '27c2e635-e44c-4ddf-9e29-6937a296ac41',\n",
       " 'a05c1837-d0dc-4c82-86c7-2a0d330d351d',\n",
       " '1bbbdfc5-143c-4b4d-b70b-80d29b5edf35',\n",
       " '93fcd1c6-5649-4866-845f-adcd62809d81',\n",
       " '91a47a3a-0abe-4957-bc5e-768560a9392a',\n",
       " '8a0b9905-a5a7-4f1e-924e-41600c5d55ff',\n",
       " '3798cef0-7c74-4dc2-97c4-992eb5f0f07c',\n",
       " 'a0401ead-3dae-4a26-8b87-1ea2d39039f6',\n",
       " '1a404321-66e3-404c-92bf-f1f50eca52e5',\n",
       " 'f7f51077-ebd6-49dd-9c97-3d1b3efba2a7',\n",
       " 'f31bffa7-d4f8-4972-9d8d-2f734da056dc',\n",
       " '6fa8bf06-77d0-4300-8f59-a7c425190e26',\n",
       " 'cfaeb54d-6f91-4093-9b44-09fafbc838db',\n",
       " '33005d35-abdd-4eb1-9ce7-e24687d5269b',\n",
       " '624d6f6d-f4ca-4f61-bb89-61da8041a913',\n",
       " '4b1eab95-b626-4386-a003-e5146df453dd',\n",
       " '9501509d-e2aa-45bc-b986-733fcd9123e8',\n",
       " '0e4ec265-c4cc-4837-af30-01ad167b9e02',\n",
       " 'f0a8e891-6ee8-4dc1-9bc9-5ef97add3897',\n",
       " '47e8dd65-03c2-4266-a42f-9697028d883d',\n",
       " '07362953-41d6-476b-9300-dc47819dc5be',\n",
       " '492a2583-5fba-435e-bd76-7d0be083ddd0',\n",
       " '3d34a2ed-2f10-42a1-934b-e9b8d9be8bd3',\n",
       " '85d27d1e-c493-4e4f-8ae2-4c86ed6757f0',\n",
       " 'f96a80b6-ff2b-4170-9ce6-a62281ddfd33',\n",
       " '69cdee6f-2807-4c2c-a694-883e07d11113',\n",
       " '6f47c42b-2072-4d1d-abb2-8ad0a0ef3a97',\n",
       " '2b4bc4df-7cf3-4953-999c-428272c2f869',\n",
       " '27ccacda-84d5-4f76-8bdd-c23cf308d354',\n",
       " '57ef2e1e-bc3c-4f5d-84f3-9bb2bf77461e',\n",
       " 'b94b754c-76ce-42a6-8631-a5b1fccc3494',\n",
       " '3fc0e4a5-d0e2-4e69-87ec-33b36092a79e',\n",
       " '3bb7b082-760e-4508-b035-65e2112434f6',\n",
       " '31d401b1-900e-4d4b-8df4-4f9c4430c54f',\n",
       " '89f1bed2-ddf0-4c88-b9e5-72db36e47925',\n",
       " '30d1e910-4946-4899-a8c4-c6fcaed962ac',\n",
       " '1190cd8d-59c4-4930-9160-442581e2e974',\n",
       " '400ed172-2c65-45bb-9760-c256f52a55ba',\n",
       " '8228e5a5-b32c-4ad9-a639-0f8b0a90dc03',\n",
       " '8a477dfc-3ead-4f8b-b7c2-9fa52bdd8368',\n",
       " 'f4178bdb-c477-4538-a6e5-942bb39e7e33',\n",
       " '2f56a2f4-3c62-4758-9245-e23fe485a5f7',\n",
       " '281a4f45-8df7-464d-8bb6-cf243d5d0a65',\n",
       " '083f9d1b-cedc-47c4-9b09-6be16d2f9b31',\n",
       " '98bc3533-cc52-4518-8355-e0ac5a2eec6e',\n",
       " '425210bc-57b9-4ebd-adef-11981a940a8b',\n",
       " 'c7b21d78-8171-4bcb-aa04-ac92d09be9c1',\n",
       " 'e4538be2-adae-4003-937a-c16eff6dca04',\n",
       " 'a0b2ff30-bccc-495f-8458-0982b5257051',\n",
       " 'fe07351b-6b61-4ac5-93f6-0eadd9c50360',\n",
       " '9d7398da-c187-4b84-b915-90d886bbeb85',\n",
       " '49e37958-0321-4dda-a9f2-52a245c9c81e',\n",
       " 'b003ea22-1564-48af-a98e-cc3c459cfd1c',\n",
       " '73142251-6b76-417c-9657-9f4a45ca8d0d',\n",
       " 'dcbd2357-f8c5-45ed-b1d3-b32d3eb057c8',\n",
       " '78432505-8c5c-4a5c-87f3-e8927b42b9af',\n",
       " '859ac35b-03cd-4701-a77d-9dfef3e06f93',\n",
       " '125cf736-7c23-445c-91b9-85d8ab58b6a2',\n",
       " '23ce05eb-18fa-4e9d-af65-bcad01ec361f',\n",
       " 'cdfc61b2-5d17-4277-b122-6a391fe872cf',\n",
       " 'a41b189b-eba9-4742-b24c-e241c1d29f8d',\n",
       " '617e4756-9fa5-4ed5-bbc2-08ed4405ce9f',\n",
       " '719f0052-04e0-45aa-917a-5120b8f60b13',\n",
       " '600ae044-0854-44a2-aad5-9d7f7fbddab2',\n",
       " '28992363-4bac-462f-86a8-7b940e9672b2',\n",
       " '72e1fb37-bad4-4b15-930b-52210fc03d2d',\n",
       " '3784cc92-ebdc-4303-8dea-2de0ceeed580',\n",
       " 'a2d11066-8cfb-4427-9afd-fe7ff2da16b4',\n",
       " '34019bbd-38ed-4a0a-847e-80a7c3fc3faa',\n",
       " '285d775c-417c-4c5b-a6d2-dead52bdb10b',\n",
       " '627f190c-f9ab-41c8-9195-6cc3a221dcc2',\n",
       " '7de8ddb3-58cc-45f3-8357-a2a63a1f9a6e',\n",
       " '8ce74fd9-b688-487b-a667-9738f182d900',\n",
       " 'a7c77d13-cc90-4baf-a408-5203bd8a032b',\n",
       " 'af779ee3-8777-45b4-91fd-2bbd1def9f3a',\n",
       " 'e7939f50-925e-4d6e-bd31-792f79bd13b5',\n",
       " '3e9d86a1-036e-4e89-a9da-689576e067a1',\n",
       " 'f7adf802-a7dc-4632-857d-6be6ada64abb',\n",
       " '519cb590-573d-4719-9884-a462a1adbec7',\n",
       " '5c931df0-9d07-4a18-9ca9-c904eb9a0b36',\n",
       " '80688cf3-0779-4786-bb77-ef3198b943a6',\n",
       " 'a960dda1-8bd4-448b-b6ac-498ad7212f04',\n",
       " 'a16613df-6bc0-4d5d-b4fb-774884cce200',\n",
       " '6b760b92-f505-4355-b417-4aa2a6c7cfbc',\n",
       " '79227ed1-990b-40bd-86d1-b131e7a90076',\n",
       " '66965cff-5fb5-483e-865b-fac06c38d6b5',\n",
       " '102bbf44-e703-4066-9155-b5903e29ac68',\n",
       " 'aa05e57e-77d8-4fa8-b6e7-49b5a5462aec',\n",
       " '9f227670-39bb-4943-8918-ff71236afb93',\n",
       " '8439c014-1784-4c5f-ba3e-48ccb1a2d140',\n",
       " 'f019cf91-e68d-40ee-8f78-bc7d073d054c',\n",
       " '10083182-10bb-4227-b226-366687ba21a2',\n",
       " 'eaf6219c-55f2-400e-b2c1-e9e5f76e105e',\n",
       " '5d8602ab-51a4-4bda-8a42-336682ecebcf',\n",
       " '018c5760-b7e7-4101-9b50-06a58d035b6c',\n",
       " 'a771085c-481a-46f6-a19c-36585ddf8d28',\n",
       " '2c6a3bb8-7f74-43b9-aca3-d127a9d5b913',\n",
       " '917ba211-769a-4cd4-8d7a-74cb8980b1d4',\n",
       " '6a5fc534-949c-4f21-942b-1a1ca7946cdf',\n",
       " '65fb5754-505b-4b09-9707-7e243346115b',\n",
       " 'ff73cf24-b7f7-48e9-8e17-4a79da2437e6',\n",
       " 'a541f298-f23c-4660-a434-1b7c5dc91cd3',\n",
       " '0852238c-4643-419d-a0d7-cf5e259376b8',\n",
       " '00c20194-140c-4ca1-bc39-7ba588931a46',\n",
       " 'd503b0c3-ce43-4f89-bce6-8771e373597c',\n",
       " '26df5dab-bb6f-4409-af31-2dd1cbeb840c',\n",
       " 'deff4205-991a-4ccf-93d1-b9c1130639bf',\n",
       " 'b77ccefd-91f2-4121-a279-2ef176c5e090',\n",
       " 'f9091a95-bd12-4755-a2ff-e23d3a50e924',\n",
       " 'de2b68d4-3bf5-4524-a3cd-e7e8dd3e806a',\n",
       " 'eb1df7ca-c2e6-4999-bbaa-b57777ae9a2d',\n",
       " '1f7600ba-e3e2-457c-84ca-c86a6997e154',\n",
       " '98815250-3c6e-4cb9-b85a-4b62231ade56',\n",
       " 'd5fccc36-9aa7-44fc-a22d-a25c29f1fd0e',\n",
       " '91fe350d-63a6-4f64-8c9e-41d2decfa38f',\n",
       " '194b1720-1f17-40f0-9e85-f1239243c430',\n",
       " '30a46aaa-9c8c-4495-8b62-82d7a81d99fd',\n",
       " '7bcaf184-86e6-4bff-a006-0854370e4e07',\n",
       " 'bf4e7e28-b11d-49b5-a3b2-137c02ab51b7',\n",
       " 'e230ce98-fd68-4a5b-a91a-3a789a4244dc',\n",
       " 'acd61632-d1d1-42f5-9e89-fe380df3f2c1',\n",
       " '1db8e24c-f08e-494c-9993-665caf6da549',\n",
       " '7cad39ab-cb8f-415f-8fb2-c6f99ef43442',\n",
       " '3dad8a4f-adb4-4923-9229-6bd4af58ae28',\n",
       " 'a2953f31-ee7a-4e6d-92ef-be3dae4509a1',\n",
       " 'c6ad8b2e-1141-4338-9100-281a036663fd',\n",
       " '3f69a92e-d2e5-4b6b-83f3-af35f8ea8699',\n",
       " 'eca2f44e-2f89-492d-bf4a-7085be56f613',\n",
       " '492553e2-15cb-4172-b6ec-966398583fe7',\n",
       " '5b156936-592d-4104-ac51-0760131dfd75',\n",
       " 'fdc5c8ce-8bf8-4b9e-a037-4b2c46ee8e4b',\n",
       " '9155d343-4d18-425e-97af-d6b897617903',\n",
       " 'f83e25a8-f0b6-47c3-b93e-56535df8f889',\n",
       " 'c4e19600-01cb-427f-a038-8e3a00722b8a',\n",
       " '2b0732fd-9a1f-4e10-984c-8ea3dd6e310a',\n",
       " '9dfe94b5-6ba3-4ff5-a6b7-c8e406caf184',\n",
       " 'afc9585c-9d19-4cef-8cad-5ada2ca60542',\n",
       " '84d01ed3-2300-46a3-9974-42a7cc42d954',\n",
       " 'b8ef9562-6afd-4810-a0b1-7a88f329b114',\n",
       " '67675844-82a5-468a-b3e0-c022446da5c4',\n",
       " '5178cc0e-2bcd-4a83-ad88-8f7704659495',\n",
       " '0262622a-bafe-4722-a2d8-2f7761b9fd89',\n",
       " '28684dea-b345-4610-82e8-dbaa3b6ae18d',\n",
       " '1f0363cd-3901-49e1-bfb3-a2bd0a7e5ee1',\n",
       " 'ffff7652-0ace-4ff3-a097-f59cbeb437a2',\n",
       " 'd1955fcc-19e1-4b5c-babb-d2ecf05fbc49',\n",
       " '819291a6-f119-4711-a658-2a6cc4d5376b',\n",
       " '1657c7b3-6a73-4f8f-8784-d4e94ba832c9',\n",
       " '505e5e5b-62c5-4d1c-9fa0-8032840be9d9',\n",
       " 'd7d5e826-6a16-4e8d-ba93-937f0282d16a',\n",
       " 'a6859302-1e13-47f9-9b5c-79baede446c1',\n",
       " 'aa923288-ac69-4b9a-b1b5-6c0b6ed4e915',\n",
       " '9f879b51-6d0f-4a7a-bb0f-9b888c18e2c9',\n",
       " 'af702951-de21-4941-a85d-f21c72a84e3d',\n",
       " '54681dca-568d-47df-826a-302dab55db04',\n",
       " 'd753d5e8-132e-4837-acd9-4aa670d44ea0',\n",
       " '1e33aa02-de96-4293-9afa-15f841695f24',\n",
       " '2f0a9052-2426-4f61-b5a9-3c7a4bf9fb03',\n",
       " '6c0e3b5f-85b9-403e-94f7-6f0fd3a105a6',\n",
       " '1aafcef2-e74e-440d-9654-848341779be7',\n",
       " 'c6d61306-addd-44c3-849d-fbe50f9840a7',\n",
       " 'e0eb1c66-9deb-4abb-86e2-91f215f5cfc7',\n",
       " 'd1bf49ec-2a6d-4083-ab0b-d99b57edf72e',\n",
       " '5f62c63a-221a-4124-86ea-ebfc99c06918',\n",
       " 'f7eb108d-d46a-4cb0-a018-abb4dcaa6389',\n",
       " 'fcdf6ffe-47a3-4982-990d-2313120c9cbf',\n",
       " '0f1d6d34-6644-4fb5-8c64-a3b4844d1053',\n",
       " '58721f5d-d8a0-4df1-9959-9d0e72497101',\n",
       " '60483e69-422f-4e5b-ab38-e7cd5cfd385b',\n",
       " '6632c132-a586-46b9-b611-93385e65a720',\n",
       " 'f6bee109-87e6-494e-a7f5-64b1b209870b',\n",
       " '4f94c043-d66b-4dae-919a-ad979f16fcfe',\n",
       " '922aca91-b7e1-4389-8887-39928bbeaf31',\n",
       " '304351c3-327c-4a4f-9aae-90690b732ff5',\n",
       " 'b01810d9-54b9-4548-810e-c50c220d6fed',\n",
       " '272cc9cb-4c9a-4933-86af-97e888fa7408']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f26c4",
   "metadata": {},
   "source": [
    "# Query the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ea1614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the main methods available for RAG?\"\n",
    "results = vector_store.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57cc0a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "\n",
      "Figure 66: Retrieval-Augmented Generation (RAG) Framework mainly consists of 3 steps. 1) In-\n",
      "dexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2)\n",
      "Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\n",
      "Generation. Input the original question and the retrieved chunks together into LLM to generate the\n",
      "final answer. Source: Gao et al. [375]\n",
      "ing and feedback mechanisms, improving task performance [303].\n",
      " Generate-Read: Replaces retrieval with LLM-generated content for certain scenar-\n",
      "ios [238].\n",
      " Recite-Read: Retrieves directly from model weights to better handle knowledge-\n",
      "intensive tasks [262].\n",
      " Iterative and Hybrid Retrieval: Combines multiple retrieval strategies, including\n",
      "keyword, semantic, and vector searches, or uses hypothetical document embeddings\n",
      "(HyDE) for improved relevance [320].\n",
      " Dynamic Frameworks: Frameworks like DSP [365] and ITERRETGEN [320] iter-\n",
      "Source: Page 119\n",
      "\n",
      "--- Result 2 ---\n",
      "\n",
      "1. Enhanced Knowledge Integration : By querying external databases, RAG systems\n",
      "continuously update their knowledge base, addressing the limitations of static pre-trained\n",
      "models.\n",
      "2. Improved Accuracy: Retrieved data serves as contextual grounding, reducing halluci-\n",
      "nations and increasing the factual reliability of generated outputs.\n",
      "117\n",
      "Source: Page 116\n",
      "\n",
      "--- Result 3 ---\n",
      "\n",
      "Figure 64: Final Pass rates of models across LLM Modulo Iterations. Source: Kambhampati et al.\n",
      "[379]\n",
      "3. Domain Adaptability: RAG enables LLMs to integrate domain-specific information,\n",
      "improving performance in specialized areas like law, medicine, and engineering.\n",
      "RAG systems are categorized into three main paradigms:\n",
      "1. Na ve RAG: it was the first iteration of RAG systems. It follows the traditional pipeline\n",
      "of indexing, retrieval, and generation, which is also characterized as a Retrieve-Read\n",
      "framework [304]. This approach is simple and effective but suffers notable drawbacks in\n",
      "terms of retrieval precision (e.g., missing crucial information) and generation accuracy\n",
      "(e.g., allowing for hallucinations, toxicity or bias).\n",
      "2. Advanced RAG: it introduces specific improvements to address the limitations of Na ve\n",
      "RAG. About retrieval quality, it employes pre-retrieval and post-retrieval strategies to en-\n",
      "hance the relevance of retrieved data. For indexing, it uses more sophisticated techniques\n",
      "Source: Page 117\n",
      "\n",
      "--- Result 4 ---\n",
      "\n",
      "Figure 65: Technology tree of RAG research. The stages of involving RAG mainly include pre-\n",
      "training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused\n",
      "on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the in-\n",
      "ference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning\n",
      "of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training\n",
      "stage through retrieval-augmented techniques. Source: Gao et al. [375]\n",
      " Memory Module: Uses LLM memory to iteratively align retrieval processes with\n",
      "data distribution and enable unbounded memory pools [262].\n",
      " Routing Module: Dynamically selects pathways (e.g., summarization or database\n",
      "querying) to ensure optimal information retrieval and merging [365].\n",
      " Predict Module: Reduces redundancy and enhances context relevance by generating\n",
      "content directly via the LLM [238].\n",
      "Source: Page 118\n",
      "\n",
      "--- Result 5 ---\n",
      "\n",
      "hance the relevance of retrieved data. For indexing, it uses more sophisticated techniques\n",
      "like sliding window approach, fine-grained segmentation and metadata. It incorporates\n",
      "additional optimization techniques to streamline the retrieval process [280].\n",
      "3. Modular RAG: this architecture advances beyond previous RAG paradigms (Naive and\n",
      "Advanced RAG) by offering greater adaptability, flexibility, and functionality. It intro-\n",
      "duces new components and interaction patterns to address the challenges of static and\n",
      "rigid retrieval-generation frameworks, making it suitable for diverse tasks and dynamic\n",
      "scenarios. Modular RAG incorporates specialized modules to enhance retrieval and gen-\n",
      "eration:\n",
      " Search Module: Supports direct searches across diverse data sources such as databases,\n",
      "search engines, and knowledge graphs using LLM-generated queries [303].\n",
      " RAGFusion: Implements multi-query strategies for diverse perspectives, utilizing\n",
      "parallel searches and re-ranking for knowledge discovery [320].\n",
      "118\n",
      "Source: Page 117\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n--- Result {i} ---\\n\")\n",
    "    print(doc.page_content)\n",
    "    # print(doc'page_content')\n",
    "    print(f\"Source: Page {doc.metadata['page']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95aa9ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Figure 66: Retrieval-Augmented Generation (RAG) Framework mainly consists of 3 steps. 1) In-\\ndexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2)\\nRetrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the\\nfinal answer. Source: Gao et al. [375]\\ning and feedback mechanisms, improving task performance [303].\\n Generate-Read: Replaces retrieval with LLM-generated content for certain scenar-\\nios [238].\\n Recite-Read: Retrieves directly from model weights to better handle knowledge-\\nintensive tasks [262].\\n Iterative and Hybrid Retrieval: Combines multiple retrieval strategies, including\\nkeyword, semantic, and vector searches, or uses hypothetical document embeddings\\n(HyDE) for improved relevance [320].\\n Dynamic Frameworks: Frameworks like DSP [365] and ITERRETGEN [320] iter-'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd0b4dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 (Score: 1.1948102712631226) ---\n",
      "\n",
      "Figure 66: Retrieval-Augmented Generation (RAG) Framework mainly consists of 3 steps. 1) In-\n",
      "dexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2)\n",
      "Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\n",
      "Generation. Input the original question and the retrieved chunks together into LLM to generate the\n",
      "final answer. Source: Gao et al. [375]\n",
      "ing and feedback mechanisms, improving task performance [303].\n",
      " Generate-Read: Replaces retrieval with LLM-generated content for certain scenar-\n",
      "ios [238].\n",
      " Recite-Read: Retrieves directly from model weights to better handle knowledge-\n",
      "intensive tasks [262].\n",
      " Iterative and Hybrid Retrieval: Combines multiple retrieval strategies, including\n",
      "keyword, semantic, and vector searches, or uses hypothetical document embeddings\n",
      "(HyDE) for improved relevance [320].\n",
      " Dynamic Frameworks: Frameworks like DSP [365] and ITERRETGEN [320] iter-\n",
      "Source: Page 119\n",
      "\n",
      "--- Result 2 (Score: 1.2301374673843384) ---\n",
      "\n",
      "1. Enhanced Knowledge Integration : By querying external databases, RAG systems\n",
      "continuously update their knowledge base, addressing the limitations of static pre-trained\n",
      "models.\n",
      "2. Improved Accuracy: Retrieved data serves as contextual grounding, reducing halluci-\n",
      "nations and increasing the factual reliability of generated outputs.\n",
      "117\n",
      "Source: Page 116\n",
      "\n",
      "--- Result 3 (Score: 1.280112385749817) ---\n",
      "\n",
      "Figure 64: Final Pass rates of models across LLM Modulo Iterations. Source: Kambhampati et al.\n",
      "[379]\n",
      "3. Domain Adaptability: RAG enables LLMs to integrate domain-specific information,\n",
      "improving performance in specialized areas like law, medicine, and engineering.\n",
      "RAG systems are categorized into three main paradigms:\n",
      "1. Na ve RAG: it was the first iteration of RAG systems. It follows the traditional pipeline\n",
      "of indexing, retrieval, and generation, which is also characterized as a Retrieve-Read\n",
      "framework [304]. This approach is simple and effective but suffers notable drawbacks in\n",
      "terms of retrieval precision (e.g., missing crucial information) and generation accuracy\n",
      "(e.g., allowing for hallucinations, toxicity or bias).\n",
      "2. Advanced RAG: it introduces specific improvements to address the limitations of Na ve\n",
      "RAG. About retrieval quality, it employes pre-retrieval and post-retrieval strategies to en-\n",
      "hance the relevance of retrieved data. For indexing, it uses more sophisticated techniques\n",
      "Source: Page 117\n",
      "\n",
      "--- Result 4 (Score: 1.2923541069030762) ---\n",
      "\n",
      "Figure 65: Technology tree of RAG research. The stages of involving RAG mainly include pre-\n",
      "training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused\n",
      "on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the in-\n",
      "ference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning\n",
      "of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training\n",
      "stage through retrieval-augmented techniques. Source: Gao et al. [375]\n",
      " Memory Module: Uses LLM memory to iteratively align retrieval processes with\n",
      "data distribution and enable unbounded memory pools [262].\n",
      " Routing Module: Dynamically selects pathways (e.g., summarization or database\n",
      "querying) to ensure optimal information retrieval and merging [365].\n",
      " Predict Module: Reduces redundancy and enhances context relevance by generating\n",
      "content directly via the LLM [238].\n",
      "Source: Page 118\n",
      "\n",
      "--- Result 5 (Score: 1.324202537536621) ---\n",
      "\n",
      "hance the relevance of retrieved data. For indexing, it uses more sophisticated techniques\n",
      "like sliding window approach, fine-grained segmentation and metadata. It incorporates\n",
      "additional optimization techniques to streamline the retrieval process [280].\n",
      "3. Modular RAG: this architecture advances beyond previous RAG paradigms (Naive and\n",
      "Advanced RAG) by offering greater adaptability, flexibility, and functionality. It intro-\n",
      "duces new components and interaction patterns to address the challenges of static and\n",
      "rigid retrieval-generation frameworks, making it suitable for diverse tasks and dynamic\n",
      "scenarios. Modular RAG incorporates specialized modules to enhance retrieval and gen-\n",
      "eration:\n",
      " Search Module: Supports direct searches across diverse data sources such as databases,\n",
      "search engines, and knowledge graphs using LLM-generated queries [303].\n",
      " RAGFusion: Implements multi-query strategies for diverse perspectives, utilizing\n",
      "parallel searches and re-ranking for knowledge discovery [320].\n",
      "118\n",
      "Source: Page 117\n"
     ]
    }
   ],
   "source": [
    "results_with_score = vector_store.similarity_search_with_score(query, k=5)\n",
    "\n",
    "for i, (doc, score) in enumerate(results_with_score, 1):\n",
    "    print(f\"\\n--- Result {i} (Score: {score}) ---\\n\")\n",
    "    print(doc.page_content)\n",
    "    print(f\"Source: Page {doc.metadata['page']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18e9dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Metadata Filtering\n",
      "sample metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-11T01:48:37+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-11T01:48:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2501.04040', 'total_pages': 174, 'page': 0, 'page_label': '1', 'start_index': 0}\n",
      "Available page numbers in first 10 chunks: {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Metadata Filtering\")\n",
    "\n",
    "if chunks:\n",
    "    sample_metadata = chunks[0].metadata\n",
    "    print(f\"sample metadata: {sample_metadata}\")\n",
    "\n",
    "    page_numbers = set()\n",
    "    for chunk in chunks[:10]:\n",
    "        if 'page' in chunk.metadata:\n",
    "            page_numbers.add(chunk.metadata['page'])\n",
    "    print(f\"Available page numbers in first 10 chunks: {page_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8176f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching only in Page  0 :\n",
      "\n",
      "--- Result 1 ---\n",
      "\n",
      "frameworks that integrate external systems, allowing LLMs to handle complex, dynamic\n",
      "tasks. By analyzing these factors, this paper aims to foster the ongoing discussion on the\n",
      "capabilities and limits of LLMs, promoting their responsible development and application\n",
      "in novel and increasingly complex environments.\n",
      "1\n",
      "arXiv:2501.04040v2  [cs.CL]  9 Feb 2025\n",
      "Source: Page 0\n",
      "\n",
      "--- Result 2 ---\n",
      "\n",
      "architectural strategies that drive these capabilities. Emphasizing models like GPT and\n",
      "LLaMA, we analyze the impact of exponential data and computational growth on LLM\n",
      "performance, while also addressing the trade-offs associated with scaling. We also ex-\n",
      "amine LLM applications across sectors, such as healthcare, finance, education, and law,\n",
      "highlighting their adaptability and potential to solve domain-specific challenges.\n",
      "Central to this work are the questions of how LLMs generalize across diverse tasks,\n",
      "exhibit planning, and reasoning abilities, and whether these emergent abilities can be\n",
      "systematically elicited or enhanced. In particular, we provide some insights into the CoT\n",
      "(Chain of Thought) and PoT (Plan of Thought) abilities within LLMs, focusing on how\n",
      "pre-training data influences their emergence. Additionally, we investigate LLM-modulo\n",
      "frameworks that integrate external systems, allowing LLMs to handle complex, dynamic\n",
      "Source: Page 0\n"
     ]
    }
   ],
   "source": [
    "if page_numbers:\n",
    "    target_page = sorted(list(page_numbers))[0]\n",
    "    page_results = vector_store.similarity_search(\n",
    "        \"methodology approach\",\n",
    "        k=2,\n",
    "        filter={\"page\": target_page}\n",
    "    )\n",
    "    print(\"Searching only in Page \", target_page,\":\")\n",
    "    for i, doc in enumerate(page_results, 1):\n",
    "        print(f\"\\n--- Result {i} ---\\n\")\n",
    "        print(doc.page_content)\n",
    "        print(f\"Source: Page {doc.metadata['page']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "776913be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multiple Metadata Filtering \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 --- \n",
      "\n",
      "Taylor need to do before this? A. get a certificate , B. teach\n",
      "small children, C. work in a school\n",
      "ARC Science Choose your answer to the question: Which technology was\n",
      "developed most recently? A. cellular telephone, B. television,\n",
      "C. refrigerator, D. airplane\n",
      "QASC Science Choose your answer to the question: What is described in terms\n",
      "of temperature and water in the air? A. storms; B. climate;\n",
      "C. mass; D. seasonal; E. winter; F. density; G. length\n",
      "HellaSWAG Event Choose your answer to the question: We see a chair with a pillow\n",
      "on it. A. a man holding a cat does curling. B. a man holding a\n",
      "cat starts hitting objects on an item. C. a man holding a cat is\n",
      "wrapping a box. D. a man holding a cat sits down on the\n",
      "chair.\n",
      "NumerSense Numerical a square is a shape with maskequally length sides. (four)\n",
      "ProtoQA Prototypical Use simple words separated by commas to name something in\n",
      "your life that could cause you to lose weight. ( Eating less,\n",
      "exercising more, stress.)\n",
      "Source: Page 95\n",
      "\n",
      "--- Result 2 --- \n",
      "\n",
      "guage and programmatic plan generation.\n",
      "Plan-and-Solve (PS) prompting is a text-based plan generation approach that consists of\n",
      "two components: devising a plan and carrying out the subtasks. The process includes:\n",
      "1. Step 1: Prompting for Reasoning Generation . To meet the criteria for effective\n",
      "problem-solving, templates guide LLMs in devising and completing a plan with atten-\n",
      "tion to calculations and intermediate results. For example: Lets first understand the\n",
      "problem, extract relevant variables, devise a plan, and solve the problem step by step.\n",
      "2. Step 2: Prompting for Answer Extraction. Similar to Zero-shot-CoT, another\n",
      "prompt extracts the final numerical answer from the reasoning text.\n",
      "A comparison of prompting strategies is shown in Figure 46. The PS+ variant of Plan-and-Solve\n",
      "is an extension that adds detailed instructions to improve reasoning quality.\n",
      "Figure 46: Example inputs and outputs of GPT-3 with (a) Zero-shot-CoT prompting, (b) Plan-and-\n",
      "Source: Page 100\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(f\" Multiple Metadata Filtering \")\n",
    "complex_results = vector_store.similarity_search(\n",
    "    \"methodology approach\",\n",
    "    k=2,\n",
    "    filter={\n",
    "        \"$and\":[\n",
    "        {\"page\": {\"$gte\": 10}}, # Page greater than or equal to target_page\n",
    "          {\"source\": {\"$ne\": \"\"}} # Source not equal to empty string\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(complex_results, 1):\n",
    "    print(f\"\\n--- Result {i} --- \\n\")\n",
    "    print(doc.page_content)\n",
    "    print(f\"Source: Page {doc.metadata['page']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88152b",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36f9e568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating Retriever...\n",
      "\n",
      "Query: 'What are the key techniques discussed in the paper?'\n",
      "\n",
      "--- Retrieved Document 1 ---\n",
      "\n",
      "4 Utilization Strategies and Techniques 81\n",
      "4.1 In-Context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n",
      "4.1.1 ICL strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n",
      "4.1.2 ICL performance and origins . . . . . . . . . . . . . . . . . . . . . . . . . 86\n",
      "4.1.3 ICL future research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n",
      "4.2 Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n",
      "4.2.1 CoT strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n",
      "4.2.2 CoT performance and origins . . . . . . . . . . . . . . . . . . . . . . . . 93\n",
      "4.3 Program-of-Thoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "4.4 Planning for complex tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n",
      "4.4.1 Commonsense knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n",
      "4.4.2 Prompt and code based planning . . . . . . . . . . . . . . . . . . . . . . 99\n",
      "Source: Page 2\n",
      "\n",
      "--- Retrieved Document 2 ---\n",
      "\n",
      "is in the realm of support vector machines (SVMs), where it is employed to predict labels for a\n",
      "given, fixed set of test data, optimizing the margin not only for the training data but also for\n",
      "the test data, despite their labels being unknown [4].\n",
      "Conversely, inductive learning aims to build a general model that predicts outcomes for\n",
      "new, unseen data based on the patterns learned from the training data. Label propagation\n",
      "(Figure 15) is a common technique in inductive learning, where the model infers the labels of\n",
      "unlabeled data points based on the labels of their neighbours in the feature space.\n",
      "Figure 15: LEFT: original labelled and unlabeled data points. RIGHT: using label propagation, the\n",
      "unlabeled data points have been assigned pseudo-labels. Source: Bergmann [252].\n",
      "Active learning is another inductive learning method that involves iteratively selecting the\n",
      "most informative data points for labelling and optimizing the models performance with minimal\n",
      "Source: Page 44\n",
      "\n",
      "--- Retrieved Document 3 ---\n",
      "\n",
      "on these individual components.\n",
      "Figure 48: The DECOMP framework. Source: Khot et al. [175]\n",
      "In DECOMP, the core is a decomposer LLM that tries to solve a complex task by generating\n",
      "a prompting program P. Each step of P directs a simpler sub-query to a function in an\n",
      "auxiliary set of sub-task functions F available to the system. Given a query Q whose answer\n",
      "is A, the program P is a sequence of the form (( f1, Q1, A1), . . . ,(fk, Qk, Ak)) where Ak is the\n",
      "final answer predicted by P and Qi is a sub-query directed to the sub-task function fi  F. P\n",
      "is executed by a high-level imperative controller, which passes the inputs and outputs between\n",
      "the decomposer and sub-task handler until a stopping condition in P is met and the final\n",
      "output is obtained. Using a software engineering analogy, the decomposer defines the top-\n",
      "level program for the complex task using interfaces to more straightforward sub-task functions.\n",
      "The sub-task handlers serve as modular, debuggable, and upgradable implementations of these\n",
      "Source: Page 103\n",
      "\n",
      "--- Retrieved Document 4 ---\n",
      "\n",
      "particular, we will try to address some fundamental questions. How do these models learn and\n",
      "generalize across tasks and domains? What are these emergent abilities, and how can they be\n",
      "elicited? Which factors contribute to their development (e.g., model size, data, architecture)?\n",
      "What are the inherent limitations of these models and how can they be addressed?\n",
      "4\n",
      "Source: Page 3\n",
      "\n",
      "--- Retrieved Document 5 ---\n",
      "\n",
      "knowledge and intelligence.\n",
      "Bibliography\n",
      "[1] Philip W. Anderson. More is Different: Broken Symmetry and the Nature of the Hier-\n",
      "archical Structure of Science. In: (1972). url: http://www.lanais.famaf.unc.edu.\n",
      "ar/cursos/em/Anderson-MoreDifferent-1972.pdf.\n",
      "[2] Tom M. Mitchell. Machine Learning. McGraw-Hill, 1997.\n",
      "[3] Vladimir Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.\n",
      "[4] Thorsten Joachims. Transductive inference for text classification using support vector\n",
      "machines. In: ICML. Citeseer. 1999.\n",
      "[5] John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random\n",
      "fields: Probabilistic models for segmenting and labeling sequence data. In: Proceedings\n",
      "of the Eighteenth International Conference on Machine Learning (ICML 2001) . Ed. by\n",
      "Carla E. Brodley and Andrea P. Danyluk. Morgan Kaufmann, 2001, pp. 282289.\n",
      "[6] Yoshua Bengio et al. A Neural Probabilistic Language Model. In: Journal of Machine\n",
      "Learning Research 3 (2003), pp. 11371155.\n",
      "Source: Page 151\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Creating Retriever...\")\n",
    "similarity_retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "query = \"What are the key techniques discussed in the paper?\"\n",
    "retrived_docs = similarity_retriever.invoke(query)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "for i, doc in enumerate(retrived_docs, 1):\n",
    "    print(f\"\\n--- Retrieved Document {i} ---\\n\")\n",
    "    print(doc.page_content)\n",
    "    print(f\"Source: Page {doc.metadata['page']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af93451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
